// Copyright 2022 Google LLC
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     https://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

// Code generated by protoc-gen-go. DO NOT EDIT.
// versions:
// 	protoc-gen-go v1.36.8
// 	protoc        v3.21.6
// source: cloud/visionai/v1/data.proto

package visionaidata

import (
	protoreflect "google.golang.org/protobuf/reflect/protoreflect"
	protoimpl "google.golang.org/protobuf/runtime/protoimpl"
	durationpb "google.golang.org/protobuf/types/known/durationpb"
	_ "google.golang.org/protobuf/types/known/structpb"
	timestamppb "google.golang.org/protobuf/types/known/timestamppb"
	reflect "reflect"
	sync "sync"
	unsafe "unsafe"
)

const (
	// Verify that this generated code is sufficiently up-to-date.
	_ = protoimpl.EnforceVersion(20 - protoimpl.MinVersion)
	// Verify that runtime/protoimpl is sufficiently up-to-date.
	_ = protoimpl.EnforceVersion(protoimpl.MaxVersion - 20)
)

// Enum describing all possible types of a stream annotation.
type StreamAnnotationType int32

const (
	// Type UNSPECIFIED.
	StreamAnnotationType_STREAM_ANNOTATION_TYPE_UNSPECIFIED StreamAnnotationType = 0
	// active_zone annotation defines a polygon on top of the content from an
	// image/video based stream, following processing will only focus on the
	// content inside the active zone.
	StreamAnnotationType_STREAM_ANNOTATION_TYPE_ACTIVE_ZONE StreamAnnotationType = 1
	// crossing_line annotation defines a polyline on top of the content from an
	// image/video based Vision AI stream, events happening across the line will
	// be captured. For example, the counts of people who goes acroos the line
	// in Occupancy Analytic Processor.
	StreamAnnotationType_STREAM_ANNOTATION_TYPE_CROSSING_LINE StreamAnnotationType = 2
)

// Enum value maps for StreamAnnotationType.
var (
	StreamAnnotationType_name = map[int32]string{
		0: "STREAM_ANNOTATION_TYPE_UNSPECIFIED",
		1: "STREAM_ANNOTATION_TYPE_ACTIVE_ZONE",
		2: "STREAM_ANNOTATION_TYPE_CROSSING_LINE",
	}
	StreamAnnotationType_value = map[string]int32{
		"STREAM_ANNOTATION_TYPE_UNSPECIFIED":   0,
		"STREAM_ANNOTATION_TYPE_ACTIVE_ZONE":   1,
		"STREAM_ANNOTATION_TYPE_CROSSING_LINE": 2,
	}
)

func (x StreamAnnotationType) Enum() *StreamAnnotationType {
	p := new(StreamAnnotationType)
	*p = x
	return p
}

func (x StreamAnnotationType) String() string {
	return protoimpl.X.EnumStringOf(x.Descriptor(), protoreflect.EnumNumber(x))
}

func (StreamAnnotationType) Descriptor() protoreflect.EnumDescriptor {
	return file_cloud_visionai_v1_data_proto_enumTypes[0].Descriptor()
}

func (StreamAnnotationType) Type() protoreflect.EnumType {
	return &file_cloud_visionai_v1_data_proto_enumTypes[0]
}

func (x StreamAnnotationType) Number() protoreflect.EnumNumber {
	return protoreflect.EnumNumber(x)
}

// Deprecated: Use StreamAnnotationType.Descriptor instead.
func (StreamAnnotationType) EnumDescriptor() ([]byte, []int) {
	return file_cloud_visionai_v1_data_proto_rawDescGZIP(), []int{0}
}

// RunMode represents the mode to launch the Process on.
type RunMode int32

const (
	// Mode is unspecified.
	RunMode_RUN_MODE_UNSPECIFIED RunMode = 0
	// Live mode. Meaning the Process is launched to handle live video
	// source, and possible packet drops are expected.
	RunMode_LIVE RunMode = 1
	// Submission mode. Meaning the Process is launched to handle bounded video
	// files, with no packet drop. Completion status is tracked.
	RunMode_SUBMISSION RunMode = 2
)

// Enum value maps for RunMode.
var (
	RunMode_name = map[int32]string{
		0: "RUN_MODE_UNSPECIFIED",
		1: "LIVE",
		2: "SUBMISSION",
	}
	RunMode_value = map[string]int32{
		"RUN_MODE_UNSPECIFIED": 0,
		"LIVE":                 1,
		"SUBMISSION":           2,
	}
)

func (x RunMode) Enum() *RunMode {
	p := new(RunMode)
	*p = x
	return p
}

func (x RunMode) String() string {
	return protoimpl.X.EnumStringOf(x.Descriptor(), protoreflect.EnumNumber(x))
}

func (RunMode) Descriptor() protoreflect.EnumDescriptor {
	return file_cloud_visionai_v1_data_proto_enumTypes[1].Descriptor()
}

func (RunMode) Type() protoreflect.EnumType {
	return &file_cloud_visionai_v1_data_proto_enumTypes[1]
}

func (x RunMode) Number() protoreflect.EnumNumber {
	return protoreflect.EnumNumber(x)
}

// Deprecated: Use RunMode.Descriptor instead.
func (RunMode) EnumDescriptor() ([]byte, []int) {
	return file_cloud_visionai_v1_data_proto_rawDescGZIP(), []int{1}
}

// All the supported model types in Vision AI App Platform.
type ModelType int32

const (
	// Processor Type UNSPECIFIED.
	ModelType_MODEL_TYPE_UNSPECIFIED ModelType = 0
	// Model Type Image Classification.
	ModelType_IMAGE_CLASSIFICATION ModelType = 1
	// Model Type Object Detection.
	ModelType_OBJECT_DETECTION ModelType = 2
	// Model Type Video Classification.
	ModelType_VIDEO_CLASSIFICATION ModelType = 3
	// Model Type Object Tracking.
	ModelType_VIDEO_OBJECT_TRACKING ModelType = 4
	// Model Type Action Recognition.
	ModelType_VIDEO_ACTION_RECOGNITION ModelType = 5
	// Model Type Occupancy Counting.
	ModelType_OCCUPANCY_COUNTING ModelType = 6
	// Model Type Person Blur.
	ModelType_PERSON_BLUR ModelType = 7
	// Model Type Vertex Custom.
	ModelType_VERTEX_CUSTOM ModelType = 8
)

// Enum value maps for ModelType.
var (
	ModelType_name = map[int32]string{
		0: "MODEL_TYPE_UNSPECIFIED",
		1: "IMAGE_CLASSIFICATION",
		2: "OBJECT_DETECTION",
		3: "VIDEO_CLASSIFICATION",
		4: "VIDEO_OBJECT_TRACKING",
		5: "VIDEO_ACTION_RECOGNITION",
		6: "OCCUPANCY_COUNTING",
		7: "PERSON_BLUR",
		8: "VERTEX_CUSTOM",
	}
	ModelType_value = map[string]int32{
		"MODEL_TYPE_UNSPECIFIED":   0,
		"IMAGE_CLASSIFICATION":     1,
		"OBJECT_DETECTION":         2,
		"VIDEO_CLASSIFICATION":     3,
		"VIDEO_OBJECT_TRACKING":    4,
		"VIDEO_ACTION_RECOGNITION": 5,
		"OCCUPANCY_COUNTING":       6,
		"PERSON_BLUR":              7,
		"VERTEX_CUSTOM":            8,
	}
)

func (x ModelType) Enum() *ModelType {
	p := new(ModelType)
	*p = x
	return p
}

func (x ModelType) String() string {
	return protoimpl.X.EnumStringOf(x.Descriptor(), protoreflect.EnumNumber(x))
}

func (ModelType) Descriptor() protoreflect.EnumDescriptor {
	return file_cloud_visionai_v1_data_proto_enumTypes[2].Descriptor()
}

func (ModelType) Type() protoreflect.EnumType {
	return &file_cloud_visionai_v1_data_proto_enumTypes[2]
}

func (x ModelType) Number() protoreflect.EnumNumber {
	return protoreflect.EnumNumber(x)
}

// Deprecated: Use ModelType.Descriptor instead.
func (ModelType) EnumDescriptor() ([]byte, []int) {
	return file_cloud_visionai_v1_data_proto_rawDescGZIP(), []int{2}
}

// Represents a hardware accelerator type.
type AcceleratorType int32

const (
	// Unspecified accelerator type, which means no accelerator.
	AcceleratorType_ACCELERATOR_TYPE_UNSPECIFIED AcceleratorType = 0
	// Nvidia Tesla K80 GPU.
	AcceleratorType_NVIDIA_TESLA_K80 AcceleratorType = 1
	// Nvidia Tesla P100 GPU.
	AcceleratorType_NVIDIA_TESLA_P100 AcceleratorType = 2
	// Nvidia Tesla V100 GPU.
	AcceleratorType_NVIDIA_TESLA_V100 AcceleratorType = 3
	// Nvidia Tesla P4 GPU.
	AcceleratorType_NVIDIA_TESLA_P4 AcceleratorType = 4
	// Nvidia Tesla T4 GPU.
	AcceleratorType_NVIDIA_TESLA_T4 AcceleratorType = 5
	// Nvidia Tesla A100 GPU.
	AcceleratorType_NVIDIA_TESLA_A100 AcceleratorType = 8
	// TPU v2.
	AcceleratorType_TPU_V2 AcceleratorType = 6
	// TPU v3.
	AcceleratorType_TPU_V3 AcceleratorType = 7
)

// Enum value maps for AcceleratorType.
var (
	AcceleratorType_name = map[int32]string{
		0: "ACCELERATOR_TYPE_UNSPECIFIED",
		1: "NVIDIA_TESLA_K80",
		2: "NVIDIA_TESLA_P100",
		3: "NVIDIA_TESLA_V100",
		4: "NVIDIA_TESLA_P4",
		5: "NVIDIA_TESLA_T4",
		8: "NVIDIA_TESLA_A100",
		6: "TPU_V2",
		7: "TPU_V3",
	}
	AcceleratorType_value = map[string]int32{
		"ACCELERATOR_TYPE_UNSPECIFIED": 0,
		"NVIDIA_TESLA_K80":             1,
		"NVIDIA_TESLA_P100":            2,
		"NVIDIA_TESLA_V100":            3,
		"NVIDIA_TESLA_P4":              4,
		"NVIDIA_TESLA_T4":              5,
		"NVIDIA_TESLA_A100":            8,
		"TPU_V2":                       6,
		"TPU_V3":                       7,
	}
)

func (x AcceleratorType) Enum() *AcceleratorType {
	p := new(AcceleratorType)
	*p = x
	return p
}

func (x AcceleratorType) String() string {
	return protoimpl.X.EnumStringOf(x.Descriptor(), protoreflect.EnumNumber(x))
}

func (AcceleratorType) Descriptor() protoreflect.EnumDescriptor {
	return file_cloud_visionai_v1_data_proto_enumTypes[3].Descriptor()
}

func (AcceleratorType) Type() protoreflect.EnumType {
	return &file_cloud_visionai_v1_data_proto_enumTypes[3]
}

func (x AcceleratorType) Number() protoreflect.EnumNumber {
	return protoreflect.EnumNumber(x)
}

// Deprecated: Use AcceleratorType.Descriptor instead.
func (AcceleratorType) EnumDescriptor() ([]byte, []int) {
	return file_cloud_visionai_v1_data_proto_rawDescGZIP(), []int{3}
}

// All supported data types.
type DataType int32

const (
	// The default value of DataType.
	DataType_DATA_TYPE_UNSPECIFIED DataType = 0
	// Video data type like H264.
	DataType_VIDEO DataType = 1
	// Image data type.
	DataType_IMAGE DataType = 3
	// Protobuf data type, usually used for general data blob.
	DataType_PROTO DataType = 2
)

// Enum value maps for DataType.
var (
	DataType_name = map[int32]string{
		0: "DATA_TYPE_UNSPECIFIED",
		1: "VIDEO",
		3: "IMAGE",
		2: "PROTO",
	}
	DataType_value = map[string]int32{
		"DATA_TYPE_UNSPECIFIED": 0,
		"VIDEO":                 1,
		"IMAGE":                 3,
		"PROTO":                 2,
	}
)

func (x DataType) Enum() *DataType {
	p := new(DataType)
	*p = x
	return p
}

func (x DataType) String() string {
	return protoimpl.X.EnumStringOf(x.Descriptor(), protoreflect.EnumNumber(x))
}

func (DataType) Descriptor() protoreflect.EnumDescriptor {
	return file_cloud_visionai_v1_data_proto_enumTypes[4].Descriptor()
}

func (DataType) Type() protoreflect.EnumType {
	return &file_cloud_visionai_v1_data_proto_enumTypes[4]
}

func (x DataType) Number() protoreflect.EnumNumber {
	return protoreflect.EnumNumber(x)
}

// Deprecated: Use DataType.Descriptor instead.
func (DataType) EnumDescriptor() ([]byte, []int) {
	return file_cloud_visionai_v1_data_proto_rawDescGZIP(), []int{4}
}

// The current state of the cluster.
type Cluster_State int32

const (
	// Not set.
	Cluster_STATE_UNSPECIFIED Cluster_State = 0
	// The PROVISIONING state indicates the cluster is being created.
	Cluster_PROVISIONING Cluster_State = 1
	// The RUNNING state indicates the cluster has been created and is fully
	// usable.
	Cluster_RUNNING Cluster_State = 2
	// The STOPPING state indicates the cluster is being deleted.
	Cluster_STOPPING Cluster_State = 3
	// The ERROR state indicates the cluster is unusable. It will be
	// automatically deleted.
	Cluster_ERROR Cluster_State = 4
)

// Enum value maps for Cluster_State.
var (
	Cluster_State_name = map[int32]string{
		0: "STATE_UNSPECIFIED",
		1: "PROVISIONING",
		2: "RUNNING",
		3: "STOPPING",
		4: "ERROR",
	}
	Cluster_State_value = map[string]int32{
		"STATE_UNSPECIFIED": 0,
		"PROVISIONING":      1,
		"RUNNING":           2,
		"STOPPING":          3,
		"ERROR":             4,
	}
)

func (x Cluster_State) Enum() *Cluster_State {
	p := new(Cluster_State)
	*p = x
	return p
}

func (x Cluster_State) String() string {
	return protoimpl.X.EnumStringOf(x.Descriptor(), protoreflect.EnumNumber(x))
}

func (Cluster_State) Descriptor() protoreflect.EnumDescriptor {
	return file_cloud_visionai_v1_data_proto_enumTypes[5].Descriptor()
}

func (Cluster_State) Type() protoreflect.EnumType {
	return &file_cloud_visionai_v1_data_proto_enumTypes[5]
}

func (x Cluster_State) Number() protoreflect.EnumNumber {
	return protoreflect.EnumNumber(x)
}

// Deprecated: Use Cluster_State.Descriptor instead.
func (Cluster_State) EnumDescriptor() ([]byte, []int) {
	return file_cloud_visionai_v1_data_proto_rawDescGZIP(), []int{4, 0}
}

// State represents the running status of the Process.
type RunStatus_State int32

const (
	// State is unspecified.
	RunStatus_STATE_UNSPECIFIED RunStatus_State = 0
	// INITIALIZING means the Process is scheduled but yet ready to handle
	// real traffic.
	RunStatus_INITIALIZING RunStatus_State = 1
	// RUNNING means the Process is up running and handling traffic.
	RunStatus_RUNNING RunStatus_State = 2
	// COMPLETED means the Process has completed the processing, especially
	// for non-streaming use case.
	RunStatus_COMPLETED RunStatus_State = 3
	// FAILED means the Process failed to complete the processing.
	RunStatus_FAILED RunStatus_State = 4
	// PENDING means the Process is created but yet to be scheduled.
	RunStatus_PENDING RunStatus_State = 5
)

// Enum value maps for RunStatus_State.
var (
	RunStatus_State_name = map[int32]string{
		0: "STATE_UNSPECIFIED",
		1: "INITIALIZING",
		2: "RUNNING",
		3: "COMPLETED",
		4: "FAILED",
		5: "PENDING",
	}
	RunStatus_State_value = map[string]int32{
		"STATE_UNSPECIFIED": 0,
		"INITIALIZING":      1,
		"RUNNING":           2,
		"COMPLETED":         3,
		"FAILED":            4,
		"PENDING":           5,
	}
)

func (x RunStatus_State) Enum() *RunStatus_State {
	p := new(RunStatus_State)
	*p = x
	return p
}

func (x RunStatus_State) String() string {
	return protoimpl.X.EnumStringOf(x.Descriptor(), protoreflect.EnumNumber(x))
}

func (RunStatus_State) Descriptor() protoreflect.EnumDescriptor {
	return file_cloud_visionai_v1_data_proto_enumTypes[6].Descriptor()
}

func (RunStatus_State) Type() protoreflect.EnumType {
	return &file_cloud_visionai_v1_data_proto_enumTypes[6]
}

func (x RunStatus_State) Number() protoreflect.EnumNumber {
	return protoreflect.EnumNumber(x)
}

// Deprecated: Use RunStatus_State.Descriptor instead.
func (RunStatus_State) EnumDescriptor() ([]byte, []int) {
	return file_cloud_visionai_v1_data_proto_rawDescGZIP(), []int{9, 0}
}

// State of the Application
type Application_State int32

const (
	// The default value. This value is used if the state is omitted.
	Application_STATE_UNSPECIFIED Application_State = 0
	// State CREATED.
	Application_CREATED Application_State = 1
	// State DEPLOYING.
	Application_DEPLOYING Application_State = 2
	// State DEPLOYED.
	Application_DEPLOYED Application_State = 3
	// State UNDEPLOYING.
	Application_UNDEPLOYING Application_State = 4
	// State DELETED.
	Application_DELETED Application_State = 5
	// State ERROR.
	Application_ERROR Application_State = 6
	// State CREATING.
	Application_CREATING Application_State = 7
	// State Updating.
	Application_UPDATING Application_State = 8
	// State Deleting.
	Application_DELETING Application_State = 9
	// State Fixing.
	Application_FIXING Application_State = 10
)

// Enum value maps for Application_State.
var (
	Application_State_name = map[int32]string{
		0:  "STATE_UNSPECIFIED",
		1:  "CREATED",
		2:  "DEPLOYING",
		3:  "DEPLOYED",
		4:  "UNDEPLOYING",
		5:  "DELETED",
		6:  "ERROR",
		7:  "CREATING",
		8:  "UPDATING",
		9:  "DELETING",
		10: "FIXING",
	}
	Application_State_value = map[string]int32{
		"STATE_UNSPECIFIED": 0,
		"CREATED":           1,
		"DEPLOYING":         2,
		"DEPLOYED":          3,
		"UNDEPLOYING":       4,
		"DELETED":           5,
		"ERROR":             6,
		"CREATING":          7,
		"UPDATING":          8,
		"DELETING":          9,
		"FIXING":            10,
	}
)

func (x Application_State) Enum() *Application_State {
	p := new(Application_State)
	*p = x
	return p
}

func (x Application_State) String() string {
	return protoimpl.X.EnumStringOf(x.Descriptor(), protoreflect.EnumNumber(x))
}

func (Application_State) Descriptor() protoreflect.EnumDescriptor {
	return file_cloud_visionai_v1_data_proto_enumTypes[7].Descriptor()
}

func (Application_State) Type() protoreflect.EnumType {
	return &file_cloud_visionai_v1_data_proto_enumTypes[7]
}

func (x Application_State) Number() protoreflect.EnumNumber {
	return protoreflect.EnumNumber(x)
}

// Deprecated: Use Application_State.Descriptor instead.
func (Application_State) EnumDescriptor() ([]byte, []int) {
	return file_cloud_visionai_v1_data_proto_rawDescGZIP(), []int{12, 0}
}

// Billing mode of the Application
type Application_BillingMode int32

const (
	// The default value.
	Application_BILLING_MODE_UNSPECIFIED Application_BillingMode = 0
	// Pay as you go billing mode.
	Application_PAYG Application_BillingMode = 1
	// Monthly billing mode.
	Application_MONTHLY Application_BillingMode = 2
)

// Enum value maps for Application_BillingMode.
var (
	Application_BillingMode_name = map[int32]string{
		0: "BILLING_MODE_UNSPECIFIED",
		1: "PAYG",
		2: "MONTHLY",
	}
	Application_BillingMode_value = map[string]int32{
		"BILLING_MODE_UNSPECIFIED": 0,
		"PAYG":                     1,
		"MONTHLY":                  2,
	}
)

func (x Application_BillingMode) Enum() *Application_BillingMode {
	p := new(Application_BillingMode)
	*p = x
	return p
}

func (x Application_BillingMode) String() string {
	return protoimpl.X.EnumStringOf(x.Descriptor(), protoreflect.EnumNumber(x))
}

func (Application_BillingMode) Descriptor() protoreflect.EnumDescriptor {
	return file_cloud_visionai_v1_data_proto_enumTypes[8].Descriptor()
}

func (Application_BillingMode) Type() protoreflect.EnumType {
	return &file_cloud_visionai_v1_data_proto_enumTypes[8]
}

func (x Application_BillingMode) Number() protoreflect.EnumNumber {
	return protoreflect.EnumNumber(x)
}

// Deprecated: Use Application_BillingMode.Descriptor instead.
func (Application_BillingMode) EnumDescriptor() ([]byte, []int) {
	return file_cloud_visionai_v1_data_proto_rawDescGZIP(), []int{12, 1}
}

// Type
type Processor_ProcessorType int32

const (
	// Processor Type UNSPECIFIED.
	Processor_PROCESSOR_TYPE_UNSPECIFIED Processor_ProcessorType = 0
	// Processor Type PRETRAINED.
	// Pretrained processor is developed by Vision AI App Platform with
	// state-of-the-art vision data processing functionality, like occupancy
	// counting or person blur. Pretrained processor is usually publicly
	// available.
	Processor_PRETRAINED Processor_ProcessorType = 1
	// Processor Type CUSTOM.
	// Custom processors are specialized processors which are either uploaded by
	// customers or imported from other GCP platform (for example Vertex AI).
	// Custom processor is only visible to the creator.
	Processor_CUSTOM Processor_ProcessorType = 2
	// Processor Type CONNECTOR.
	// Connector processors are special processors which perform I/O for the
	// application, they do not processing the data but either deliver the data
	// to other processors or receive data from other processors.
	Processor_CONNECTOR Processor_ProcessorType = 3
)

// Enum value maps for Processor_ProcessorType.
var (
	Processor_ProcessorType_name = map[int32]string{
		0: "PROCESSOR_TYPE_UNSPECIFIED",
		1: "PRETRAINED",
		2: "CUSTOM",
		3: "CONNECTOR",
	}
	Processor_ProcessorType_value = map[string]int32{
		"PROCESSOR_TYPE_UNSPECIFIED": 0,
		"PRETRAINED":                 1,
		"CUSTOM":                     2,
		"CONNECTOR":                  3,
	}
)

func (x Processor_ProcessorType) Enum() *Processor_ProcessorType {
	p := new(Processor_ProcessorType)
	*p = x
	return p
}

func (x Processor_ProcessorType) String() string {
	return protoimpl.X.EnumStringOf(x.Descriptor(), protoreflect.EnumNumber(x))
}

func (Processor_ProcessorType) Descriptor() protoreflect.EnumDescriptor {
	return file_cloud_visionai_v1_data_proto_enumTypes[9].Descriptor()
}

func (Processor_ProcessorType) Type() protoreflect.EnumType {
	return &file_cloud_visionai_v1_data_proto_enumTypes[9]
}

func (x Processor_ProcessorType) Number() protoreflect.EnumNumber {
	return protoreflect.EnumNumber(x)
}

// Deprecated: Use Processor_ProcessorType.Descriptor instead.
func (Processor_ProcessorType) EnumDescriptor() ([]byte, []int) {
	return file_cloud_visionai_v1_data_proto_rawDescGZIP(), []int{16, 0}
}

type Processor_ProcessorState int32

const (
	// Unspecified Processor state.
	Processor_PROCESSOR_STATE_UNSPECIFIED Processor_ProcessorState = 0
	// Processor is being created (not ready for use).
	Processor_CREATING Processor_ProcessorState = 1
	// Processor is and ready for use.
	Processor_ACTIVE Processor_ProcessorState = 2
	// Processor is being deleted (not ready for use).
	Processor_DELETING Processor_ProcessorState = 3
	// Processor deleted or creation failed .
	Processor_FAILED Processor_ProcessorState = 4
)

// Enum value maps for Processor_ProcessorState.
var (
	Processor_ProcessorState_name = map[int32]string{
		0: "PROCESSOR_STATE_UNSPECIFIED",
		1: "CREATING",
		2: "ACTIVE",
		3: "DELETING",
		4: "FAILED",
	}
	Processor_ProcessorState_value = map[string]int32{
		"PROCESSOR_STATE_UNSPECIFIED": 0,
		"CREATING":                    1,
		"ACTIVE":                      2,
		"DELETING":                    3,
		"FAILED":                      4,
	}
)

func (x Processor_ProcessorState) Enum() *Processor_ProcessorState {
	p := new(Processor_ProcessorState)
	*p = x
	return p
}

func (x Processor_ProcessorState) String() string {
	return protoimpl.X.EnumStringOf(x.Descriptor(), protoreflect.EnumNumber(x))
}

func (Processor_ProcessorState) Descriptor() protoreflect.EnumDescriptor {
	return file_cloud_visionai_v1_data_proto_enumTypes[10].Descriptor()
}

func (Processor_ProcessorState) Type() protoreflect.EnumType {
	return &file_cloud_visionai_v1_data_proto_enumTypes[10]
}

func (x Processor_ProcessorState) Number() protoreflect.EnumNumber {
	return protoreflect.EnumNumber(x)
}

// Deprecated: Use Processor_ProcessorState.Descriptor instead.
func (Processor_ProcessorState) EnumDescriptor() ([]byte, []int) {
	return file_cloud_visionai_v1_data_proto_rawDescGZIP(), []int{16, 1}
}

// Source type of the imported custom processor.
type CustomProcessorSourceInfo_SourceType int32

const (
	// Source type unspecified.
	CustomProcessorSourceInfo_SOURCE_TYPE_UNSPECIFIED CustomProcessorSourceInfo_SourceType = 0
	// Custom processors coming from Vertex AutoML product.
	CustomProcessorSourceInfo_VERTEX_AUTOML CustomProcessorSourceInfo_SourceType = 1
	// Custom processors coming from general custom models from Vertex.
	CustomProcessorSourceInfo_VERTEX_CUSTOM CustomProcessorSourceInfo_SourceType = 2
	// Source for Product Recognizer.
	CustomProcessorSourceInfo_PRODUCT_RECOGNIZER CustomProcessorSourceInfo_SourceType = 3
)

// Enum value maps for CustomProcessorSourceInfo_SourceType.
var (
	CustomProcessorSourceInfo_SourceType_name = map[int32]string{
		0: "SOURCE_TYPE_UNSPECIFIED",
		1: "VERTEX_AUTOML",
		2: "VERTEX_CUSTOM",
		3: "PRODUCT_RECOGNIZER",
	}
	CustomProcessorSourceInfo_SourceType_value = map[string]int32{
		"SOURCE_TYPE_UNSPECIFIED": 0,
		"VERTEX_AUTOML":           1,
		"VERTEX_CUSTOM":           2,
		"PRODUCT_RECOGNIZER":      3,
	}
)

func (x CustomProcessorSourceInfo_SourceType) Enum() *CustomProcessorSourceInfo_SourceType {
	p := new(CustomProcessorSourceInfo_SourceType)
	*p = x
	return p
}

func (x CustomProcessorSourceInfo_SourceType) String() string {
	return protoimpl.X.EnumStringOf(x.Descriptor(), protoreflect.EnumNumber(x))
}

func (CustomProcessorSourceInfo_SourceType) Descriptor() protoreflect.EnumDescriptor {
	return file_cloud_visionai_v1_data_proto_enumTypes[11].Descriptor()
}

func (CustomProcessorSourceInfo_SourceType) Type() protoreflect.EnumType {
	return &file_cloud_visionai_v1_data_proto_enumTypes[11]
}

func (x CustomProcessorSourceInfo_SourceType) Number() protoreflect.EnumNumber {
	return protoreflect.EnumNumber(x)
}

// Deprecated: Use CustomProcessorSourceInfo_SourceType.Descriptor instead.
func (CustomProcessorSourceInfo_SourceType) EnumDescriptor() ([]byte, []int) {
	return file_cloud_visionai_v1_data_proto_rawDescGZIP(), []int{18, 0}
}

// Type of Person Blur
type PersonBlurConfig_PersonBlurType int32

const (
	// PersonBlur Type UNSPECIFIED.
	PersonBlurConfig_PERSON_BLUR_TYPE_UNSPECIFIED PersonBlurConfig_PersonBlurType = 0
	// FaceBlur Type full occlusion.
	PersonBlurConfig_FULL_OCCULUSION PersonBlurConfig_PersonBlurType = 1
	// FaceBlur Type blur filter.
	PersonBlurConfig_BLUR_FILTER PersonBlurConfig_PersonBlurType = 2
)

// Enum value maps for PersonBlurConfig_PersonBlurType.
var (
	PersonBlurConfig_PersonBlurType_name = map[int32]string{
		0: "PERSON_BLUR_TYPE_UNSPECIFIED",
		1: "FULL_OCCULUSION",
		2: "BLUR_FILTER",
	}
	PersonBlurConfig_PersonBlurType_value = map[string]int32{
		"PERSON_BLUR_TYPE_UNSPECIFIED": 0,
		"FULL_OCCULUSION":              1,
		"BLUR_FILTER":                  2,
	}
)

func (x PersonBlurConfig_PersonBlurType) Enum() *PersonBlurConfig_PersonBlurType {
	p := new(PersonBlurConfig_PersonBlurType)
	*p = x
	return p
}

func (x PersonBlurConfig_PersonBlurType) String() string {
	return protoimpl.X.EnumStringOf(x.Descriptor(), protoreflect.EnumNumber(x))
}

func (PersonBlurConfig_PersonBlurType) Descriptor() protoreflect.EnumDescriptor {
	return file_cloud_visionai_v1_data_proto_enumTypes[12].Descriptor()
}

func (PersonBlurConfig_PersonBlurType) Type() protoreflect.EnumType {
	return &file_cloud_visionai_v1_data_proto_enumTypes[12]
}

func (x PersonBlurConfig_PersonBlurType) Number() protoreflect.EnumNumber {
	return protoreflect.EnumNumber(x)
}

// Deprecated: Use PersonBlurConfig_PersonBlurType.Descriptor instead.
func (PersonBlurConfig_PersonBlurType) EnumDescriptor() ([]byte, []int) {
	return file_cloud_visionai_v1_data_proto_rawDescGZIP(), []int{24, 0}
}

// Clock that will be used for joining streams.
type Event_Clock int32

const (
	// Clock is not specified.
	Event_CLOCK_UNSPECIFIED Event_Clock = 0
	// Use the timestamp when the data is captured. Clients need to sync the
	// clock.
	Event_CAPTURE Event_Clock = 1
	// Use the timestamp when the data is received.
	Event_INGEST Event_Clock = 2
)

// Enum value maps for Event_Clock.
var (
	Event_Clock_name = map[int32]string{
		0: "CLOCK_UNSPECIFIED",
		1: "CAPTURE",
		2: "INGEST",
	}
	Event_Clock_value = map[string]int32{
		"CLOCK_UNSPECIFIED": 0,
		"CAPTURE":           1,
		"INGEST":            2,
	}
)

func (x Event_Clock) Enum() *Event_Clock {
	p := new(Event_Clock)
	*p = x
	return p
}

func (x Event_Clock) String() string {
	return protoimpl.X.EnumStringOf(x.Descriptor(), protoreflect.EnumNumber(x))
}

func (Event_Clock) Descriptor() protoreflect.EnumDescriptor {
	return file_cloud_visionai_v1_data_proto_enumTypes[13].Descriptor()
}

func (Event_Clock) Type() protoreflect.EnumType {
	return &file_cloud_visionai_v1_data_proto_enumTypes[13]
}

func (x Event_Clock) Number() protoreflect.EnumNumber {
	return protoreflect.EnumNumber(x)
}

// Deprecated: Use Event_Clock.Descriptor instead.
func (Event_Clock) EnumDescriptor() ([]byte, []int) {
	return file_cloud_visionai_v1_data_proto_rawDescGZIP(), []int{37, 0}
}

// message about annotations about Vision AI stream resource.
type StreamAnnotation struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Types that are valid to be assigned to AnnotationPayload:
	//
	//	*StreamAnnotation_ActiveZone
	//	*StreamAnnotation_CrossingLine
	AnnotationPayload isStreamAnnotation_AnnotationPayload `protobuf_oneof:"annotation_payload"`
	// ID of the annotation. It must be unique when used in the certain context.
	// For example, all the annotations to one input streams of a Vision AI
	// application.
	Id string `protobuf:"bytes,1,opt,name=id,proto3" json:"id,omitempty"`
	// User-friendly name for the annotation.
	DisplayName string `protobuf:"bytes,2,opt,name=display_name,json=displayName,proto3" json:"display_name,omitempty"`
	// The Vision AI stream resource name.
	SourceStream string `protobuf:"bytes,3,opt,name=source_stream,json=sourceStream,proto3" json:"source_stream,omitempty"`
	// The actual type of Annotation.
	Type          StreamAnnotationType `protobuf:"varint,4,opt,name=type,proto3,enum=google.events.cloud.visionai.v1.StreamAnnotationType" json:"type,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *StreamAnnotation) Reset() {
	*x = StreamAnnotation{}
	mi := &file_cloud_visionai_v1_data_proto_msgTypes[0]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *StreamAnnotation) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*StreamAnnotation) ProtoMessage() {}

func (x *StreamAnnotation) ProtoReflect() protoreflect.Message {
	mi := &file_cloud_visionai_v1_data_proto_msgTypes[0]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use StreamAnnotation.ProtoReflect.Descriptor instead.
func (*StreamAnnotation) Descriptor() ([]byte, []int) {
	return file_cloud_visionai_v1_data_proto_rawDescGZIP(), []int{0}
}

func (x *StreamAnnotation) GetAnnotationPayload() isStreamAnnotation_AnnotationPayload {
	if x != nil {
		return x.AnnotationPayload
	}
	return nil
}

func (x *StreamAnnotation) GetActiveZone() *NormalizedPolygon {
	if x != nil {
		if x, ok := x.AnnotationPayload.(*StreamAnnotation_ActiveZone); ok {
			return x.ActiveZone
		}
	}
	return nil
}

func (x *StreamAnnotation) GetCrossingLine() *NormalizedPolyline {
	if x != nil {
		if x, ok := x.AnnotationPayload.(*StreamAnnotation_CrossingLine); ok {
			return x.CrossingLine
		}
	}
	return nil
}

func (x *StreamAnnotation) GetId() string {
	if x != nil {
		return x.Id
	}
	return ""
}

func (x *StreamAnnotation) GetDisplayName() string {
	if x != nil {
		return x.DisplayName
	}
	return ""
}

func (x *StreamAnnotation) GetSourceStream() string {
	if x != nil {
		return x.SourceStream
	}
	return ""
}

func (x *StreamAnnotation) GetType() StreamAnnotationType {
	if x != nil {
		return x.Type
	}
	return StreamAnnotationType_STREAM_ANNOTATION_TYPE_UNSPECIFIED
}

type isStreamAnnotation_AnnotationPayload interface {
	isStreamAnnotation_AnnotationPayload()
}

type StreamAnnotation_ActiveZone struct {
	// Annotation for type ACTIVE_ZONE
	ActiveZone *NormalizedPolygon `protobuf:"bytes,5,opt,name=active_zone,json=activeZone,proto3,oneof"`
}

type StreamAnnotation_CrossingLine struct {
	// Annotation for type CROSSING_LINE
	CrossingLine *NormalizedPolyline `protobuf:"bytes,6,opt,name=crossing_line,json=crossingLine,proto3,oneof"`
}

func (*StreamAnnotation_ActiveZone) isStreamAnnotation_AnnotationPayload() {}

func (*StreamAnnotation_CrossingLine) isStreamAnnotation_AnnotationPayload() {}

// Normalized Polygon.
type NormalizedPolygon struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// The bounding polygon normalized vertices. Top left corner of the image
	// will be [0, 0].
	NormalizedVertices []*NormalizedVertex `protobuf:"bytes,1,rep,name=normalized_vertices,json=normalizedVertices,proto3" json:"normalized_vertices,omitempty"`
	unknownFields      protoimpl.UnknownFields
	sizeCache          protoimpl.SizeCache
}

func (x *NormalizedPolygon) Reset() {
	*x = NormalizedPolygon{}
	mi := &file_cloud_visionai_v1_data_proto_msgTypes[1]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *NormalizedPolygon) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*NormalizedPolygon) ProtoMessage() {}

func (x *NormalizedPolygon) ProtoReflect() protoreflect.Message {
	mi := &file_cloud_visionai_v1_data_proto_msgTypes[1]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use NormalizedPolygon.ProtoReflect.Descriptor instead.
func (*NormalizedPolygon) Descriptor() ([]byte, []int) {
	return file_cloud_visionai_v1_data_proto_rawDescGZIP(), []int{1}
}

func (x *NormalizedPolygon) GetNormalizedVertices() []*NormalizedVertex {
	if x != nil {
		return x.NormalizedVertices
	}
	return nil
}

// Normalized Pplyline, which represents a curve consisting of connected
// straight-line segments.
type NormalizedPolyline struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// A sequence of vertices connected by straight lines.
	NormalizedVertices []*NormalizedVertex `protobuf:"bytes,1,rep,name=normalized_vertices,json=normalizedVertices,proto3" json:"normalized_vertices,omitempty"`
	unknownFields      protoimpl.UnknownFields
	sizeCache          protoimpl.SizeCache
}

func (x *NormalizedPolyline) Reset() {
	*x = NormalizedPolyline{}
	mi := &file_cloud_visionai_v1_data_proto_msgTypes[2]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *NormalizedPolyline) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*NormalizedPolyline) ProtoMessage() {}

func (x *NormalizedPolyline) ProtoReflect() protoreflect.Message {
	mi := &file_cloud_visionai_v1_data_proto_msgTypes[2]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use NormalizedPolyline.ProtoReflect.Descriptor instead.
func (*NormalizedPolyline) Descriptor() ([]byte, []int) {
	return file_cloud_visionai_v1_data_proto_rawDescGZIP(), []int{2}
}

func (x *NormalizedPolyline) GetNormalizedVertices() []*NormalizedVertex {
	if x != nil {
		return x.NormalizedVertices
	}
	return nil
}

// A vertex represents a 2D point in the image.
// NOTE: the normalized vertex coordinates are relative to the original image
// and range from 0 to 1.
type NormalizedVertex struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// X coordinate.
	X float32 `protobuf:"fixed32,1,opt,name=x,proto3" json:"x,omitempty"`
	// Y coordinate.
	Y             float32 `protobuf:"fixed32,2,opt,name=y,proto3" json:"y,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *NormalizedVertex) Reset() {
	*x = NormalizedVertex{}
	mi := &file_cloud_visionai_v1_data_proto_msgTypes[3]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *NormalizedVertex) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*NormalizedVertex) ProtoMessage() {}

func (x *NormalizedVertex) ProtoReflect() protoreflect.Message {
	mi := &file_cloud_visionai_v1_data_proto_msgTypes[3]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use NormalizedVertex.ProtoReflect.Descriptor instead.
func (*NormalizedVertex) Descriptor() ([]byte, []int) {
	return file_cloud_visionai_v1_data_proto_rawDescGZIP(), []int{3}
}

func (x *NormalizedVertex) GetX() float32 {
	if x != nil {
		return x.X
	}
	return 0
}

func (x *NormalizedVertex) GetY() float32 {
	if x != nil {
		return x.Y
	}
	return 0
}

// Message describing the Cluster object.
type Cluster struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Output only. Name of the resource.
	Name string `protobuf:"bytes,1,opt,name=name,proto3" json:"name,omitempty"`
	// Output only. The create timestamp.
	CreateTime *timestamppb.Timestamp `protobuf:"bytes,2,opt,name=create_time,json=createTime,proto3" json:"create_time,omitempty"`
	// Output only. The update timestamp.
	UpdateTime *timestamppb.Timestamp `protobuf:"bytes,3,opt,name=update_time,json=updateTime,proto3" json:"update_time,omitempty"`
	// Labels as key value pairs
	Labels map[string]string `protobuf:"bytes,4,rep,name=labels,proto3" json:"labels,omitempty" protobuf_key:"bytes,1,opt,name=key" protobuf_val:"bytes,2,opt,name=value"`
	// Annotations to allow clients to store small amounts of arbitrary data.
	Annotations map[string]string `protobuf:"bytes,5,rep,name=annotations,proto3" json:"annotations,omitempty" protobuf_key:"bytes,1,opt,name=key" protobuf_val:"bytes,2,opt,name=value"`
	// Output only. The DNS name of the data plane service
	DataplaneServiceEndpoint string `protobuf:"bytes,6,opt,name=dataplane_service_endpoint,json=dataplaneServiceEndpoint,proto3" json:"dataplane_service_endpoint,omitempty"`
	// Output only. The current state of the cluster.
	State Cluster_State `protobuf:"varint,7,opt,name=state,proto3,enum=google.events.cloud.visionai.v1.Cluster_State" json:"state,omitempty"`
	// Output only. The private service connection service target name.
	PscTarget     string `protobuf:"bytes,8,opt,name=psc_target,json=pscTarget,proto3" json:"psc_target,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *Cluster) Reset() {
	*x = Cluster{}
	mi := &file_cloud_visionai_v1_data_proto_msgTypes[4]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *Cluster) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*Cluster) ProtoMessage() {}

func (x *Cluster) ProtoReflect() protoreflect.Message {
	mi := &file_cloud_visionai_v1_data_proto_msgTypes[4]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use Cluster.ProtoReflect.Descriptor instead.
func (*Cluster) Descriptor() ([]byte, []int) {
	return file_cloud_visionai_v1_data_proto_rawDescGZIP(), []int{4}
}

func (x *Cluster) GetName() string {
	if x != nil {
		return x.Name
	}
	return ""
}

func (x *Cluster) GetCreateTime() *timestamppb.Timestamp {
	if x != nil {
		return x.CreateTime
	}
	return nil
}

func (x *Cluster) GetUpdateTime() *timestamppb.Timestamp {
	if x != nil {
		return x.UpdateTime
	}
	return nil
}

func (x *Cluster) GetLabels() map[string]string {
	if x != nil {
		return x.Labels
	}
	return nil
}

func (x *Cluster) GetAnnotations() map[string]string {
	if x != nil {
		return x.Annotations
	}
	return nil
}

func (x *Cluster) GetDataplaneServiceEndpoint() string {
	if x != nil {
		return x.DataplaneServiceEndpoint
	}
	return ""
}

func (x *Cluster) GetState() Cluster_State {
	if x != nil {
		return x.State
	}
	return Cluster_STATE_UNSPECIFIED
}

func (x *Cluster) GetPscTarget() string {
	if x != nil {
		return x.PscTarget
	}
	return ""
}

// The Google Cloud Storage location for the input content.
type GcsSource struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Required. References to a Google Cloud Storage paths.
	Uris          []string `protobuf:"bytes,1,rep,name=uris,proto3" json:"uris,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *GcsSource) Reset() {
	*x = GcsSource{}
	mi := &file_cloud_visionai_v1_data_proto_msgTypes[5]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *GcsSource) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*GcsSource) ProtoMessage() {}

func (x *GcsSource) ProtoReflect() protoreflect.Message {
	mi := &file_cloud_visionai_v1_data_proto_msgTypes[5]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use GcsSource.ProtoReflect.Descriptor instead.
func (*GcsSource) Descriptor() ([]byte, []int) {
	return file_cloud_visionai_v1_data_proto_rawDescGZIP(), []int{5}
}

func (x *GcsSource) GetUris() []string {
	if x != nil {
		return x.Uris
	}
	return nil
}

// Represents an actual value of an operator attribute.
type AttributeValue struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Attribute value.
	//
	// Types that are valid to be assigned to Value:
	//
	//	*AttributeValue_I
	//	*AttributeValue_F
	//	*AttributeValue_B
	//	*AttributeValue_S
	Value         isAttributeValue_Value `protobuf_oneof:"value"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *AttributeValue) Reset() {
	*x = AttributeValue{}
	mi := &file_cloud_visionai_v1_data_proto_msgTypes[6]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *AttributeValue) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*AttributeValue) ProtoMessage() {}

func (x *AttributeValue) ProtoReflect() protoreflect.Message {
	mi := &file_cloud_visionai_v1_data_proto_msgTypes[6]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use AttributeValue.ProtoReflect.Descriptor instead.
func (*AttributeValue) Descriptor() ([]byte, []int) {
	return file_cloud_visionai_v1_data_proto_rawDescGZIP(), []int{6}
}

func (x *AttributeValue) GetValue() isAttributeValue_Value {
	if x != nil {
		return x.Value
	}
	return nil
}

func (x *AttributeValue) GetI() int64 {
	if x != nil {
		if x, ok := x.Value.(*AttributeValue_I); ok {
			return x.I
		}
	}
	return 0
}

func (x *AttributeValue) GetF() float32 {
	if x != nil {
		if x, ok := x.Value.(*AttributeValue_F); ok {
			return x.F
		}
	}
	return 0
}

func (x *AttributeValue) GetB() bool {
	if x != nil {
		if x, ok := x.Value.(*AttributeValue_B); ok {
			return x.B
		}
	}
	return false
}

func (x *AttributeValue) GetS() []byte {
	if x != nil {
		if x, ok := x.Value.(*AttributeValue_S); ok {
			return x.S
		}
	}
	return nil
}

type isAttributeValue_Value interface {
	isAttributeValue_Value()
}

type AttributeValue_I struct {
	// int.
	I int64 `protobuf:"varint,1,opt,name=i,proto3,oneof"`
}

type AttributeValue_F struct {
	// float.
	F float32 `protobuf:"fixed32,2,opt,name=f,proto3,oneof"`
}

type AttributeValue_B struct {
	// bool.
	B bool `protobuf:"varint,3,opt,name=b,proto3,oneof"`
}

type AttributeValue_S struct {
	// string.
	S []byte `protobuf:"bytes,4,opt,name=s,proto3,oneof"`
}

func (*AttributeValue_I) isAttributeValue_Value() {}

func (*AttributeValue_F) isAttributeValue_Value() {}

func (*AttributeValue_B) isAttributeValue_Value() {}

func (*AttributeValue_S) isAttributeValue_Value() {}

// Defines an Analyzer.
//
// An analyzer processes data from its input streams using the logic defined in
// the Operator that it represents. Of course, it produces data for the output
// streams declared in the Operator.
type AnalyzerDefinition struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// The name of this analyzer.
	//
	// Tentatively [a-z][a-z0-9]*(_[a-z0-9]+)*.
	Analyzer string `protobuf:"bytes,1,opt,name=analyzer,proto3" json:"analyzer,omitempty"`
	// The name of the operator that this analyzer runs.
	//
	// Must match the name of a supported operator.
	Operator string `protobuf:"bytes,2,opt,name=operator,proto3" json:"operator,omitempty"`
	// Input streams.
	Inputs []*AnalyzerDefinition_StreamInput `protobuf:"bytes,3,rep,name=inputs,proto3" json:"inputs,omitempty"`
	// The attribute values that this analyzer applies to the operator.
	//
	// Supply a mapping between the attribute names and the actual value you wish
	// to apply. If an attribute name is omitted, then it will take a
	// preconfigured default value.
	Attrs map[string]*AttributeValue `protobuf:"bytes,4,rep,name=attrs,proto3" json:"attrs,omitempty" protobuf_key:"bytes,1,opt,name=key" protobuf_val:"bytes,2,opt,name=value"`
	// Debug options.
	DebugOptions  *AnalyzerDefinition_DebugOptions `protobuf:"bytes,5,opt,name=debug_options,json=debugOptions,proto3" json:"debug_options,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *AnalyzerDefinition) Reset() {
	*x = AnalyzerDefinition{}
	mi := &file_cloud_visionai_v1_data_proto_msgTypes[7]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *AnalyzerDefinition) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*AnalyzerDefinition) ProtoMessage() {}

func (x *AnalyzerDefinition) ProtoReflect() protoreflect.Message {
	mi := &file_cloud_visionai_v1_data_proto_msgTypes[7]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use AnalyzerDefinition.ProtoReflect.Descriptor instead.
func (*AnalyzerDefinition) Descriptor() ([]byte, []int) {
	return file_cloud_visionai_v1_data_proto_rawDescGZIP(), []int{7}
}

func (x *AnalyzerDefinition) GetAnalyzer() string {
	if x != nil {
		return x.Analyzer
	}
	return ""
}

func (x *AnalyzerDefinition) GetOperator() string {
	if x != nil {
		return x.Operator
	}
	return ""
}

func (x *AnalyzerDefinition) GetInputs() []*AnalyzerDefinition_StreamInput {
	if x != nil {
		return x.Inputs
	}
	return nil
}

func (x *AnalyzerDefinition) GetAttrs() map[string]*AttributeValue {
	if x != nil {
		return x.Attrs
	}
	return nil
}

func (x *AnalyzerDefinition) GetDebugOptions() *AnalyzerDefinition_DebugOptions {
	if x != nil {
		return x.DebugOptions
	}
	return nil
}

// Defines a full analysis.
//
// This is a description of the overall live analytics pipeline.
// You may think of this as an edge list representation of a multigraph.
//
// This may be directly authored by a human in protobuf textformat, or it may be
// generated by a programming API (perhaps Python or JavaScript depending on
// context).
type AnalysisDefinition struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Analyzer definitions.
	Analyzers     []*AnalyzerDefinition `protobuf:"bytes,1,rep,name=analyzers,proto3" json:"analyzers,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *AnalysisDefinition) Reset() {
	*x = AnalysisDefinition{}
	mi := &file_cloud_visionai_v1_data_proto_msgTypes[8]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *AnalysisDefinition) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*AnalysisDefinition) ProtoMessage() {}

func (x *AnalysisDefinition) ProtoReflect() protoreflect.Message {
	mi := &file_cloud_visionai_v1_data_proto_msgTypes[8]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use AnalysisDefinition.ProtoReflect.Descriptor instead.
func (*AnalysisDefinition) Descriptor() ([]byte, []int) {
	return file_cloud_visionai_v1_data_proto_rawDescGZIP(), []int{8}
}

func (x *AnalysisDefinition) GetAnalyzers() []*AnalyzerDefinition {
	if x != nil {
		return x.Analyzers
	}
	return nil
}

// Message describing the status of the Process.
type RunStatus struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// The state of the Process.
	State RunStatus_State `protobuf:"varint,1,opt,name=state,proto3,enum=google.events.cloud.visionai.v1.RunStatus_State" json:"state,omitempty"`
	// The reason of becoming the state.
	Reason        string `protobuf:"bytes,2,opt,name=reason,proto3" json:"reason,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *RunStatus) Reset() {
	*x = RunStatus{}
	mi := &file_cloud_visionai_v1_data_proto_msgTypes[9]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *RunStatus) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*RunStatus) ProtoMessage() {}

func (x *RunStatus) ProtoReflect() protoreflect.Message {
	mi := &file_cloud_visionai_v1_data_proto_msgTypes[9]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use RunStatus.ProtoReflect.Descriptor instead.
func (*RunStatus) Descriptor() ([]byte, []int) {
	return file_cloud_visionai_v1_data_proto_rawDescGZIP(), []int{9}
}

func (x *RunStatus) GetState() RunStatus_State {
	if x != nil {
		return x.State
	}
	return RunStatus_STATE_UNSPECIFIED
}

func (x *RunStatus) GetReason() string {
	if x != nil {
		return x.Reason
	}
	return ""
}

// Message describing the Analysis object.
type Analysis struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// The name of resource.
	Name string `protobuf:"bytes,1,opt,name=name,proto3" json:"name,omitempty"`
	// Output only. The create timestamp.
	CreateTime *timestamppb.Timestamp `protobuf:"bytes,2,opt,name=create_time,json=createTime,proto3" json:"create_time,omitempty"`
	// Output only. The update timestamp.
	UpdateTime *timestamppb.Timestamp `protobuf:"bytes,3,opt,name=update_time,json=updateTime,proto3" json:"update_time,omitempty"`
	// Labels as key value pairs.
	Labels map[string]string `protobuf:"bytes,4,rep,name=labels,proto3" json:"labels,omitempty" protobuf_key:"bytes,1,opt,name=key" protobuf_val:"bytes,2,opt,name=value"`
	// The definition of the analysis.
	AnalysisDefinition *AnalysisDefinition `protobuf:"bytes,5,opt,name=analysis_definition,json=analysisDefinition,proto3" json:"analysis_definition,omitempty"`
	// Map from the input parameter in the definition to the real stream.
	// E.g., suppose you had a stream source operator named "input-0" and you try
	// to receive from the real stream "stream-0". You can add the following
	// mapping: [input-0: stream-0].
	InputStreamsMapping map[string]string `protobuf:"bytes,6,rep,name=input_streams_mapping,json=inputStreamsMapping,proto3" json:"input_streams_mapping,omitempty" protobuf_key:"bytes,1,opt,name=key" protobuf_val:"bytes,2,opt,name=value"`
	// Map from the output parameter in the definition to the real stream.
	// E.g., suppose you had a stream sink operator named "output-0" and you try
	// to send to the real stream "stream-0". You can add the following
	// mapping: [output-0: stream-0].
	OutputStreamsMapping map[string]string `protobuf:"bytes,7,rep,name=output_streams_mapping,json=outputStreamsMapping,proto3" json:"output_streams_mapping,omitempty" protobuf_key:"bytes,1,opt,name=key" protobuf_val:"bytes,2,opt,name=value"`
	// Boolean flag to indicate whether you would like to disable the ability
	// to automatically start a Process when new event happening in the input
	// Stream. If you would like to start a Process manually, the field needs
	// to be set to true.
	DisableEventWatch bool `protobuf:"varint,8,opt,name=disable_event_watch,json=disableEventWatch,proto3" json:"disable_event_watch,omitempty"`
	unknownFields     protoimpl.UnknownFields
	sizeCache         protoimpl.SizeCache
}

func (x *Analysis) Reset() {
	*x = Analysis{}
	mi := &file_cloud_visionai_v1_data_proto_msgTypes[10]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *Analysis) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*Analysis) ProtoMessage() {}

func (x *Analysis) ProtoReflect() protoreflect.Message {
	mi := &file_cloud_visionai_v1_data_proto_msgTypes[10]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use Analysis.ProtoReflect.Descriptor instead.
func (*Analysis) Descriptor() ([]byte, []int) {
	return file_cloud_visionai_v1_data_proto_rawDescGZIP(), []int{10}
}

func (x *Analysis) GetName() string {
	if x != nil {
		return x.Name
	}
	return ""
}

func (x *Analysis) GetCreateTime() *timestamppb.Timestamp {
	if x != nil {
		return x.CreateTime
	}
	return nil
}

func (x *Analysis) GetUpdateTime() *timestamppb.Timestamp {
	if x != nil {
		return x.UpdateTime
	}
	return nil
}

func (x *Analysis) GetLabels() map[string]string {
	if x != nil {
		return x.Labels
	}
	return nil
}

func (x *Analysis) GetAnalysisDefinition() *AnalysisDefinition {
	if x != nil {
		return x.AnalysisDefinition
	}
	return nil
}

func (x *Analysis) GetInputStreamsMapping() map[string]string {
	if x != nil {
		return x.InputStreamsMapping
	}
	return nil
}

func (x *Analysis) GetOutputStreamsMapping() map[string]string {
	if x != nil {
		return x.OutputStreamsMapping
	}
	return nil
}

func (x *Analysis) GetDisableEventWatch() bool {
	if x != nil {
		return x.DisableEventWatch
	}
	return false
}

// Message describing the Process object.
type Process struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// The name of resource.
	Name string `protobuf:"bytes,1,opt,name=name,proto3" json:"name,omitempty"`
	// Output only. The create timestamp.
	CreateTime *timestamppb.Timestamp `protobuf:"bytes,2,opt,name=create_time,json=createTime,proto3" json:"create_time,omitempty"`
	// Output only. The update timestamp.
	UpdateTime *timestamppb.Timestamp `protobuf:"bytes,3,opt,name=update_time,json=updateTime,proto3" json:"update_time,omitempty"`
	// Required. Reference to an existing Analysis resource.
	Analysis string `protobuf:"bytes,4,opt,name=analysis,proto3" json:"analysis,omitempty"`
	// Optional. Attribute overrides of the Analyzers.
	// Format for each single override item:
	// "{analyzer_name}:{attribute_key}={value}"
	AttributeOverrides []string `protobuf:"bytes,5,rep,name=attribute_overrides,json=attributeOverrides,proto3" json:"attribute_overrides,omitempty"`
	// Optional. Status of the Process.
	RunStatus *RunStatus `protobuf:"bytes,6,opt,name=run_status,json=runStatus,proto3" json:"run_status,omitempty"`
	// Optional. Run mode of the Process.
	RunMode RunMode `protobuf:"varint,7,opt,name=run_mode,json=runMode,proto3,enum=google.events.cloud.visionai.v1.RunMode" json:"run_mode,omitempty"`
	// Optional. Event ID of the input/output streams.
	// This is useful when you have a StreamSource/StreamSink operator in the
	// Analysis, and you want to manually specify the Event to read from/write to.
	EventId string `protobuf:"bytes,8,opt,name=event_id,json=eventId,proto3" json:"event_id,omitempty"`
	// Optional. Optional: Batch ID of the Process.
	BatchId string `protobuf:"bytes,9,opt,name=batch_id,json=batchId,proto3" json:"batch_id,omitempty"`
	// Optional. Optional: The number of retries for a process in submission mode
	// the system should try before declaring failure. By default, no retry will
	// be performed.
	RetryCount    int32 `protobuf:"varint,10,opt,name=retry_count,json=retryCount,proto3" json:"retry_count,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *Process) Reset() {
	*x = Process{}
	mi := &file_cloud_visionai_v1_data_proto_msgTypes[11]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *Process) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*Process) ProtoMessage() {}

func (x *Process) ProtoReflect() protoreflect.Message {
	mi := &file_cloud_visionai_v1_data_proto_msgTypes[11]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use Process.ProtoReflect.Descriptor instead.
func (*Process) Descriptor() ([]byte, []int) {
	return file_cloud_visionai_v1_data_proto_rawDescGZIP(), []int{11}
}

func (x *Process) GetName() string {
	if x != nil {
		return x.Name
	}
	return ""
}

func (x *Process) GetCreateTime() *timestamppb.Timestamp {
	if x != nil {
		return x.CreateTime
	}
	return nil
}

func (x *Process) GetUpdateTime() *timestamppb.Timestamp {
	if x != nil {
		return x.UpdateTime
	}
	return nil
}

func (x *Process) GetAnalysis() string {
	if x != nil {
		return x.Analysis
	}
	return ""
}

func (x *Process) GetAttributeOverrides() []string {
	if x != nil {
		return x.AttributeOverrides
	}
	return nil
}

func (x *Process) GetRunStatus() *RunStatus {
	if x != nil {
		return x.RunStatus
	}
	return nil
}

func (x *Process) GetRunMode() RunMode {
	if x != nil {
		return x.RunMode
	}
	return RunMode_RUN_MODE_UNSPECIFIED
}

func (x *Process) GetEventId() string {
	if x != nil {
		return x.EventId
	}
	return ""
}

func (x *Process) GetBatchId() string {
	if x != nil {
		return x.BatchId
	}
	return ""
}

func (x *Process) GetRetryCount() int32 {
	if x != nil {
		return x.RetryCount
	}
	return 0
}

// Message describing Application object
type Application struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// name of resource
	Name string `protobuf:"bytes,1,opt,name=name,proto3" json:"name,omitempty"`
	// Output only. [Output only] Create timestamp
	CreateTime *timestamppb.Timestamp `protobuf:"bytes,2,opt,name=create_time,json=createTime,proto3" json:"create_time,omitempty"`
	// Output only. [Output only] Update timestamp
	UpdateTime *timestamppb.Timestamp `protobuf:"bytes,3,opt,name=update_time,json=updateTime,proto3" json:"update_time,omitempty"`
	// Labels as key value pairs
	Labels map[string]string `protobuf:"bytes,4,rep,name=labels,proto3" json:"labels,omitempty" protobuf_key:"bytes,1,opt,name=key" protobuf_val:"bytes,2,opt,name=value"`
	// Required. A user friendly display name for the solution.
	DisplayName string `protobuf:"bytes,5,opt,name=display_name,json=displayName,proto3" json:"display_name,omitempty"`
	// A description for this application.
	Description string `protobuf:"bytes,6,opt,name=description,proto3" json:"description,omitempty"`
	// Application graph configuration.
	ApplicationConfigs *ApplicationConfigs `protobuf:"bytes,7,opt,name=application_configs,json=applicationConfigs,proto3" json:"application_configs,omitempty"`
	// Output only. Application graph runtime info. Only exists when application
	// state equals to DEPLOYED.
	RuntimeInfo *Application_ApplicationRuntimeInfo `protobuf:"bytes,8,opt,name=runtime_info,json=runtimeInfo,proto3" json:"runtime_info,omitempty"`
	// Output only. State of the application.
	State Application_State `protobuf:"varint,9,opt,name=state,proto3,enum=google.events.cloud.visionai.v1.Application_State" json:"state,omitempty"`
	// Billing mode of the application.
	BillingMode   Application_BillingMode `protobuf:"varint,12,opt,name=billing_mode,json=billingMode,proto3,enum=google.events.cloud.visionai.v1.Application_BillingMode" json:"billing_mode,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *Application) Reset() {
	*x = Application{}
	mi := &file_cloud_visionai_v1_data_proto_msgTypes[12]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *Application) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*Application) ProtoMessage() {}

func (x *Application) ProtoReflect() protoreflect.Message {
	mi := &file_cloud_visionai_v1_data_proto_msgTypes[12]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use Application.ProtoReflect.Descriptor instead.
func (*Application) Descriptor() ([]byte, []int) {
	return file_cloud_visionai_v1_data_proto_rawDescGZIP(), []int{12}
}

func (x *Application) GetName() string {
	if x != nil {
		return x.Name
	}
	return ""
}

func (x *Application) GetCreateTime() *timestamppb.Timestamp {
	if x != nil {
		return x.CreateTime
	}
	return nil
}

func (x *Application) GetUpdateTime() *timestamppb.Timestamp {
	if x != nil {
		return x.UpdateTime
	}
	return nil
}

func (x *Application) GetLabels() map[string]string {
	if x != nil {
		return x.Labels
	}
	return nil
}

func (x *Application) GetDisplayName() string {
	if x != nil {
		return x.DisplayName
	}
	return ""
}

func (x *Application) GetDescription() string {
	if x != nil {
		return x.Description
	}
	return ""
}

func (x *Application) GetApplicationConfigs() *ApplicationConfigs {
	if x != nil {
		return x.ApplicationConfigs
	}
	return nil
}

func (x *Application) GetRuntimeInfo() *Application_ApplicationRuntimeInfo {
	if x != nil {
		return x.RuntimeInfo
	}
	return nil
}

func (x *Application) GetState() Application_State {
	if x != nil {
		return x.State
	}
	return Application_STATE_UNSPECIFIED
}

func (x *Application) GetBillingMode() Application_BillingMode {
	if x != nil {
		return x.BillingMode
	}
	return Application_BILLING_MODE_UNSPECIFIED
}

// Message storing the graph of the application.
type ApplicationConfigs struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// A list of nodes  in the application graph.
	Nodes         []*Node `protobuf:"bytes,1,rep,name=nodes,proto3" json:"nodes,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *ApplicationConfigs) Reset() {
	*x = ApplicationConfigs{}
	mi := &file_cloud_visionai_v1_data_proto_msgTypes[13]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *ApplicationConfigs) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*ApplicationConfigs) ProtoMessage() {}

func (x *ApplicationConfigs) ProtoReflect() protoreflect.Message {
	mi := &file_cloud_visionai_v1_data_proto_msgTypes[13]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use ApplicationConfigs.ProtoReflect.Descriptor instead.
func (*ApplicationConfigs) Descriptor() ([]byte, []int) {
	return file_cloud_visionai_v1_data_proto_rawDescGZIP(), []int{13}
}

func (x *ApplicationConfigs) GetNodes() []*Node {
	if x != nil {
		return x.Nodes
	}
	return nil
}

// Message describing node object.
type Node struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Types that are valid to be assigned to StreamOutputConfig:
	//
	//	*Node_OutputAllOutputChannelsToStream
	StreamOutputConfig isNode_StreamOutputConfig `protobuf_oneof:"stream_output_config"`
	// Required. A unique name for the node.
	Name string `protobuf:"bytes,1,opt,name=name,proto3" json:"name,omitempty"`
	// A user friendly display name for the node.
	DisplayName string `protobuf:"bytes,2,opt,name=display_name,json=displayName,proto3" json:"display_name,omitempty"`
	// Node config.
	NodeConfig *ProcessorConfig `protobuf:"bytes,3,opt,name=node_config,json=nodeConfig,proto3" json:"node_config,omitempty"`
	// Processor name refer to the chosen processor resource.
	Processor string `protobuf:"bytes,4,opt,name=processor,proto3" json:"processor,omitempty"`
	// Parent node. Input node should not have parent node. For V1 Alpha1/Beta
	// only media warehouse node can have multiple parents, other types of nodes
	// will only have one parent.
	Parents       []*Node_InputEdge `protobuf:"bytes,5,rep,name=parents,proto3" json:"parents,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *Node) Reset() {
	*x = Node{}
	mi := &file_cloud_visionai_v1_data_proto_msgTypes[14]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *Node) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*Node) ProtoMessage() {}

func (x *Node) ProtoReflect() protoreflect.Message {
	mi := &file_cloud_visionai_v1_data_proto_msgTypes[14]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use Node.ProtoReflect.Descriptor instead.
func (*Node) Descriptor() ([]byte, []int) {
	return file_cloud_visionai_v1_data_proto_rawDescGZIP(), []int{14}
}

func (x *Node) GetStreamOutputConfig() isNode_StreamOutputConfig {
	if x != nil {
		return x.StreamOutputConfig
	}
	return nil
}

func (x *Node) GetOutputAllOutputChannelsToStream() bool {
	if x != nil {
		if x, ok := x.StreamOutputConfig.(*Node_OutputAllOutputChannelsToStream); ok {
			return x.OutputAllOutputChannelsToStream
		}
	}
	return false
}

func (x *Node) GetName() string {
	if x != nil {
		return x.Name
	}
	return ""
}

func (x *Node) GetDisplayName() string {
	if x != nil {
		return x.DisplayName
	}
	return ""
}

func (x *Node) GetNodeConfig() *ProcessorConfig {
	if x != nil {
		return x.NodeConfig
	}
	return nil
}

func (x *Node) GetProcessor() string {
	if x != nil {
		return x.Processor
	}
	return ""
}

func (x *Node) GetParents() []*Node_InputEdge {
	if x != nil {
		return x.Parents
	}
	return nil
}

type isNode_StreamOutputConfig interface {
	isNode_StreamOutputConfig()
}

type Node_OutputAllOutputChannelsToStream struct {
	// By default, the output of the node will only be available to downstream
	// nodes. To consume the direct output from the application node, the output
	// must be sent to Vision AI Streams at first.
	//
	// By setting output_all_output_channels_to_stream to true, App Platform
	// will automatically send all the outputs of the current node to Vision AI
	// Stream resources (one stream per output channel). The output stream
	// resource will be created by App Platform automatically during deployment
	// and deleted after application un-deployment.
	// Note that this config applies to all the Application Instances.
	//
	// The output stream can be override at instance level by
	// configuring the `output_resources` section of Instance resource.
	// `producer_node` should be current node, `output_resource_binding` should
	// be the output channel name (or leave it blank if there is only 1 output
	// channel of the processor) and `output_resource` should be the target
	// output stream.
	OutputAllOutputChannelsToStream bool `protobuf:"varint,6,opt,name=output_all_output_channels_to_stream,json=outputAllOutputChannelsToStream,proto3,oneof"`
}

func (*Node_OutputAllOutputChannelsToStream) isNode_StreamOutputConfig() {}

// Message describing Draft object
type Draft struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// name of resource
	Name string `protobuf:"bytes,1,opt,name=name,proto3" json:"name,omitempty"`
	// Output only. [Output only] Create timestamp
	CreateTime *timestamppb.Timestamp `protobuf:"bytes,2,opt,name=create_time,json=createTime,proto3" json:"create_time,omitempty"`
	// Output only. [Output only] Create timestamp
	UpdateTime *timestamppb.Timestamp `protobuf:"bytes,7,opt,name=update_time,json=updateTime,proto3" json:"update_time,omitempty"`
	// Labels as key value pairs
	Labels map[string]string `protobuf:"bytes,3,rep,name=labels,proto3" json:"labels,omitempty" protobuf_key:"bytes,1,opt,name=key" protobuf_val:"bytes,2,opt,name=value"`
	// Required. A user friendly display name for the solution.
	DisplayName string `protobuf:"bytes,4,opt,name=display_name,json=displayName,proto3" json:"display_name,omitempty"`
	// A description for this application.
	Description string `protobuf:"bytes,5,opt,name=description,proto3" json:"description,omitempty"`
	// The draft application configs which haven't been updated to an application.
	DraftApplicationConfigs *ApplicationConfigs `protobuf:"bytes,6,opt,name=draft_application_configs,json=draftApplicationConfigs,proto3" json:"draft_application_configs,omitempty"`
	unknownFields           protoimpl.UnknownFields
	sizeCache               protoimpl.SizeCache
}

func (x *Draft) Reset() {
	*x = Draft{}
	mi := &file_cloud_visionai_v1_data_proto_msgTypes[15]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *Draft) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*Draft) ProtoMessage() {}

func (x *Draft) ProtoReflect() protoreflect.Message {
	mi := &file_cloud_visionai_v1_data_proto_msgTypes[15]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use Draft.ProtoReflect.Descriptor instead.
func (*Draft) Descriptor() ([]byte, []int) {
	return file_cloud_visionai_v1_data_proto_rawDescGZIP(), []int{15}
}

func (x *Draft) GetName() string {
	if x != nil {
		return x.Name
	}
	return ""
}

func (x *Draft) GetCreateTime() *timestamppb.Timestamp {
	if x != nil {
		return x.CreateTime
	}
	return nil
}

func (x *Draft) GetUpdateTime() *timestamppb.Timestamp {
	if x != nil {
		return x.UpdateTime
	}
	return nil
}

func (x *Draft) GetLabels() map[string]string {
	if x != nil {
		return x.Labels
	}
	return nil
}

func (x *Draft) GetDisplayName() string {
	if x != nil {
		return x.DisplayName
	}
	return ""
}

func (x *Draft) GetDescription() string {
	if x != nil {
		return x.Description
	}
	return ""
}

func (x *Draft) GetDraftApplicationConfigs() *ApplicationConfigs {
	if x != nil {
		return x.DraftApplicationConfigs
	}
	return nil
}

// Message describing Processor object.
// Next ID: 19
type Processor struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// name of resource.
	Name string `protobuf:"bytes,1,opt,name=name,proto3" json:"name,omitempty"`
	// Output only. [Output only] Create timestamp.
	CreateTime *timestamppb.Timestamp `protobuf:"bytes,2,opt,name=create_time,json=createTime,proto3" json:"create_time,omitempty"`
	// Output only. [Output only] Update timestamp.
	UpdateTime *timestamppb.Timestamp `protobuf:"bytes,3,opt,name=update_time,json=updateTime,proto3" json:"update_time,omitempty"`
	// Labels as key value pairs.
	Labels map[string]string `protobuf:"bytes,4,rep,name=labels,proto3" json:"labels,omitempty" protobuf_key:"bytes,1,opt,name=key" protobuf_val:"bytes,2,opt,name=value"`
	// Required. A user friendly display name for the processor.
	DisplayName string `protobuf:"bytes,5,opt,name=display_name,json=displayName,proto3" json:"display_name,omitempty"`
	// Illustrative sentences for describing the functionality of the processor.
	Description string `protobuf:"bytes,10,opt,name=description,proto3" json:"description,omitempty"`
	// Output only. Processor Type.
	ProcessorType Processor_ProcessorType `protobuf:"varint,6,opt,name=processor_type,json=processorType,proto3,enum=google.events.cloud.visionai.v1.Processor_ProcessorType" json:"processor_type,omitempty"`
	// Model Type.
	ModelType ModelType `protobuf:"varint,13,opt,name=model_type,json=modelType,proto3,enum=google.events.cloud.visionai.v1.ModelType" json:"model_type,omitempty"`
	// Source info for customer created processor.
	CustomProcessorSourceInfo *CustomProcessorSourceInfo `protobuf:"bytes,7,opt,name=custom_processor_source_info,json=customProcessorSourceInfo,proto3" json:"custom_processor_source_info,omitempty"`
	// Output only. State of the Processor.
	State Processor_ProcessorState `protobuf:"varint,8,opt,name=state,proto3,enum=google.events.cloud.visionai.v1.Processor_ProcessorState" json:"state,omitempty"`
	// Output only. [Output only] The input / output specifications of a
	// processor, each type of processor has fixed input / output specs which
	// cannot be altered by customer.
	ProcessorIoSpec *ProcessorIOSpec `protobuf:"bytes,11,opt,name=processor_io_spec,json=processorIoSpec,proto3" json:"processor_io_spec,omitempty"`
	// Output only. The corresponding configuration can be used in the Application
	// to customize the behavior of the processor.
	ConfigurationTypeurl     string                 `protobuf:"bytes,14,opt,name=configuration_typeurl,json=configurationTypeurl,proto3" json:"configuration_typeurl,omitempty"`
	SupportedAnnotationTypes []StreamAnnotationType `protobuf:"varint,15,rep,packed,name=supported_annotation_types,json=supportedAnnotationTypes,proto3,enum=google.events.cloud.visionai.v1.StreamAnnotationType" json:"supported_annotation_types,omitempty"`
	// Indicates if the processor supports post processing.
	SupportsPostProcessing bool `protobuf:"varint,17,opt,name=supports_post_processing,json=supportsPostProcessing,proto3" json:"supports_post_processing,omitempty"`
	unknownFields          protoimpl.UnknownFields
	sizeCache              protoimpl.SizeCache
}

func (x *Processor) Reset() {
	*x = Processor{}
	mi := &file_cloud_visionai_v1_data_proto_msgTypes[16]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *Processor) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*Processor) ProtoMessage() {}

func (x *Processor) ProtoReflect() protoreflect.Message {
	mi := &file_cloud_visionai_v1_data_proto_msgTypes[16]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use Processor.ProtoReflect.Descriptor instead.
func (*Processor) Descriptor() ([]byte, []int) {
	return file_cloud_visionai_v1_data_proto_rawDescGZIP(), []int{16}
}

func (x *Processor) GetName() string {
	if x != nil {
		return x.Name
	}
	return ""
}

func (x *Processor) GetCreateTime() *timestamppb.Timestamp {
	if x != nil {
		return x.CreateTime
	}
	return nil
}

func (x *Processor) GetUpdateTime() *timestamppb.Timestamp {
	if x != nil {
		return x.UpdateTime
	}
	return nil
}

func (x *Processor) GetLabels() map[string]string {
	if x != nil {
		return x.Labels
	}
	return nil
}

func (x *Processor) GetDisplayName() string {
	if x != nil {
		return x.DisplayName
	}
	return ""
}

func (x *Processor) GetDescription() string {
	if x != nil {
		return x.Description
	}
	return ""
}

func (x *Processor) GetProcessorType() Processor_ProcessorType {
	if x != nil {
		return x.ProcessorType
	}
	return Processor_PROCESSOR_TYPE_UNSPECIFIED
}

func (x *Processor) GetModelType() ModelType {
	if x != nil {
		return x.ModelType
	}
	return ModelType_MODEL_TYPE_UNSPECIFIED
}

func (x *Processor) GetCustomProcessorSourceInfo() *CustomProcessorSourceInfo {
	if x != nil {
		return x.CustomProcessorSourceInfo
	}
	return nil
}

func (x *Processor) GetState() Processor_ProcessorState {
	if x != nil {
		return x.State
	}
	return Processor_PROCESSOR_STATE_UNSPECIFIED
}

func (x *Processor) GetProcessorIoSpec() *ProcessorIOSpec {
	if x != nil {
		return x.ProcessorIoSpec
	}
	return nil
}

func (x *Processor) GetConfigurationTypeurl() string {
	if x != nil {
		return x.ConfigurationTypeurl
	}
	return ""
}

func (x *Processor) GetSupportedAnnotationTypes() []StreamAnnotationType {
	if x != nil {
		return x.SupportedAnnotationTypes
	}
	return nil
}

func (x *Processor) GetSupportsPostProcessing() bool {
	if x != nil {
		return x.SupportsPostProcessing
	}
	return false
}

// Message describing the input / output specifications of a processor.
type ProcessorIOSpec struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// For processors with input_channel_specs, the processor must be explicitly
	// connected to another processor.
	GraphInputChannelSpecs []*ProcessorIOSpec_GraphInputChannelSpec `protobuf:"bytes,3,rep,name=graph_input_channel_specs,json=graphInputChannelSpecs,proto3" json:"graph_input_channel_specs,omitempty"`
	// The output artifact specifications for the current processor.
	GraphOutputChannelSpecs []*ProcessorIOSpec_GraphOutputChannelSpec `protobuf:"bytes,4,rep,name=graph_output_channel_specs,json=graphOutputChannelSpecs,proto3" json:"graph_output_channel_specs,omitempty"`
	// The input resource that needs to be fed from the application instance.
	InstanceResourceInputBindingSpecs []*ProcessorIOSpec_InstanceResourceInputBindingSpec `protobuf:"bytes,5,rep,name=instance_resource_input_binding_specs,json=instanceResourceInputBindingSpecs,proto3" json:"instance_resource_input_binding_specs,omitempty"`
	// The output resource that the processor will generate per instance.
	// Other than the explicitly listed output bindings here, all the processors'
	// GraphOutputChannels can be binded to stream resource. The bind name then is
	// the same as the GraphOutputChannel's name.
	InstanceResourceOutputBindingSpecs []*ProcessorIOSpec_InstanceResourceOutputBindingSpec `protobuf:"bytes,6,rep,name=instance_resource_output_binding_specs,json=instanceResourceOutputBindingSpecs,proto3" json:"instance_resource_output_binding_specs,omitempty"`
	unknownFields                      protoimpl.UnknownFields
	sizeCache                          protoimpl.SizeCache
}

func (x *ProcessorIOSpec) Reset() {
	*x = ProcessorIOSpec{}
	mi := &file_cloud_visionai_v1_data_proto_msgTypes[17]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *ProcessorIOSpec) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*ProcessorIOSpec) ProtoMessage() {}

func (x *ProcessorIOSpec) ProtoReflect() protoreflect.Message {
	mi := &file_cloud_visionai_v1_data_proto_msgTypes[17]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use ProcessorIOSpec.ProtoReflect.Descriptor instead.
func (*ProcessorIOSpec) Descriptor() ([]byte, []int) {
	return file_cloud_visionai_v1_data_proto_rawDescGZIP(), []int{17}
}

func (x *ProcessorIOSpec) GetGraphInputChannelSpecs() []*ProcessorIOSpec_GraphInputChannelSpec {
	if x != nil {
		return x.GraphInputChannelSpecs
	}
	return nil
}

func (x *ProcessorIOSpec) GetGraphOutputChannelSpecs() []*ProcessorIOSpec_GraphOutputChannelSpec {
	if x != nil {
		return x.GraphOutputChannelSpecs
	}
	return nil
}

func (x *ProcessorIOSpec) GetInstanceResourceInputBindingSpecs() []*ProcessorIOSpec_InstanceResourceInputBindingSpec {
	if x != nil {
		return x.InstanceResourceInputBindingSpecs
	}
	return nil
}

func (x *ProcessorIOSpec) GetInstanceResourceOutputBindingSpecs() []*ProcessorIOSpec_InstanceResourceOutputBindingSpec {
	if x != nil {
		return x.InstanceResourceOutputBindingSpecs
	}
	return nil
}

// Describes the source info for a custom processor.
type CustomProcessorSourceInfo struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// The path where App Platform loads the artifacts for the custom processor.
	//
	// Types that are valid to be assigned to ArtifactPath:
	//
	//	*CustomProcessorSourceInfo_VertexModel
	ArtifactPath isCustomProcessorSourceInfo_ArtifactPath `protobuf_oneof:"artifact_path"`
	// The original product which holds the custom processor's functionality.
	SourceType CustomProcessorSourceInfo_SourceType `protobuf:"varint,1,opt,name=source_type,json=sourceType,proto3,enum=google.events.cloud.visionai.v1.CustomProcessorSourceInfo_SourceType" json:"source_type,omitempty"`
	// Output only. Additional info related to the imported custom processor.
	// Data is filled in by app platform during the processor creation.
	AdditionalInfo map[string]string `protobuf:"bytes,4,rep,name=additional_info,json=additionalInfo,proto3" json:"additional_info,omitempty" protobuf_key:"bytes,1,opt,name=key" protobuf_val:"bytes,2,opt,name=value"`
	// Model schema files which specifies the signature of the model.
	// For VERTEX_CUSTOM models, instances schema is required.
	// If instances schema is not specified during the processor creation,
	// VisionAI Platform will try to get it from Vertex, if it doesn't exist, the
	// creation will fail.
	ModelSchema   *CustomProcessorSourceInfo_ModelSchema `protobuf:"bytes,5,opt,name=model_schema,json=modelSchema,proto3" json:"model_schema,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *CustomProcessorSourceInfo) Reset() {
	*x = CustomProcessorSourceInfo{}
	mi := &file_cloud_visionai_v1_data_proto_msgTypes[18]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *CustomProcessorSourceInfo) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*CustomProcessorSourceInfo) ProtoMessage() {}

func (x *CustomProcessorSourceInfo) ProtoReflect() protoreflect.Message {
	mi := &file_cloud_visionai_v1_data_proto_msgTypes[18]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use CustomProcessorSourceInfo.ProtoReflect.Descriptor instead.
func (*CustomProcessorSourceInfo) Descriptor() ([]byte, []int) {
	return file_cloud_visionai_v1_data_proto_rawDescGZIP(), []int{18}
}

func (x *CustomProcessorSourceInfo) GetArtifactPath() isCustomProcessorSourceInfo_ArtifactPath {
	if x != nil {
		return x.ArtifactPath
	}
	return nil
}

func (x *CustomProcessorSourceInfo) GetVertexModel() string {
	if x != nil {
		if x, ok := x.ArtifactPath.(*CustomProcessorSourceInfo_VertexModel); ok {
			return x.VertexModel
		}
	}
	return ""
}

func (x *CustomProcessorSourceInfo) GetSourceType() CustomProcessorSourceInfo_SourceType {
	if x != nil {
		return x.SourceType
	}
	return CustomProcessorSourceInfo_SOURCE_TYPE_UNSPECIFIED
}

func (x *CustomProcessorSourceInfo) GetAdditionalInfo() map[string]string {
	if x != nil {
		return x.AdditionalInfo
	}
	return nil
}

func (x *CustomProcessorSourceInfo) GetModelSchema() *CustomProcessorSourceInfo_ModelSchema {
	if x != nil {
		return x.ModelSchema
	}
	return nil
}

type isCustomProcessorSourceInfo_ArtifactPath interface {
	isCustomProcessorSourceInfo_ArtifactPath()
}

type CustomProcessorSourceInfo_VertexModel struct {
	// The resource name original model hosted in the vertex AI platform.
	VertexModel string `protobuf:"bytes,2,opt,name=vertex_model,json=vertexModel,proto3,oneof"`
}

func (*CustomProcessorSourceInfo_VertexModel) isCustomProcessorSourceInfo_ArtifactPath() {}

// Next ID: 29
type ProcessorConfig struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Types that are valid to be assigned to ProcessorConfig:
	//
	//	*ProcessorConfig_VideoStreamInputConfig
	//	*ProcessorConfig_AiEnabledDevicesInputConfig
	//	*ProcessorConfig_MediaWarehouseConfig
	//	*ProcessorConfig_PersonBlurConfig
	//	*ProcessorConfig_OccupancyCountConfig
	//	*ProcessorConfig_PersonVehicleDetectionConfig
	//	*ProcessorConfig_VertexAutomlVisionConfig
	//	*ProcessorConfig_VertexAutomlVideoConfig
	//	*ProcessorConfig_VertexCustomConfig
	//	*ProcessorConfig_GeneralObjectDetectionConfig
	//	*ProcessorConfig_BigQueryConfig
	//	*ProcessorConfig_PersonalProtectiveEquipmentDetectionConfig
	ProcessorConfig isProcessorConfig_ProcessorConfig `protobuf_oneof:"processor_config"`
	unknownFields   protoimpl.UnknownFields
	sizeCache       protoimpl.SizeCache
}

func (x *ProcessorConfig) Reset() {
	*x = ProcessorConfig{}
	mi := &file_cloud_visionai_v1_data_proto_msgTypes[19]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *ProcessorConfig) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*ProcessorConfig) ProtoMessage() {}

func (x *ProcessorConfig) ProtoReflect() protoreflect.Message {
	mi := &file_cloud_visionai_v1_data_proto_msgTypes[19]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use ProcessorConfig.ProtoReflect.Descriptor instead.
func (*ProcessorConfig) Descriptor() ([]byte, []int) {
	return file_cloud_visionai_v1_data_proto_rawDescGZIP(), []int{19}
}

func (x *ProcessorConfig) GetProcessorConfig() isProcessorConfig_ProcessorConfig {
	if x != nil {
		return x.ProcessorConfig
	}
	return nil
}

func (x *ProcessorConfig) GetVideoStreamInputConfig() *VideoStreamInputConfig {
	if x != nil {
		if x, ok := x.ProcessorConfig.(*ProcessorConfig_VideoStreamInputConfig); ok {
			return x.VideoStreamInputConfig
		}
	}
	return nil
}

func (x *ProcessorConfig) GetAiEnabledDevicesInputConfig() *AIEnabledDevicesInputConfig {
	if x != nil {
		if x, ok := x.ProcessorConfig.(*ProcessorConfig_AiEnabledDevicesInputConfig); ok {
			return x.AiEnabledDevicesInputConfig
		}
	}
	return nil
}

func (x *ProcessorConfig) GetMediaWarehouseConfig() *MediaWarehouseConfig {
	if x != nil {
		if x, ok := x.ProcessorConfig.(*ProcessorConfig_MediaWarehouseConfig); ok {
			return x.MediaWarehouseConfig
		}
	}
	return nil
}

func (x *ProcessorConfig) GetPersonBlurConfig() *PersonBlurConfig {
	if x != nil {
		if x, ok := x.ProcessorConfig.(*ProcessorConfig_PersonBlurConfig); ok {
			return x.PersonBlurConfig
		}
	}
	return nil
}

func (x *ProcessorConfig) GetOccupancyCountConfig() *OccupancyCountConfig {
	if x != nil {
		if x, ok := x.ProcessorConfig.(*ProcessorConfig_OccupancyCountConfig); ok {
			return x.OccupancyCountConfig
		}
	}
	return nil
}

func (x *ProcessorConfig) GetPersonVehicleDetectionConfig() *PersonVehicleDetectionConfig {
	if x != nil {
		if x, ok := x.ProcessorConfig.(*ProcessorConfig_PersonVehicleDetectionConfig); ok {
			return x.PersonVehicleDetectionConfig
		}
	}
	return nil
}

func (x *ProcessorConfig) GetVertexAutomlVisionConfig() *VertexAutoMLVisionConfig {
	if x != nil {
		if x, ok := x.ProcessorConfig.(*ProcessorConfig_VertexAutomlVisionConfig); ok {
			return x.VertexAutomlVisionConfig
		}
	}
	return nil
}

func (x *ProcessorConfig) GetVertexAutomlVideoConfig() *VertexAutoMLVideoConfig {
	if x != nil {
		if x, ok := x.ProcessorConfig.(*ProcessorConfig_VertexAutomlVideoConfig); ok {
			return x.VertexAutomlVideoConfig
		}
	}
	return nil
}

func (x *ProcessorConfig) GetVertexCustomConfig() *VertexCustomConfig {
	if x != nil {
		if x, ok := x.ProcessorConfig.(*ProcessorConfig_VertexCustomConfig); ok {
			return x.VertexCustomConfig
		}
	}
	return nil
}

func (x *ProcessorConfig) GetGeneralObjectDetectionConfig() *GeneralObjectDetectionConfig {
	if x != nil {
		if x, ok := x.ProcessorConfig.(*ProcessorConfig_GeneralObjectDetectionConfig); ok {
			return x.GeneralObjectDetectionConfig
		}
	}
	return nil
}

func (x *ProcessorConfig) GetBigQueryConfig() *BigQueryConfig {
	if x != nil {
		if x, ok := x.ProcessorConfig.(*ProcessorConfig_BigQueryConfig); ok {
			return x.BigQueryConfig
		}
	}
	return nil
}

func (x *ProcessorConfig) GetPersonalProtectiveEquipmentDetectionConfig() *PersonalProtectiveEquipmentDetectionConfig {
	if x != nil {
		if x, ok := x.ProcessorConfig.(*ProcessorConfig_PersonalProtectiveEquipmentDetectionConfig); ok {
			return x.PersonalProtectiveEquipmentDetectionConfig
		}
	}
	return nil
}

type isProcessorConfig_ProcessorConfig interface {
	isProcessorConfig_ProcessorConfig()
}

type ProcessorConfig_VideoStreamInputConfig struct {
	// Configs of stream input processor.
	VideoStreamInputConfig *VideoStreamInputConfig `protobuf:"bytes,9,opt,name=video_stream_input_config,json=videoStreamInputConfig,proto3,oneof"`
}

type ProcessorConfig_AiEnabledDevicesInputConfig struct {
	// Config of AI-enabled input devices.
	AiEnabledDevicesInputConfig *AIEnabledDevicesInputConfig `protobuf:"bytes,20,opt,name=ai_enabled_devices_input_config,json=aiEnabledDevicesInputConfig,proto3,oneof"`
}

type ProcessorConfig_MediaWarehouseConfig struct {
	// Configs of media warehouse processor.
	MediaWarehouseConfig *MediaWarehouseConfig `protobuf:"bytes,10,opt,name=media_warehouse_config,json=mediaWarehouseConfig,proto3,oneof"`
}

type ProcessorConfig_PersonBlurConfig struct {
	// Configs of person blur processor.
	PersonBlurConfig *PersonBlurConfig `protobuf:"bytes,11,opt,name=person_blur_config,json=personBlurConfig,proto3,oneof"`
}

type ProcessorConfig_OccupancyCountConfig struct {
	// Configs of occupancy count processor.
	OccupancyCountConfig *OccupancyCountConfig `protobuf:"bytes,12,opt,name=occupancy_count_config,json=occupancyCountConfig,proto3,oneof"`
}

type ProcessorConfig_PersonVehicleDetectionConfig struct {
	// Configs of Person Vehicle Detection processor.
	PersonVehicleDetectionConfig *PersonVehicleDetectionConfig `protobuf:"bytes,15,opt,name=person_vehicle_detection_config,json=personVehicleDetectionConfig,proto3,oneof"`
}

type ProcessorConfig_VertexAutomlVisionConfig struct {
	// Configs of Vertex AutoML vision processor.
	VertexAutomlVisionConfig *VertexAutoMLVisionConfig `protobuf:"bytes,13,opt,name=vertex_automl_vision_config,json=vertexAutomlVisionConfig,proto3,oneof"`
}

type ProcessorConfig_VertexAutomlVideoConfig struct {
	// Configs of Vertex AutoML video processor.
	VertexAutomlVideoConfig *VertexAutoMLVideoConfig `protobuf:"bytes,14,opt,name=vertex_automl_video_config,json=vertexAutomlVideoConfig,proto3,oneof"`
}

type ProcessorConfig_VertexCustomConfig struct {
	// Configs of Vertex Custom processor.
	VertexCustomConfig *VertexCustomConfig `protobuf:"bytes,17,opt,name=vertex_custom_config,json=vertexCustomConfig,proto3,oneof"`
}

type ProcessorConfig_GeneralObjectDetectionConfig struct {
	// Configs of General Object Detection processor.
	GeneralObjectDetectionConfig *GeneralObjectDetectionConfig `protobuf:"bytes,18,opt,name=general_object_detection_config,json=generalObjectDetectionConfig,proto3,oneof"`
}

type ProcessorConfig_BigQueryConfig struct {
	// Configs of BigQuery processor.
	BigQueryConfig *BigQueryConfig `protobuf:"bytes,19,opt,name=big_query_config,json=bigQueryConfig,proto3,oneof"`
}

type ProcessorConfig_PersonalProtectiveEquipmentDetectionConfig struct {
	// Configs of personal_protective_equipment_detection_config
	PersonalProtectiveEquipmentDetectionConfig *PersonalProtectiveEquipmentDetectionConfig `protobuf:"bytes,22,opt,name=personal_protective_equipment_detection_config,json=personalProtectiveEquipmentDetectionConfig,proto3,oneof"`
}

func (*ProcessorConfig_VideoStreamInputConfig) isProcessorConfig_ProcessorConfig() {}

func (*ProcessorConfig_AiEnabledDevicesInputConfig) isProcessorConfig_ProcessorConfig() {}

func (*ProcessorConfig_MediaWarehouseConfig) isProcessorConfig_ProcessorConfig() {}

func (*ProcessorConfig_PersonBlurConfig) isProcessorConfig_ProcessorConfig() {}

func (*ProcessorConfig_OccupancyCountConfig) isProcessorConfig_ProcessorConfig() {}

func (*ProcessorConfig_PersonVehicleDetectionConfig) isProcessorConfig_ProcessorConfig() {}

func (*ProcessorConfig_VertexAutomlVisionConfig) isProcessorConfig_ProcessorConfig() {}

func (*ProcessorConfig_VertexAutomlVideoConfig) isProcessorConfig_ProcessorConfig() {}

func (*ProcessorConfig_VertexCustomConfig) isProcessorConfig_ProcessorConfig() {}

func (*ProcessorConfig_GeneralObjectDetectionConfig) isProcessorConfig_ProcessorConfig() {}

func (*ProcessorConfig_BigQueryConfig) isProcessorConfig_ProcessorConfig() {}

func (*ProcessorConfig_PersonalProtectiveEquipmentDetectionConfig) isProcessorConfig_ProcessorConfig() {
}

// Message describing Vision AI stream with application specific annotations.
// All the StreamAnnotation object inside this message MUST have unique id.
type StreamWithAnnotation struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Vision AI Stream resource name.
	Stream string `protobuf:"bytes,1,opt,name=stream,proto3" json:"stream,omitempty"`
	// Annotations that will be applied to the whole application.
	ApplicationAnnotations []*StreamAnnotation `protobuf:"bytes,2,rep,name=application_annotations,json=applicationAnnotations,proto3" json:"application_annotations,omitempty"`
	// Annotations that will be applied to the specific node of the application.
	// If the same type of the annotations is applied to both application and
	// node, the node annotation will be added in addition to the global
	// application one.
	// For example, if there is one active zone annotation for the whole
	// application and one active zone annotation for the Occupancy Analytic
	// processor, then the Occupancy Analytic processor will have two active zones
	// defined.
	NodeAnnotations []*StreamWithAnnotation_NodeAnnotation `protobuf:"bytes,3,rep,name=node_annotations,json=nodeAnnotations,proto3" json:"node_annotations,omitempty"`
	unknownFields   protoimpl.UnknownFields
	sizeCache       protoimpl.SizeCache
}

func (x *StreamWithAnnotation) Reset() {
	*x = StreamWithAnnotation{}
	mi := &file_cloud_visionai_v1_data_proto_msgTypes[20]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *StreamWithAnnotation) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*StreamWithAnnotation) ProtoMessage() {}

func (x *StreamWithAnnotation) ProtoReflect() protoreflect.Message {
	mi := &file_cloud_visionai_v1_data_proto_msgTypes[20]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use StreamWithAnnotation.ProtoReflect.Descriptor instead.
func (*StreamWithAnnotation) Descriptor() ([]byte, []int) {
	return file_cloud_visionai_v1_data_proto_rawDescGZIP(), []int{20}
}

func (x *StreamWithAnnotation) GetStream() string {
	if x != nil {
		return x.Stream
	}
	return ""
}

func (x *StreamWithAnnotation) GetApplicationAnnotations() []*StreamAnnotation {
	if x != nil {
		return x.ApplicationAnnotations
	}
	return nil
}

func (x *StreamWithAnnotation) GetNodeAnnotations() []*StreamWithAnnotation_NodeAnnotation {
	if x != nil {
		return x.NodeAnnotations
	}
	return nil
}

// Message describing Video Stream Input Config.
// This message should only be used as a placeholder for builtin:stream-input
// processor, actual stream binding should be specified using corresponding
// API.
type VideoStreamInputConfig struct {
	state                 protoimpl.MessageState  `protogen:"open.v1"`
	Streams               []string                `protobuf:"bytes,1,rep,name=streams,proto3" json:"streams,omitempty"`
	StreamsWithAnnotation []*StreamWithAnnotation `protobuf:"bytes,2,rep,name=streams_with_annotation,json=streamsWithAnnotation,proto3" json:"streams_with_annotation,omitempty"`
	unknownFields         protoimpl.UnknownFields
	sizeCache             protoimpl.SizeCache
}

func (x *VideoStreamInputConfig) Reset() {
	*x = VideoStreamInputConfig{}
	mi := &file_cloud_visionai_v1_data_proto_msgTypes[21]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *VideoStreamInputConfig) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*VideoStreamInputConfig) ProtoMessage() {}

func (x *VideoStreamInputConfig) ProtoReflect() protoreflect.Message {
	mi := &file_cloud_visionai_v1_data_proto_msgTypes[21]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use VideoStreamInputConfig.ProtoReflect.Descriptor instead.
func (*VideoStreamInputConfig) Descriptor() ([]byte, []int) {
	return file_cloud_visionai_v1_data_proto_rawDescGZIP(), []int{21}
}

func (x *VideoStreamInputConfig) GetStreams() []string {
	if x != nil {
		return x.Streams
	}
	return nil
}

func (x *VideoStreamInputConfig) GetStreamsWithAnnotation() []*StreamWithAnnotation {
	if x != nil {
		return x.StreamsWithAnnotation
	}
	return nil
}

// Message describing AI-enabled Devices Input Config.
type AIEnabledDevicesInputConfig struct {
	state         protoimpl.MessageState `protogen:"open.v1"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *AIEnabledDevicesInputConfig) Reset() {
	*x = AIEnabledDevicesInputConfig{}
	mi := &file_cloud_visionai_v1_data_proto_msgTypes[22]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *AIEnabledDevicesInputConfig) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*AIEnabledDevicesInputConfig) ProtoMessage() {}

func (x *AIEnabledDevicesInputConfig) ProtoReflect() protoreflect.Message {
	mi := &file_cloud_visionai_v1_data_proto_msgTypes[22]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use AIEnabledDevicesInputConfig.ProtoReflect.Descriptor instead.
func (*AIEnabledDevicesInputConfig) Descriptor() ([]byte, []int) {
	return file_cloud_visionai_v1_data_proto_rawDescGZIP(), []int{22}
}

// Message describing MediaWarehouseConfig.
type MediaWarehouseConfig struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Resource name of the Media Warehouse corpus.
	// Format:
	// projects/${project_id}/locations/${location_id}/corpora/${corpus_id}
	Corpus string `protobuf:"bytes,1,opt,name=corpus,proto3" json:"corpus,omitempty"`
	// Deprecated.
	Region string `protobuf:"bytes,2,opt,name=region,proto3" json:"region,omitempty"`
	// The duration for which all media assets, associated metadata, and search
	// documents can exist.
	Ttl           *durationpb.Duration `protobuf:"bytes,3,opt,name=ttl,proto3" json:"ttl,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *MediaWarehouseConfig) Reset() {
	*x = MediaWarehouseConfig{}
	mi := &file_cloud_visionai_v1_data_proto_msgTypes[23]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *MediaWarehouseConfig) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*MediaWarehouseConfig) ProtoMessage() {}

func (x *MediaWarehouseConfig) ProtoReflect() protoreflect.Message {
	mi := &file_cloud_visionai_v1_data_proto_msgTypes[23]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use MediaWarehouseConfig.ProtoReflect.Descriptor instead.
func (*MediaWarehouseConfig) Descriptor() ([]byte, []int) {
	return file_cloud_visionai_v1_data_proto_rawDescGZIP(), []int{23}
}

func (x *MediaWarehouseConfig) GetCorpus() string {
	if x != nil {
		return x.Corpus
	}
	return ""
}

func (x *MediaWarehouseConfig) GetRegion() string {
	if x != nil {
		return x.Region
	}
	return ""
}

func (x *MediaWarehouseConfig) GetTtl() *durationpb.Duration {
	if x != nil {
		return x.Ttl
	}
	return nil
}

// Message describing FaceBlurConfig.
type PersonBlurConfig struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Person blur type.
	PersonBlurType PersonBlurConfig_PersonBlurType `protobuf:"varint,1,opt,name=person_blur_type,json=personBlurType,proto3,enum=google.events.cloud.visionai.v1.PersonBlurConfig_PersonBlurType" json:"person_blur_type,omitempty"`
	// Whether only blur faces other than the whole object in the processor.
	FacesOnly     bool `protobuf:"varint,2,opt,name=faces_only,json=facesOnly,proto3" json:"faces_only,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *PersonBlurConfig) Reset() {
	*x = PersonBlurConfig{}
	mi := &file_cloud_visionai_v1_data_proto_msgTypes[24]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *PersonBlurConfig) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*PersonBlurConfig) ProtoMessage() {}

func (x *PersonBlurConfig) ProtoReflect() protoreflect.Message {
	mi := &file_cloud_visionai_v1_data_proto_msgTypes[24]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use PersonBlurConfig.ProtoReflect.Descriptor instead.
func (*PersonBlurConfig) Descriptor() ([]byte, []int) {
	return file_cloud_visionai_v1_data_proto_rawDescGZIP(), []int{24}
}

func (x *PersonBlurConfig) GetPersonBlurType() PersonBlurConfig_PersonBlurType {
	if x != nil {
		return x.PersonBlurType
	}
	return PersonBlurConfig_PERSON_BLUR_TYPE_UNSPECIFIED
}

func (x *PersonBlurConfig) GetFacesOnly() bool {
	if x != nil {
		return x.FacesOnly
	}
	return false
}

// Message describing OccupancyCountConfig.
type OccupancyCountConfig struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Whether to count the appearances of people, output counts have 'people' as
	// the key.
	EnablePeopleCounting bool `protobuf:"varint,1,opt,name=enable_people_counting,json=enablePeopleCounting,proto3" json:"enable_people_counting,omitempty"`
	// Whether to count the appearances of vehicles, output counts will have
	// 'vehicle' as the key.
	EnableVehicleCounting bool `protobuf:"varint,2,opt,name=enable_vehicle_counting,json=enableVehicleCounting,proto3" json:"enable_vehicle_counting,omitempty"`
	// Whether to track each invidual object's loitering time inside the scene or
	// specific zone.
	EnableDwellingTimeTracking bool `protobuf:"varint,3,opt,name=enable_dwelling_time_tracking,json=enableDwellingTimeTracking,proto3" json:"enable_dwelling_time_tracking,omitempty"`
	unknownFields              protoimpl.UnknownFields
	sizeCache                  protoimpl.SizeCache
}

func (x *OccupancyCountConfig) Reset() {
	*x = OccupancyCountConfig{}
	mi := &file_cloud_visionai_v1_data_proto_msgTypes[25]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *OccupancyCountConfig) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*OccupancyCountConfig) ProtoMessage() {}

func (x *OccupancyCountConfig) ProtoReflect() protoreflect.Message {
	mi := &file_cloud_visionai_v1_data_proto_msgTypes[25]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use OccupancyCountConfig.ProtoReflect.Descriptor instead.
func (*OccupancyCountConfig) Descriptor() ([]byte, []int) {
	return file_cloud_visionai_v1_data_proto_rawDescGZIP(), []int{25}
}

func (x *OccupancyCountConfig) GetEnablePeopleCounting() bool {
	if x != nil {
		return x.EnablePeopleCounting
	}
	return false
}

func (x *OccupancyCountConfig) GetEnableVehicleCounting() bool {
	if x != nil {
		return x.EnableVehicleCounting
	}
	return false
}

func (x *OccupancyCountConfig) GetEnableDwellingTimeTracking() bool {
	if x != nil {
		return x.EnableDwellingTimeTracking
	}
	return false
}

// Message describing PersonVehicleDetectionConfig.
type PersonVehicleDetectionConfig struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// At least one of enable_people_counting and enable_vehicle_counting fields
	// must be set to true.
	// Whether to count the appearances of people, output counts have 'people' as
	// the key.
	EnablePeopleCounting bool `protobuf:"varint,1,opt,name=enable_people_counting,json=enablePeopleCounting,proto3" json:"enable_people_counting,omitempty"`
	// Whether to count the appearances of vehicles, output counts will have
	// 'vehicle' as the key.
	EnableVehicleCounting bool `protobuf:"varint,2,opt,name=enable_vehicle_counting,json=enableVehicleCounting,proto3" json:"enable_vehicle_counting,omitempty"`
	unknownFields         protoimpl.UnknownFields
	sizeCache             protoimpl.SizeCache
}

func (x *PersonVehicleDetectionConfig) Reset() {
	*x = PersonVehicleDetectionConfig{}
	mi := &file_cloud_visionai_v1_data_proto_msgTypes[26]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *PersonVehicleDetectionConfig) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*PersonVehicleDetectionConfig) ProtoMessage() {}

func (x *PersonVehicleDetectionConfig) ProtoReflect() protoreflect.Message {
	mi := &file_cloud_visionai_v1_data_proto_msgTypes[26]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use PersonVehicleDetectionConfig.ProtoReflect.Descriptor instead.
func (*PersonVehicleDetectionConfig) Descriptor() ([]byte, []int) {
	return file_cloud_visionai_v1_data_proto_rawDescGZIP(), []int{26}
}

func (x *PersonVehicleDetectionConfig) GetEnablePeopleCounting() bool {
	if x != nil {
		return x.EnablePeopleCounting
	}
	return false
}

func (x *PersonVehicleDetectionConfig) GetEnableVehicleCounting() bool {
	if x != nil {
		return x.EnableVehicleCounting
	}
	return false
}

// Message describing PersonalProtectiveEquipmentDetectionConfig.
type PersonalProtectiveEquipmentDetectionConfig struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Whether to enable face coverage detection.
	EnableFaceCoverageDetection bool `protobuf:"varint,1,opt,name=enable_face_coverage_detection,json=enableFaceCoverageDetection,proto3" json:"enable_face_coverage_detection,omitempty"`
	// Whether to enable head coverage detection.
	EnableHeadCoverageDetection bool `protobuf:"varint,2,opt,name=enable_head_coverage_detection,json=enableHeadCoverageDetection,proto3" json:"enable_head_coverage_detection,omitempty"`
	// Whether to enable hands coverage detection.
	EnableHandsCoverageDetection bool `protobuf:"varint,3,opt,name=enable_hands_coverage_detection,json=enableHandsCoverageDetection,proto3" json:"enable_hands_coverage_detection,omitempty"`
	unknownFields                protoimpl.UnknownFields
	sizeCache                    protoimpl.SizeCache
}

func (x *PersonalProtectiveEquipmentDetectionConfig) Reset() {
	*x = PersonalProtectiveEquipmentDetectionConfig{}
	mi := &file_cloud_visionai_v1_data_proto_msgTypes[27]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *PersonalProtectiveEquipmentDetectionConfig) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*PersonalProtectiveEquipmentDetectionConfig) ProtoMessage() {}

func (x *PersonalProtectiveEquipmentDetectionConfig) ProtoReflect() protoreflect.Message {
	mi := &file_cloud_visionai_v1_data_proto_msgTypes[27]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use PersonalProtectiveEquipmentDetectionConfig.ProtoReflect.Descriptor instead.
func (*PersonalProtectiveEquipmentDetectionConfig) Descriptor() ([]byte, []int) {
	return file_cloud_visionai_v1_data_proto_rawDescGZIP(), []int{27}
}

func (x *PersonalProtectiveEquipmentDetectionConfig) GetEnableFaceCoverageDetection() bool {
	if x != nil {
		return x.EnableFaceCoverageDetection
	}
	return false
}

func (x *PersonalProtectiveEquipmentDetectionConfig) GetEnableHeadCoverageDetection() bool {
	if x != nil {
		return x.EnableHeadCoverageDetection
	}
	return false
}

func (x *PersonalProtectiveEquipmentDetectionConfig) GetEnableHandsCoverageDetection() bool {
	if x != nil {
		return x.EnableHandsCoverageDetection
	}
	return false
}

// Message of configurations for General Object Detection processor.
type GeneralObjectDetectionConfig struct {
	state         protoimpl.MessageState `protogen:"open.v1"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *GeneralObjectDetectionConfig) Reset() {
	*x = GeneralObjectDetectionConfig{}
	mi := &file_cloud_visionai_v1_data_proto_msgTypes[28]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *GeneralObjectDetectionConfig) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*GeneralObjectDetectionConfig) ProtoMessage() {}

func (x *GeneralObjectDetectionConfig) ProtoReflect() protoreflect.Message {
	mi := &file_cloud_visionai_v1_data_proto_msgTypes[28]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use GeneralObjectDetectionConfig.ProtoReflect.Descriptor instead.
func (*GeneralObjectDetectionConfig) Descriptor() ([]byte, []int) {
	return file_cloud_visionai_v1_data_proto_rawDescGZIP(), []int{28}
}

// Message of configurations for BigQuery processor.
type BigQueryConfig struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// BigQuery table resource for Vision AI Platform to ingest annotations to.
	Table string `protobuf:"bytes,1,opt,name=table,proto3" json:"table,omitempty"`
	// Data Schema
	// By default, Vision AI Application will try to write annotations to the
	// target BigQuery table using the following schema:
	//
	// ingestion_time: TIMESTAMP, the ingestion time of the original data.
	//
	// application: STRING, name of the application which produces the annotation.
	//
	// instance: STRING, Id of the instance which produces the annotation.
	//
	// node: STRING, name of the application graph node which produces the
	// annotation.
	//
	// annotation: STRING or JSON, the actual annotation protobuf will be
	// converted to json string with bytes field as 64 encoded string. It can be
	// written to both String or Json type column.
	//
	// To forward annotation data to an existing BigQuery table, customer needs to
	// make sure the compatibility of the schema.
	// The map maps application node name to its corresponding cloud function
	// endpoint to transform the annotations directly to the
	// google.cloud.bigquery.storage.v1.AppendRowsRequest (only avro_rows or
	// proto_rows should be set). If configured, annotations produced by
	// corresponding application node will sent to the Cloud Function at first
	// before be forwarded to BigQuery.
	//
	// If the default table schema doesn't fit, customer is able to transform the
	// annotation output from Vision AI Application to arbitrary BigQuery table
	// schema with CloudFunction.
	// * The cloud function will receive AppPlatformCloudFunctionRequest where
	// the annotations field will be the json format of Vision AI annotation.
	// * The cloud function should return AppPlatformCloudFunctionResponse with
	// AppendRowsRequest stored in the annotations field.
	// * To drop the annotation, simply clear the annotations field in the
	// returned AppPlatformCloudFunctionResponse.
	CloudFunctionMapping map[string]string `protobuf:"bytes,2,rep,name=cloud_function_mapping,json=cloudFunctionMapping,proto3" json:"cloud_function_mapping,omitempty" protobuf_key:"bytes,1,opt,name=key" protobuf_val:"bytes,2,opt,name=value"`
	// If true, App Platform will create the BigQuery DataSet and the
	// BigQuery Table with default schema if the specified table doesn't exist.
	// This doesn't work if any cloud function customized schema is specified
	// since the system doesn't know your desired schema.
	// JSON column will be used in the default table created by App Platform.
	CreateDefaultTableIfNotExists bool `protobuf:"varint,3,opt,name=create_default_table_if_not_exists,json=createDefaultTableIfNotExists,proto3" json:"create_default_table_if_not_exists,omitempty"`
	unknownFields                 protoimpl.UnknownFields
	sizeCache                     protoimpl.SizeCache
}

func (x *BigQueryConfig) Reset() {
	*x = BigQueryConfig{}
	mi := &file_cloud_visionai_v1_data_proto_msgTypes[29]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *BigQueryConfig) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*BigQueryConfig) ProtoMessage() {}

func (x *BigQueryConfig) ProtoReflect() protoreflect.Message {
	mi := &file_cloud_visionai_v1_data_proto_msgTypes[29]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use BigQueryConfig.ProtoReflect.Descriptor instead.
func (*BigQueryConfig) Descriptor() ([]byte, []int) {
	return file_cloud_visionai_v1_data_proto_rawDescGZIP(), []int{29}
}

func (x *BigQueryConfig) GetTable() string {
	if x != nil {
		return x.Table
	}
	return ""
}

func (x *BigQueryConfig) GetCloudFunctionMapping() map[string]string {
	if x != nil {
		return x.CloudFunctionMapping
	}
	return nil
}

func (x *BigQueryConfig) GetCreateDefaultTableIfNotExists() bool {
	if x != nil {
		return x.CreateDefaultTableIfNotExists
	}
	return false
}

// Message of configurations of Vertex AutoML Vision Processors.
type VertexAutoMLVisionConfig struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Only entities with higher score than the threshold will be returned.
	// Value 0.0 means to return all the detected entities.
	ConfidenceThreshold float32 `protobuf:"fixed32,1,opt,name=confidence_threshold,json=confidenceThreshold,proto3" json:"confidence_threshold,omitempty"`
	// At most this many predictions will be returned per output frame.
	// Value 0 means to return all the detected entities.
	MaxPredictions int32 `protobuf:"varint,2,opt,name=max_predictions,json=maxPredictions,proto3" json:"max_predictions,omitempty"`
	unknownFields  protoimpl.UnknownFields
	sizeCache      protoimpl.SizeCache
}

func (x *VertexAutoMLVisionConfig) Reset() {
	*x = VertexAutoMLVisionConfig{}
	mi := &file_cloud_visionai_v1_data_proto_msgTypes[30]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *VertexAutoMLVisionConfig) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*VertexAutoMLVisionConfig) ProtoMessage() {}

func (x *VertexAutoMLVisionConfig) ProtoReflect() protoreflect.Message {
	mi := &file_cloud_visionai_v1_data_proto_msgTypes[30]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use VertexAutoMLVisionConfig.ProtoReflect.Descriptor instead.
func (*VertexAutoMLVisionConfig) Descriptor() ([]byte, []int) {
	return file_cloud_visionai_v1_data_proto_rawDescGZIP(), []int{30}
}

func (x *VertexAutoMLVisionConfig) GetConfidenceThreshold() float32 {
	if x != nil {
		return x.ConfidenceThreshold
	}
	return 0
}

func (x *VertexAutoMLVisionConfig) GetMaxPredictions() int32 {
	if x != nil {
		return x.MaxPredictions
	}
	return 0
}

// Message describing VertexAutoMLVideoConfig.
type VertexAutoMLVideoConfig struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Only entities with higher score than the threshold will be returned.
	// Value 0.0 means returns all the detected entities.
	ConfidenceThreshold float32 `protobuf:"fixed32,1,opt,name=confidence_threshold,json=confidenceThreshold,proto3" json:"confidence_threshold,omitempty"`
	// Labels specified in this field won't be returned.
	BlockedLabels []string `protobuf:"bytes,2,rep,name=blocked_labels,json=blockedLabels,proto3" json:"blocked_labels,omitempty"`
	// At most this many predictions will be returned per output frame.
	// Value 0 means to return all the detected entities.
	MaxPredictions int32 `protobuf:"varint,3,opt,name=max_predictions,json=maxPredictions,proto3" json:"max_predictions,omitempty"`
	// Only Bounding Box whose size is larger than this limit will be returned.
	// Object Tracking only.
	// Value 0.0 means to return all the detected entities.
	BoundingBoxSizeLimit float32 `protobuf:"fixed32,4,opt,name=bounding_box_size_limit,json=boundingBoxSizeLimit,proto3" json:"bounding_box_size_limit,omitempty"`
	unknownFields        protoimpl.UnknownFields
	sizeCache            protoimpl.SizeCache
}

func (x *VertexAutoMLVideoConfig) Reset() {
	*x = VertexAutoMLVideoConfig{}
	mi := &file_cloud_visionai_v1_data_proto_msgTypes[31]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *VertexAutoMLVideoConfig) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*VertexAutoMLVideoConfig) ProtoMessage() {}

func (x *VertexAutoMLVideoConfig) ProtoReflect() protoreflect.Message {
	mi := &file_cloud_visionai_v1_data_proto_msgTypes[31]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use VertexAutoMLVideoConfig.ProtoReflect.Descriptor instead.
func (*VertexAutoMLVideoConfig) Descriptor() ([]byte, []int) {
	return file_cloud_visionai_v1_data_proto_rawDescGZIP(), []int{31}
}

func (x *VertexAutoMLVideoConfig) GetConfidenceThreshold() float32 {
	if x != nil {
		return x.ConfidenceThreshold
	}
	return 0
}

func (x *VertexAutoMLVideoConfig) GetBlockedLabels() []string {
	if x != nil {
		return x.BlockedLabels
	}
	return nil
}

func (x *VertexAutoMLVideoConfig) GetMaxPredictions() int32 {
	if x != nil {
		return x.MaxPredictions
	}
	return 0
}

func (x *VertexAutoMLVideoConfig) GetBoundingBoxSizeLimit() float32 {
	if x != nil {
		return x.BoundingBoxSizeLimit
	}
	return 0
}

// Message describing VertexCustomConfig.
type VertexCustomConfig struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// The max prediction frame per second. This attribute sets how fast the
	// operator sends prediction requests to Vertex AI endpoint. Default value is
	// 0, which means there is no max prediction fps limit. The operator sends
	// prediction requests at input fps.
	MaxPredictionFps int32 `protobuf:"varint,1,opt,name=max_prediction_fps,json=maxPredictionFps,proto3" json:"max_prediction_fps,omitempty"`
	// A description of resources that are dedicated to the DeployedModel, and
	// that need a higher degree of manual configuration.
	DedicatedResources *DedicatedResources `protobuf:"bytes,2,opt,name=dedicated_resources,json=dedicatedResources,proto3" json:"dedicated_resources,omitempty"`
	// If not empty, the prediction result will be sent to the specified cloud
	// function for post processing.
	// * The cloud function will receive AppPlatformCloudFunctionRequest where
	// the annotations field will be the json format of proto PredictResponse.
	// * The cloud function should return AppPlatformCloudFunctionResponse with
	// PredictResponse stored in the annotations field.
	// * To drop the prediction output, simply clear the payload field in the
	// returned AppPlatformCloudFunctionResponse.
	PostProcessingCloudFunction string `protobuf:"bytes,3,opt,name=post_processing_cloud_function,json=postProcessingCloudFunction,proto3" json:"post_processing_cloud_function,omitempty"`
	// If true, the prediction request received by custom model will also contain
	// metadata with the following schema:
	//
	//	'appPlatformMetadata': {
	//	      'ingestionTime': DOUBLE; (UNIX timestamp)
	//	      'application': STRING;
	//	      'instanceId': STRING;
	//	      'node': STRING;
	//	      'processor': STRING;
	//	 }
	AttachApplicationMetadata bool `protobuf:"varint,4,opt,name=attach_application_metadata,json=attachApplicationMetadata,proto3" json:"attach_application_metadata,omitempty"`
	unknownFields             protoimpl.UnknownFields
	sizeCache                 protoimpl.SizeCache
}

func (x *VertexCustomConfig) Reset() {
	*x = VertexCustomConfig{}
	mi := &file_cloud_visionai_v1_data_proto_msgTypes[32]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *VertexCustomConfig) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*VertexCustomConfig) ProtoMessage() {}

func (x *VertexCustomConfig) ProtoReflect() protoreflect.Message {
	mi := &file_cloud_visionai_v1_data_proto_msgTypes[32]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use VertexCustomConfig.ProtoReflect.Descriptor instead.
func (*VertexCustomConfig) Descriptor() ([]byte, []int) {
	return file_cloud_visionai_v1_data_proto_rawDescGZIP(), []int{32}
}

func (x *VertexCustomConfig) GetMaxPredictionFps() int32 {
	if x != nil {
		return x.MaxPredictionFps
	}
	return 0
}

func (x *VertexCustomConfig) GetDedicatedResources() *DedicatedResources {
	if x != nil {
		return x.DedicatedResources
	}
	return nil
}

func (x *VertexCustomConfig) GetPostProcessingCloudFunction() string {
	if x != nil {
		return x.PostProcessingCloudFunction
	}
	return ""
}

func (x *VertexCustomConfig) GetAttachApplicationMetadata() bool {
	if x != nil {
		return x.AttachApplicationMetadata
	}
	return false
}

// Specification of a single machine.
type MachineSpec struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Immutable. The type of the machine.
	//
	// See the [list of machine types supported for
	// prediction](https://cloud.google.com/vertex-ai/docs/predictions/configure-compute#machine-types)
	//
	// See the [list of machine types supported for custom
	// training](https://cloud.google.com/vertex-ai/docs/training/configure-compute#machine-types).
	//
	// For [DeployedModel][] this field is optional, and the default
	// value is `n1-standard-2`. For [BatchPredictionJob][] or as part of
	// [WorkerPoolSpec][] this field is required.
	MachineType string `protobuf:"bytes,1,opt,name=machine_type,json=machineType,proto3" json:"machine_type,omitempty"`
	// Immutable. The type of accelerator(s) that may be attached to the machine
	// as per
	// [accelerator_count][google.cloud.visionai.v1.MachineSpec.accelerator_count].
	AcceleratorType AcceleratorType `protobuf:"varint,2,opt,name=accelerator_type,json=acceleratorType,proto3,enum=google.events.cloud.visionai.v1.AcceleratorType" json:"accelerator_type,omitempty"`
	// The number of accelerators to attach to the machine.
	AcceleratorCount int32 `protobuf:"varint,3,opt,name=accelerator_count,json=acceleratorCount,proto3" json:"accelerator_count,omitempty"`
	unknownFields    protoimpl.UnknownFields
	sizeCache        protoimpl.SizeCache
}

func (x *MachineSpec) Reset() {
	*x = MachineSpec{}
	mi := &file_cloud_visionai_v1_data_proto_msgTypes[33]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *MachineSpec) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*MachineSpec) ProtoMessage() {}

func (x *MachineSpec) ProtoReflect() protoreflect.Message {
	mi := &file_cloud_visionai_v1_data_proto_msgTypes[33]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use MachineSpec.ProtoReflect.Descriptor instead.
func (*MachineSpec) Descriptor() ([]byte, []int) {
	return file_cloud_visionai_v1_data_proto_rawDescGZIP(), []int{33}
}

func (x *MachineSpec) GetMachineType() string {
	if x != nil {
		return x.MachineType
	}
	return ""
}

func (x *MachineSpec) GetAcceleratorType() AcceleratorType {
	if x != nil {
		return x.AcceleratorType
	}
	return AcceleratorType_ACCELERATOR_TYPE_UNSPECIFIED
}

func (x *MachineSpec) GetAcceleratorCount() int32 {
	if x != nil {
		return x.AcceleratorCount
	}
	return 0
}

// The metric specification that defines the target resource utilization
// (CPU utilization, accelerator's duty cycle, and so on) for calculating the
// desired replica count.
type AutoscalingMetricSpec struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Required. The resource metric name.
	// Supported metrics:
	//
	// * For Online Prediction:
	// * `aiplatform.googleapis.com/prediction/online/accelerator/duty_cycle`
	// * `aiplatform.googleapis.com/prediction/online/cpu/utilization`
	MetricName string `protobuf:"bytes,1,opt,name=metric_name,json=metricName,proto3" json:"metric_name,omitempty"`
	// The target resource utilization in percentage (1% - 100%) for the given
	// metric; once the real usage deviates from the target by a certain
	// percentage, the machine replicas change. The default value is 60
	// (representing 60%) if not provided.
	Target        int32 `protobuf:"varint,2,opt,name=target,proto3" json:"target,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *AutoscalingMetricSpec) Reset() {
	*x = AutoscalingMetricSpec{}
	mi := &file_cloud_visionai_v1_data_proto_msgTypes[34]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *AutoscalingMetricSpec) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*AutoscalingMetricSpec) ProtoMessage() {}

func (x *AutoscalingMetricSpec) ProtoReflect() protoreflect.Message {
	mi := &file_cloud_visionai_v1_data_proto_msgTypes[34]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use AutoscalingMetricSpec.ProtoReflect.Descriptor instead.
func (*AutoscalingMetricSpec) Descriptor() ([]byte, []int) {
	return file_cloud_visionai_v1_data_proto_rawDescGZIP(), []int{34}
}

func (x *AutoscalingMetricSpec) GetMetricName() string {
	if x != nil {
		return x.MetricName
	}
	return ""
}

func (x *AutoscalingMetricSpec) GetTarget() int32 {
	if x != nil {
		return x.Target
	}
	return 0
}

// A description of resources that are dedicated to a DeployedModel, and
// that need a higher degree of manual configuration.
type DedicatedResources struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Required. Immutable. The specification of a single machine used by the
	// prediction.
	MachineSpec *MachineSpec `protobuf:"bytes,1,opt,name=machine_spec,json=machineSpec,proto3" json:"machine_spec,omitempty"`
	// Required. Immutable. The minimum number of machine replicas this
	// DeployedModel will be always deployed on. This value must be greater than
	// or equal to 1.
	//
	// If traffic against the DeployedModel increases, it may dynamically be
	// deployed onto more replicas, and as traffic decreases, some of these extra
	// replicas may be freed.
	MinReplicaCount int32 `protobuf:"varint,2,opt,name=min_replica_count,json=minReplicaCount,proto3" json:"min_replica_count,omitempty"`
	// Immutable. The maximum number of replicas this DeployedModel may be
	// deployed on when the traffic against it increases. If the requested value
	// is too large, the deployment will error, but if deployment succeeds then
	// the ability to scale the model to that many replicas is guaranteed (barring
	// service outages). If traffic against the DeployedModel increases beyond
	// what its replicas at maximum may handle, a portion of the traffic will be
	// dropped. If this value is not provided, will use
	// [min_replica_count][google.cloud.visionai.v1.DedicatedResources.min_replica_count]
	// as the default value.
	//
	// The value of this field impacts the charge against Vertex CPU and GPU
	// quotas. Specifically, you will be charged for max_replica_count *
	// number of cores in the selected machine type) and (max_replica_count *
	// number of GPUs per replica in the selected machine type).
	MaxReplicaCount int32 `protobuf:"varint,3,opt,name=max_replica_count,json=maxReplicaCount,proto3" json:"max_replica_count,omitempty"`
	// Immutable. The metric specifications that overrides a resource
	// utilization metric (CPU utilization, accelerator's duty cycle, and so on)
	// target value (default to 60 if not set). At most one entry is allowed per
	// metric.
	//
	// If
	// [machine_spec.accelerator_count][google.cloud.visionai.v1.MachineSpec.accelerator_count]
	// is above 0, the autoscaling will be based on both CPU utilization and
	// accelerator's duty cycle metrics and scale up when either metrics exceeds
	// its target value while scale down if both metrics are under their target
	// value. The default target value is 60 for both metrics.
	//
	// If
	// [machine_spec.accelerator_count][google.cloud.visionai.v1.MachineSpec.accelerator_count]
	// is 0, the autoscaling will be based on CPU utilization metric only with
	// default target value 60 if not explicitly set.
	//
	// For example, in the case of Online Prediction, if you want to override
	// target CPU utilization to 80, you should set
	// [autoscaling_metric_specs.metric_name][google.cloud.visionai.v1.AutoscalingMetricSpec.metric_name]
	// to `aiplatform.googleapis.com/prediction/online/cpu/utilization` and
	// [autoscaling_metric_specs.target][google.cloud.visionai.v1.AutoscalingMetricSpec.target]
	// to `80`.
	AutoscalingMetricSpecs []*AutoscalingMetricSpec `protobuf:"bytes,4,rep,name=autoscaling_metric_specs,json=autoscalingMetricSpecs,proto3" json:"autoscaling_metric_specs,omitempty"`
	unknownFields          protoimpl.UnknownFields
	sizeCache              protoimpl.SizeCache
}

func (x *DedicatedResources) Reset() {
	*x = DedicatedResources{}
	mi := &file_cloud_visionai_v1_data_proto_msgTypes[35]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *DedicatedResources) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*DedicatedResources) ProtoMessage() {}

func (x *DedicatedResources) ProtoReflect() protoreflect.Message {
	mi := &file_cloud_visionai_v1_data_proto_msgTypes[35]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use DedicatedResources.ProtoReflect.Descriptor instead.
func (*DedicatedResources) Descriptor() ([]byte, []int) {
	return file_cloud_visionai_v1_data_proto_rawDescGZIP(), []int{35}
}

func (x *DedicatedResources) GetMachineSpec() *MachineSpec {
	if x != nil {
		return x.MachineSpec
	}
	return nil
}

func (x *DedicatedResources) GetMinReplicaCount() int32 {
	if x != nil {
		return x.MinReplicaCount
	}
	return 0
}

func (x *DedicatedResources) GetMaxReplicaCount() int32 {
	if x != nil {
		return x.MaxReplicaCount
	}
	return 0
}

func (x *DedicatedResources) GetAutoscalingMetricSpecs() []*AutoscalingMetricSpec {
	if x != nil {
		return x.AutoscalingMetricSpecs
	}
	return nil
}

// Message describing the Stream object. The Stream and the Event resources are
// many to many; i.e., each Stream resource can associate to many Event
// resources and each Event resource can associate to many Stream resources.
type Stream struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Name of the resource.
	Name string `protobuf:"bytes,1,opt,name=name,proto3" json:"name,omitempty"`
	// Output only. The create timestamp.
	CreateTime *timestamppb.Timestamp `protobuf:"bytes,2,opt,name=create_time,json=createTime,proto3" json:"create_time,omitempty"`
	// Output only. The update timestamp.
	UpdateTime *timestamppb.Timestamp `protobuf:"bytes,3,opt,name=update_time,json=updateTime,proto3" json:"update_time,omitempty"`
	// Labels as key value pairs.
	Labels map[string]string `protobuf:"bytes,4,rep,name=labels,proto3" json:"labels,omitempty" protobuf_key:"bytes,1,opt,name=key" protobuf_val:"bytes,2,opt,name=value"`
	// Annotations to allow clients to store small amounts of arbitrary data.
	Annotations map[string]string `protobuf:"bytes,5,rep,name=annotations,proto3" json:"annotations,omitempty" protobuf_key:"bytes,1,opt,name=key" protobuf_val:"bytes,2,opt,name=value"`
	// The display name for the stream resource.
	DisplayName string `protobuf:"bytes,6,opt,name=display_name,json=displayName,proto3" json:"display_name,omitempty"`
	// Whether to enable the HLS playback service on this stream.
	EnableHlsPlayback bool `protobuf:"varint,7,opt,name=enable_hls_playback,json=enableHlsPlayback,proto3" json:"enable_hls_playback,omitempty"`
	// The name of the media warehouse asset for long term storage of stream data.
	// Format: projects/${p_id}/locations/${l_id}/corpora/${c_id}/assets/${a_id}
	// Remain empty if the media warehouse storage is not needed for the stream.
	MediaWarehouseAsset string `protobuf:"bytes,8,opt,name=media_warehouse_asset,json=mediaWarehouseAsset,proto3" json:"media_warehouse_asset,omitempty"`
	unknownFields       protoimpl.UnknownFields
	sizeCache           protoimpl.SizeCache
}

func (x *Stream) Reset() {
	*x = Stream{}
	mi := &file_cloud_visionai_v1_data_proto_msgTypes[36]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *Stream) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*Stream) ProtoMessage() {}

func (x *Stream) ProtoReflect() protoreflect.Message {
	mi := &file_cloud_visionai_v1_data_proto_msgTypes[36]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use Stream.ProtoReflect.Descriptor instead.
func (*Stream) Descriptor() ([]byte, []int) {
	return file_cloud_visionai_v1_data_proto_rawDescGZIP(), []int{36}
}

func (x *Stream) GetName() string {
	if x != nil {
		return x.Name
	}
	return ""
}

func (x *Stream) GetCreateTime() *timestamppb.Timestamp {
	if x != nil {
		return x.CreateTime
	}
	return nil
}

func (x *Stream) GetUpdateTime() *timestamppb.Timestamp {
	if x != nil {
		return x.UpdateTime
	}
	return nil
}

func (x *Stream) GetLabels() map[string]string {
	if x != nil {
		return x.Labels
	}
	return nil
}

func (x *Stream) GetAnnotations() map[string]string {
	if x != nil {
		return x.Annotations
	}
	return nil
}

func (x *Stream) GetDisplayName() string {
	if x != nil {
		return x.DisplayName
	}
	return ""
}

func (x *Stream) GetEnableHlsPlayback() bool {
	if x != nil {
		return x.EnableHlsPlayback
	}
	return false
}

func (x *Stream) GetMediaWarehouseAsset() string {
	if x != nil {
		return x.MediaWarehouseAsset
	}
	return ""
}

// Message describing the Event object.
type Event struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Name of the resource.
	Name string `protobuf:"bytes,1,opt,name=name,proto3" json:"name,omitempty"`
	// Output only. The create timestamp.
	CreateTime *timestamppb.Timestamp `protobuf:"bytes,2,opt,name=create_time,json=createTime,proto3" json:"create_time,omitempty"`
	// Output only. The update timestamp.
	UpdateTime *timestamppb.Timestamp `protobuf:"bytes,3,opt,name=update_time,json=updateTime,proto3" json:"update_time,omitempty"`
	// Labels as key value pairs.
	Labels map[string]string `protobuf:"bytes,4,rep,name=labels,proto3" json:"labels,omitempty" protobuf_key:"bytes,1,opt,name=key" protobuf_val:"bytes,2,opt,name=value"`
	// Annotations to allow clients to store small amounts of arbitrary data.
	Annotations map[string]string `protobuf:"bytes,5,rep,name=annotations,proto3" json:"annotations,omitempty" protobuf_key:"bytes,1,opt,name=key" protobuf_val:"bytes,2,opt,name=value"`
	// The clock used for joining streams.
	AlignmentClock Event_Clock `protobuf:"varint,6,opt,name=alignment_clock,json=alignmentClock,proto3,enum=google.events.cloud.visionai.v1.Event_Clock" json:"alignment_clock,omitempty"`
	// Grace period for cleaning up the event. This is the time the controller
	// waits for before deleting the event. During this period, if there is any
	// active channel on the event. The deletion of the event after grace_period
	// will be ignored.
	GracePeriod   *durationpb.Duration `protobuf:"bytes,7,opt,name=grace_period,json=gracePeriod,proto3" json:"grace_period,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *Event) Reset() {
	*x = Event{}
	mi := &file_cloud_visionai_v1_data_proto_msgTypes[37]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *Event) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*Event) ProtoMessage() {}

func (x *Event) ProtoReflect() protoreflect.Message {
	mi := &file_cloud_visionai_v1_data_proto_msgTypes[37]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use Event.ProtoReflect.Descriptor instead.
func (*Event) Descriptor() ([]byte, []int) {
	return file_cloud_visionai_v1_data_proto_rawDescGZIP(), []int{37}
}

func (x *Event) GetName() string {
	if x != nil {
		return x.Name
	}
	return ""
}

func (x *Event) GetCreateTime() *timestamppb.Timestamp {
	if x != nil {
		return x.CreateTime
	}
	return nil
}

func (x *Event) GetUpdateTime() *timestamppb.Timestamp {
	if x != nil {
		return x.UpdateTime
	}
	return nil
}

func (x *Event) GetLabels() map[string]string {
	if x != nil {
		return x.Labels
	}
	return nil
}

func (x *Event) GetAnnotations() map[string]string {
	if x != nil {
		return x.Annotations
	}
	return nil
}

func (x *Event) GetAlignmentClock() Event_Clock {
	if x != nil {
		return x.AlignmentClock
	}
	return Event_CLOCK_UNSPECIFIED
}

func (x *Event) GetGracePeriod() *durationpb.Duration {
	if x != nil {
		return x.GracePeriod
	}
	return nil
}

// Message describing the Series object.
type Series struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Name of the resource.
	Name string `protobuf:"bytes,1,opt,name=name,proto3" json:"name,omitempty"`
	// Output only. The create timestamp.
	CreateTime *timestamppb.Timestamp `protobuf:"bytes,2,opt,name=create_time,json=createTime,proto3" json:"create_time,omitempty"`
	// Output only. The update timestamp.
	UpdateTime *timestamppb.Timestamp `protobuf:"bytes,3,opt,name=update_time,json=updateTime,proto3" json:"update_time,omitempty"`
	// Labels as key value pairs.
	Labels map[string]string `protobuf:"bytes,4,rep,name=labels,proto3" json:"labels,omitempty" protobuf_key:"bytes,1,opt,name=key" protobuf_val:"bytes,2,opt,name=value"`
	// Annotations to allow clients to store small amounts of arbitrary data.
	Annotations map[string]string `protobuf:"bytes,5,rep,name=annotations,proto3" json:"annotations,omitempty" protobuf_key:"bytes,1,opt,name=key" protobuf_val:"bytes,2,opt,name=value"`
	// Required. Stream that is associated with this series.
	Stream string `protobuf:"bytes,6,opt,name=stream,proto3" json:"stream,omitempty"`
	// Required. Event that is associated with this series.
	Event         string `protobuf:"bytes,7,opt,name=event,proto3" json:"event,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *Series) Reset() {
	*x = Series{}
	mi := &file_cloud_visionai_v1_data_proto_msgTypes[38]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *Series) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*Series) ProtoMessage() {}

func (x *Series) ProtoReflect() protoreflect.Message {
	mi := &file_cloud_visionai_v1_data_proto_msgTypes[38]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use Series.ProtoReflect.Descriptor instead.
func (*Series) Descriptor() ([]byte, []int) {
	return file_cloud_visionai_v1_data_proto_rawDescGZIP(), []int{38}
}

func (x *Series) GetName() string {
	if x != nil {
		return x.Name
	}
	return ""
}

func (x *Series) GetCreateTime() *timestamppb.Timestamp {
	if x != nil {
		return x.CreateTime
	}
	return nil
}

func (x *Series) GetUpdateTime() *timestamppb.Timestamp {
	if x != nil {
		return x.UpdateTime
	}
	return nil
}

func (x *Series) GetLabels() map[string]string {
	if x != nil {
		return x.Labels
	}
	return nil
}

func (x *Series) GetAnnotations() map[string]string {
	if x != nil {
		return x.Annotations
	}
	return nil
}

func (x *Series) GetStream() string {
	if x != nil {
		return x.Stream
	}
	return ""
}

func (x *Series) GetEvent() string {
	if x != nil {
		return x.Event
	}
	return ""
}

// The data within all Series events.
type SeriesEventData struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Optional. The Series event payload. Unset for deletion events.
	Payload       *Series `protobuf:"bytes,1,opt,name=payload,proto3,oneof" json:"payload,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *SeriesEventData) Reset() {
	*x = SeriesEventData{}
	mi := &file_cloud_visionai_v1_data_proto_msgTypes[39]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *SeriesEventData) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*SeriesEventData) ProtoMessage() {}

func (x *SeriesEventData) ProtoReflect() protoreflect.Message {
	mi := &file_cloud_visionai_v1_data_proto_msgTypes[39]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use SeriesEventData.ProtoReflect.Descriptor instead.
func (*SeriesEventData) Descriptor() ([]byte, []int) {
	return file_cloud_visionai_v1_data_proto_rawDescGZIP(), []int{39}
}

func (x *SeriesEventData) GetPayload() *Series {
	if x != nil {
		return x.Payload
	}
	return nil
}

// The data within all Draft events.
type DraftEventData struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Optional. The Draft event payload. Unset for deletion events.
	Payload       *Draft `protobuf:"bytes,1,opt,name=payload,proto3,oneof" json:"payload,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *DraftEventData) Reset() {
	*x = DraftEventData{}
	mi := &file_cloud_visionai_v1_data_proto_msgTypes[40]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *DraftEventData) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*DraftEventData) ProtoMessage() {}

func (x *DraftEventData) ProtoReflect() protoreflect.Message {
	mi := &file_cloud_visionai_v1_data_proto_msgTypes[40]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use DraftEventData.ProtoReflect.Descriptor instead.
func (*DraftEventData) Descriptor() ([]byte, []int) {
	return file_cloud_visionai_v1_data_proto_rawDescGZIP(), []int{40}
}

func (x *DraftEventData) GetPayload() *Draft {
	if x != nil {
		return x.Payload
	}
	return nil
}

// The data within all Processor events.
type ProcessorEventData struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Optional. The Processor event payload. Unset for deletion events.
	Payload       *Processor `protobuf:"bytes,1,opt,name=payload,proto3,oneof" json:"payload,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *ProcessorEventData) Reset() {
	*x = ProcessorEventData{}
	mi := &file_cloud_visionai_v1_data_proto_msgTypes[41]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *ProcessorEventData) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*ProcessorEventData) ProtoMessage() {}

func (x *ProcessorEventData) ProtoReflect() protoreflect.Message {
	mi := &file_cloud_visionai_v1_data_proto_msgTypes[41]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use ProcessorEventData.ProtoReflect.Descriptor instead.
func (*ProcessorEventData) Descriptor() ([]byte, []int) {
	return file_cloud_visionai_v1_data_proto_rawDescGZIP(), []int{41}
}

func (x *ProcessorEventData) GetPayload() *Processor {
	if x != nil {
		return x.Payload
	}
	return nil
}

// The data within all Analysis events.
type AnalysisEventData struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Optional. The Analysis event payload. Unset for deletion events.
	Payload       *Analysis `protobuf:"bytes,1,opt,name=payload,proto3,oneof" json:"payload,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *AnalysisEventData) Reset() {
	*x = AnalysisEventData{}
	mi := &file_cloud_visionai_v1_data_proto_msgTypes[42]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *AnalysisEventData) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*AnalysisEventData) ProtoMessage() {}

func (x *AnalysisEventData) ProtoReflect() protoreflect.Message {
	mi := &file_cloud_visionai_v1_data_proto_msgTypes[42]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use AnalysisEventData.ProtoReflect.Descriptor instead.
func (*AnalysisEventData) Descriptor() ([]byte, []int) {
	return file_cloud_visionai_v1_data_proto_rawDescGZIP(), []int{42}
}

func (x *AnalysisEventData) GetPayload() *Analysis {
	if x != nil {
		return x.Payload
	}
	return nil
}

// The data within all Cluster events.
type ClusterEventData struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Optional. The Cluster event payload. Unset for deletion events.
	Payload       *Cluster `protobuf:"bytes,1,opt,name=payload,proto3,oneof" json:"payload,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *ClusterEventData) Reset() {
	*x = ClusterEventData{}
	mi := &file_cloud_visionai_v1_data_proto_msgTypes[43]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *ClusterEventData) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*ClusterEventData) ProtoMessage() {}

func (x *ClusterEventData) ProtoReflect() protoreflect.Message {
	mi := &file_cloud_visionai_v1_data_proto_msgTypes[43]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use ClusterEventData.ProtoReflect.Descriptor instead.
func (*ClusterEventData) Descriptor() ([]byte, []int) {
	return file_cloud_visionai_v1_data_proto_rawDescGZIP(), []int{43}
}

func (x *ClusterEventData) GetPayload() *Cluster {
	if x != nil {
		return x.Payload
	}
	return nil
}

// The data within all Event events.
type EventEventData struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Optional. The Event event payload. Unset for deletion events.
	Payload       *Event `protobuf:"bytes,1,opt,name=payload,proto3,oneof" json:"payload,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *EventEventData) Reset() {
	*x = EventEventData{}
	mi := &file_cloud_visionai_v1_data_proto_msgTypes[44]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *EventEventData) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*EventEventData) ProtoMessage() {}

func (x *EventEventData) ProtoReflect() protoreflect.Message {
	mi := &file_cloud_visionai_v1_data_proto_msgTypes[44]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use EventEventData.ProtoReflect.Descriptor instead.
func (*EventEventData) Descriptor() ([]byte, []int) {
	return file_cloud_visionai_v1_data_proto_rawDescGZIP(), []int{44}
}

func (x *EventEventData) GetPayload() *Event {
	if x != nil {
		return x.Payload
	}
	return nil
}

// The data within all Process events.
type ProcessEventData struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Optional. The Process event payload. Unset for deletion events.
	Payload       *Process `protobuf:"bytes,1,opt,name=payload,proto3,oneof" json:"payload,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *ProcessEventData) Reset() {
	*x = ProcessEventData{}
	mi := &file_cloud_visionai_v1_data_proto_msgTypes[45]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *ProcessEventData) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*ProcessEventData) ProtoMessage() {}

func (x *ProcessEventData) ProtoReflect() protoreflect.Message {
	mi := &file_cloud_visionai_v1_data_proto_msgTypes[45]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use ProcessEventData.ProtoReflect.Descriptor instead.
func (*ProcessEventData) Descriptor() ([]byte, []int) {
	return file_cloud_visionai_v1_data_proto_rawDescGZIP(), []int{45}
}

func (x *ProcessEventData) GetPayload() *Process {
	if x != nil {
		return x.Payload
	}
	return nil
}

// The data within all Stream events.
type StreamEventData struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Optional. The Stream event payload. Unset for deletion events.
	Payload       *Stream `protobuf:"bytes,1,opt,name=payload,proto3,oneof" json:"payload,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *StreamEventData) Reset() {
	*x = StreamEventData{}
	mi := &file_cloud_visionai_v1_data_proto_msgTypes[46]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *StreamEventData) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*StreamEventData) ProtoMessage() {}

func (x *StreamEventData) ProtoReflect() protoreflect.Message {
	mi := &file_cloud_visionai_v1_data_proto_msgTypes[46]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use StreamEventData.ProtoReflect.Descriptor instead.
func (*StreamEventData) Descriptor() ([]byte, []int) {
	return file_cloud_visionai_v1_data_proto_rawDescGZIP(), []int{46}
}

func (x *StreamEventData) GetPayload() *Stream {
	if x != nil {
		return x.Payload
	}
	return nil
}

// The data within all Application events.
type ApplicationEventData struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Optional. The Application event payload. Unset for deletion events.
	Payload       *Application `protobuf:"bytes,1,opt,name=payload,proto3,oneof" json:"payload,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *ApplicationEventData) Reset() {
	*x = ApplicationEventData{}
	mi := &file_cloud_visionai_v1_data_proto_msgTypes[47]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *ApplicationEventData) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*ApplicationEventData) ProtoMessage() {}

func (x *ApplicationEventData) ProtoReflect() protoreflect.Message {
	mi := &file_cloud_visionai_v1_data_proto_msgTypes[47]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use ApplicationEventData.ProtoReflect.Descriptor instead.
func (*ApplicationEventData) Descriptor() ([]byte, []int) {
	return file_cloud_visionai_v1_data_proto_rawDescGZIP(), []int{47}
}

func (x *ApplicationEventData) GetPayload() *Application {
	if x != nil {
		return x.Payload
	}
	return nil
}

// The inputs to this analyzer.
//
// We accept input name references of the following form:
// <analyzer-name>:<output-argument-name>
//
// Example:
//
// Suppose you had an operator named "SomeOp" that has 2 output
// arguments, the first of which is named "foo" and the second of which is
// named "bar", and an operator named "MyOp" that accepts 2 inputs.
//
// Also suppose that there is an analyzer named "some-analyzer" that is
// running "SomeOp" and another analyzer named "my-analyzer" running "MyOp".
//
// To indicate that "my-analyzer" is to consume "some-analyzer"'s "foo"
// output as its first input and "some-analyzer"'s "bar" output as its
// second input, you can set this field to the following:
// input = ["some-analyzer:foo", "some-analyzer:bar"]
type AnalyzerDefinition_StreamInput struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// The name of the stream input (as discussed above).
	Input         string `protobuf:"bytes,1,opt,name=input,proto3" json:"input,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *AnalyzerDefinition_StreamInput) Reset() {
	*x = AnalyzerDefinition_StreamInput{}
	mi := &file_cloud_visionai_v1_data_proto_msgTypes[50]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *AnalyzerDefinition_StreamInput) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*AnalyzerDefinition_StreamInput) ProtoMessage() {}

func (x *AnalyzerDefinition_StreamInput) ProtoReflect() protoreflect.Message {
	mi := &file_cloud_visionai_v1_data_proto_msgTypes[50]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use AnalyzerDefinition_StreamInput.ProtoReflect.Descriptor instead.
func (*AnalyzerDefinition_StreamInput) Descriptor() ([]byte, []int) {
	return file_cloud_visionai_v1_data_proto_rawDescGZIP(), []int{7, 0}
}

func (x *AnalyzerDefinition_StreamInput) GetInput() string {
	if x != nil {
		return x.Input
	}
	return ""
}

// Options available for debugging purposes only.
type AnalyzerDefinition_DebugOptions struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Environment variables.
	EnvironmentVariables map[string]string `protobuf:"bytes,1,rep,name=environment_variables,json=environmentVariables,proto3" json:"environment_variables,omitempty" protobuf_key:"bytes,1,opt,name=key" protobuf_val:"bytes,2,opt,name=value"`
	unknownFields        protoimpl.UnknownFields
	sizeCache            protoimpl.SizeCache
}

func (x *AnalyzerDefinition_DebugOptions) Reset() {
	*x = AnalyzerDefinition_DebugOptions{}
	mi := &file_cloud_visionai_v1_data_proto_msgTypes[51]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *AnalyzerDefinition_DebugOptions) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*AnalyzerDefinition_DebugOptions) ProtoMessage() {}

func (x *AnalyzerDefinition_DebugOptions) ProtoReflect() protoreflect.Message {
	mi := &file_cloud_visionai_v1_data_proto_msgTypes[51]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use AnalyzerDefinition_DebugOptions.ProtoReflect.Descriptor instead.
func (*AnalyzerDefinition_DebugOptions) Descriptor() ([]byte, []int) {
	return file_cloud_visionai_v1_data_proto_rawDescGZIP(), []int{7, 1}
}

func (x *AnalyzerDefinition_DebugOptions) GetEnvironmentVariables() map[string]string {
	if x != nil {
		return x.EnvironmentVariables
	}
	return nil
}

// Message storing the runtime information of the application.
type Application_ApplicationRuntimeInfo struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Timestamp when the engine be deployed
	DeployTime *timestamppb.Timestamp `protobuf:"bytes,1,opt,name=deploy_time,json=deployTime,proto3" json:"deploy_time,omitempty"`
	// Globally created resources like warehouse dataschemas.
	GlobalOutputResources []*Application_ApplicationRuntimeInfo_GlobalOutputResource `protobuf:"bytes,3,rep,name=global_output_resources,json=globalOutputResources,proto3" json:"global_output_resources,omitempty"`
	// Monitoring-related configuration for this application.
	MonitoringConfig *Application_ApplicationRuntimeInfo_MonitoringConfig `protobuf:"bytes,4,opt,name=monitoring_config,json=monitoringConfig,proto3" json:"monitoring_config,omitempty"`
	unknownFields    protoimpl.UnknownFields
	sizeCache        protoimpl.SizeCache
}

func (x *Application_ApplicationRuntimeInfo) Reset() {
	*x = Application_ApplicationRuntimeInfo{}
	mi := &file_cloud_visionai_v1_data_proto_msgTypes[57]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *Application_ApplicationRuntimeInfo) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*Application_ApplicationRuntimeInfo) ProtoMessage() {}

func (x *Application_ApplicationRuntimeInfo) ProtoReflect() protoreflect.Message {
	mi := &file_cloud_visionai_v1_data_proto_msgTypes[57]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use Application_ApplicationRuntimeInfo.ProtoReflect.Descriptor instead.
func (*Application_ApplicationRuntimeInfo) Descriptor() ([]byte, []int) {
	return file_cloud_visionai_v1_data_proto_rawDescGZIP(), []int{12, 0}
}

func (x *Application_ApplicationRuntimeInfo) GetDeployTime() *timestamppb.Timestamp {
	if x != nil {
		return x.DeployTime
	}
	return nil
}

func (x *Application_ApplicationRuntimeInfo) GetGlobalOutputResources() []*Application_ApplicationRuntimeInfo_GlobalOutputResource {
	if x != nil {
		return x.GlobalOutputResources
	}
	return nil
}

func (x *Application_ApplicationRuntimeInfo) GetMonitoringConfig() *Application_ApplicationRuntimeInfo_MonitoringConfig {
	if x != nil {
		return x.MonitoringConfig
	}
	return nil
}

// Message about output resources from application.
type Application_ApplicationRuntimeInfo_GlobalOutputResource struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// The full resource name of the outputted resources.
	OutputResource string `protobuf:"bytes,1,opt,name=output_resource,json=outputResource,proto3" json:"output_resource,omitempty"`
	// The name of graph node who produces the output resource name.
	// For example:
	// output_resource:
	// /projects/123/locations/us-central1/corpora/my-corpus/dataSchemas/my-schema
	// producer_node: occupancy-count
	ProducerNode string `protobuf:"bytes,2,opt,name=producer_node,json=producerNode,proto3" json:"producer_node,omitempty"`
	// The key of the output resource, it has to be unique within the same
	// producer node. One producer node can output several output resources,
	// the key can be used to match corresponding output resources.
	Key           string `protobuf:"bytes,3,opt,name=key,proto3" json:"key,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *Application_ApplicationRuntimeInfo_GlobalOutputResource) Reset() {
	*x = Application_ApplicationRuntimeInfo_GlobalOutputResource{}
	mi := &file_cloud_visionai_v1_data_proto_msgTypes[59]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *Application_ApplicationRuntimeInfo_GlobalOutputResource) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*Application_ApplicationRuntimeInfo_GlobalOutputResource) ProtoMessage() {}

func (x *Application_ApplicationRuntimeInfo_GlobalOutputResource) ProtoReflect() protoreflect.Message {
	mi := &file_cloud_visionai_v1_data_proto_msgTypes[59]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use Application_ApplicationRuntimeInfo_GlobalOutputResource.ProtoReflect.Descriptor instead.
func (*Application_ApplicationRuntimeInfo_GlobalOutputResource) Descriptor() ([]byte, []int) {
	return file_cloud_visionai_v1_data_proto_rawDescGZIP(), []int{12, 0, 0}
}

func (x *Application_ApplicationRuntimeInfo_GlobalOutputResource) GetOutputResource() string {
	if x != nil {
		return x.OutputResource
	}
	return ""
}

func (x *Application_ApplicationRuntimeInfo_GlobalOutputResource) GetProducerNode() string {
	if x != nil {
		return x.ProducerNode
	}
	return ""
}

func (x *Application_ApplicationRuntimeInfo_GlobalOutputResource) GetKey() string {
	if x != nil {
		return x.Key
	}
	return ""
}

// Monitoring-related configuration for an application.
type Application_ApplicationRuntimeInfo_MonitoringConfig struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Whether this application has monitoring enabled.
	Enabled       bool `protobuf:"varint,1,opt,name=enabled,proto3" json:"enabled,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *Application_ApplicationRuntimeInfo_MonitoringConfig) Reset() {
	*x = Application_ApplicationRuntimeInfo_MonitoringConfig{}
	mi := &file_cloud_visionai_v1_data_proto_msgTypes[60]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *Application_ApplicationRuntimeInfo_MonitoringConfig) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*Application_ApplicationRuntimeInfo_MonitoringConfig) ProtoMessage() {}

func (x *Application_ApplicationRuntimeInfo_MonitoringConfig) ProtoReflect() protoreflect.Message {
	mi := &file_cloud_visionai_v1_data_proto_msgTypes[60]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use Application_ApplicationRuntimeInfo_MonitoringConfig.ProtoReflect.Descriptor instead.
func (*Application_ApplicationRuntimeInfo_MonitoringConfig) Descriptor() ([]byte, []int) {
	return file_cloud_visionai_v1_data_proto_rawDescGZIP(), []int{12, 0, 1}
}

func (x *Application_ApplicationRuntimeInfo_MonitoringConfig) GetEnabled() bool {
	if x != nil {
		return x.Enabled
	}
	return false
}

// Message describing one edge pointing into a node.
type Node_InputEdge struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// The name of the parent node.
	ParentNode string `protobuf:"bytes,1,opt,name=parent_node,json=parentNode,proto3" json:"parent_node,omitempty"`
	// The connected output artifact of the parent node.
	// It can be omitted if target processor only has 1 output artifact.
	ParentOutputChannel string `protobuf:"bytes,2,opt,name=parent_output_channel,json=parentOutputChannel,proto3" json:"parent_output_channel,omitempty"`
	// The connected input channel of the current node's processor.
	// It can be omitted if target processor only has 1 input channel.
	ConnectedInputChannel string `protobuf:"bytes,3,opt,name=connected_input_channel,json=connectedInputChannel,proto3" json:"connected_input_channel,omitempty"`
	unknownFields         protoimpl.UnknownFields
	sizeCache             protoimpl.SizeCache
}

func (x *Node_InputEdge) Reset() {
	*x = Node_InputEdge{}
	mi := &file_cloud_visionai_v1_data_proto_msgTypes[61]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *Node_InputEdge) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*Node_InputEdge) ProtoMessage() {}

func (x *Node_InputEdge) ProtoReflect() protoreflect.Message {
	mi := &file_cloud_visionai_v1_data_proto_msgTypes[61]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use Node_InputEdge.ProtoReflect.Descriptor instead.
func (*Node_InputEdge) Descriptor() ([]byte, []int) {
	return file_cloud_visionai_v1_data_proto_rawDescGZIP(), []int{14, 0}
}

func (x *Node_InputEdge) GetParentNode() string {
	if x != nil {
		return x.ParentNode
	}
	return ""
}

func (x *Node_InputEdge) GetParentOutputChannel() string {
	if x != nil {
		return x.ParentOutputChannel
	}
	return ""
}

func (x *Node_InputEdge) GetConnectedInputChannel() string {
	if x != nil {
		return x.ConnectedInputChannel
	}
	return ""
}

// Message for input channel specification.
type ProcessorIOSpec_GraphInputChannelSpec struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// The name of the current input channel.
	Name string `protobuf:"bytes,1,opt,name=name,proto3" json:"name,omitempty"`
	// The data types of the current input channel.
	// When this field has more than 1 value, it means this input channel can be
	// connected to either of these different data types.
	DataType DataType `protobuf:"varint,2,opt,name=data_type,json=dataType,proto3,enum=google.events.cloud.visionai.v1.DataType" json:"data_type,omitempty"`
	// If specified, only those detailed data types can be connected to the
	// processor. For example, jpeg stream for MEDIA, or PredictionResult proto
	// for PROTO type. If unspecified, then any proto is accepted.
	AcceptedDataTypeUris []string `protobuf:"bytes,5,rep,name=accepted_data_type_uris,json=acceptedDataTypeUris,proto3" json:"accepted_data_type_uris,omitempty"`
	// Whether the current input channel is required by the processor.
	// For example, for a processor with required video input and optional audio
	// input, if video input is missing, the application will be rejected while
	// the audio input can be missing as long as the video input exists.
	Required bool `protobuf:"varint,3,opt,name=required,proto3" json:"required,omitempty"`
	// How many input edges can be connected to this input channel. 0 means
	// unlimited.
	MaxConnectionAllowed int64 `protobuf:"varint,4,opt,name=max_connection_allowed,json=maxConnectionAllowed,proto3" json:"max_connection_allowed,omitempty"`
	unknownFields        protoimpl.UnknownFields
	sizeCache            protoimpl.SizeCache
}

func (x *ProcessorIOSpec_GraphInputChannelSpec) Reset() {
	*x = ProcessorIOSpec_GraphInputChannelSpec{}
	mi := &file_cloud_visionai_v1_data_proto_msgTypes[64]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *ProcessorIOSpec_GraphInputChannelSpec) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*ProcessorIOSpec_GraphInputChannelSpec) ProtoMessage() {}

func (x *ProcessorIOSpec_GraphInputChannelSpec) ProtoReflect() protoreflect.Message {
	mi := &file_cloud_visionai_v1_data_proto_msgTypes[64]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use ProcessorIOSpec_GraphInputChannelSpec.ProtoReflect.Descriptor instead.
func (*ProcessorIOSpec_GraphInputChannelSpec) Descriptor() ([]byte, []int) {
	return file_cloud_visionai_v1_data_proto_rawDescGZIP(), []int{17, 0}
}

func (x *ProcessorIOSpec_GraphInputChannelSpec) GetName() string {
	if x != nil {
		return x.Name
	}
	return ""
}

func (x *ProcessorIOSpec_GraphInputChannelSpec) GetDataType() DataType {
	if x != nil {
		return x.DataType
	}
	return DataType_DATA_TYPE_UNSPECIFIED
}

func (x *ProcessorIOSpec_GraphInputChannelSpec) GetAcceptedDataTypeUris() []string {
	if x != nil {
		return x.AcceptedDataTypeUris
	}
	return nil
}

func (x *ProcessorIOSpec_GraphInputChannelSpec) GetRequired() bool {
	if x != nil {
		return x.Required
	}
	return false
}

func (x *ProcessorIOSpec_GraphInputChannelSpec) GetMaxConnectionAllowed() int64 {
	if x != nil {
		return x.MaxConnectionAllowed
	}
	return 0
}

// Message for output channel specification.
type ProcessorIOSpec_GraphOutputChannelSpec struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// The name of the current output channel.
	Name string `protobuf:"bytes,1,opt,name=name,proto3" json:"name,omitempty"`
	// The data type of the current output channel.
	DataType      DataType `protobuf:"varint,2,opt,name=data_type,json=dataType,proto3,enum=google.events.cloud.visionai.v1.DataType" json:"data_type,omitempty"`
	DataTypeUri   string   `protobuf:"bytes,3,opt,name=data_type_uri,json=dataTypeUri,proto3" json:"data_type_uri,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *ProcessorIOSpec_GraphOutputChannelSpec) Reset() {
	*x = ProcessorIOSpec_GraphOutputChannelSpec{}
	mi := &file_cloud_visionai_v1_data_proto_msgTypes[65]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *ProcessorIOSpec_GraphOutputChannelSpec) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*ProcessorIOSpec_GraphOutputChannelSpec) ProtoMessage() {}

func (x *ProcessorIOSpec_GraphOutputChannelSpec) ProtoReflect() protoreflect.Message {
	mi := &file_cloud_visionai_v1_data_proto_msgTypes[65]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use ProcessorIOSpec_GraphOutputChannelSpec.ProtoReflect.Descriptor instead.
func (*ProcessorIOSpec_GraphOutputChannelSpec) Descriptor() ([]byte, []int) {
	return file_cloud_visionai_v1_data_proto_rawDescGZIP(), []int{17, 1}
}

func (x *ProcessorIOSpec_GraphOutputChannelSpec) GetName() string {
	if x != nil {
		return x.Name
	}
	return ""
}

func (x *ProcessorIOSpec_GraphOutputChannelSpec) GetDataType() DataType {
	if x != nil {
		return x.DataType
	}
	return DataType_DATA_TYPE_UNSPECIFIED
}

func (x *ProcessorIOSpec_GraphOutputChannelSpec) GetDataTypeUri() string {
	if x != nil {
		return x.DataTypeUri
	}
	return ""
}

// Message for instance resource channel specification.
// External resources are virtual nodes which are not expressed in the
// application graph. Each processor expresses its out-graph spec, so customer
// is able to override the external source or destinations to the
type ProcessorIOSpec_InstanceResourceInputBindingSpec struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Types that are valid to be assigned to ResourceType:
	//
	//	*ProcessorIOSpec_InstanceResourceInputBindingSpec_ConfigTypeUri
	//	*ProcessorIOSpec_InstanceResourceInputBindingSpec_ResourceTypeUri
	ResourceType isProcessorIOSpec_InstanceResourceInputBindingSpec_ResourceType `protobuf_oneof:"resource_type"`
	// Name of the input binding, unique within the processor.
	Name          string `protobuf:"bytes,1,opt,name=name,proto3" json:"name,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *ProcessorIOSpec_InstanceResourceInputBindingSpec) Reset() {
	*x = ProcessorIOSpec_InstanceResourceInputBindingSpec{}
	mi := &file_cloud_visionai_v1_data_proto_msgTypes[66]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *ProcessorIOSpec_InstanceResourceInputBindingSpec) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*ProcessorIOSpec_InstanceResourceInputBindingSpec) ProtoMessage() {}

func (x *ProcessorIOSpec_InstanceResourceInputBindingSpec) ProtoReflect() protoreflect.Message {
	mi := &file_cloud_visionai_v1_data_proto_msgTypes[66]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use ProcessorIOSpec_InstanceResourceInputBindingSpec.ProtoReflect.Descriptor instead.
func (*ProcessorIOSpec_InstanceResourceInputBindingSpec) Descriptor() ([]byte, []int) {
	return file_cloud_visionai_v1_data_proto_rawDescGZIP(), []int{17, 2}
}

func (x *ProcessorIOSpec_InstanceResourceInputBindingSpec) GetResourceType() isProcessorIOSpec_InstanceResourceInputBindingSpec_ResourceType {
	if x != nil {
		return x.ResourceType
	}
	return nil
}

func (x *ProcessorIOSpec_InstanceResourceInputBindingSpec) GetConfigTypeUri() string {
	if x != nil {
		if x, ok := x.ResourceType.(*ProcessorIOSpec_InstanceResourceInputBindingSpec_ConfigTypeUri); ok {
			return x.ConfigTypeUri
		}
	}
	return ""
}

func (x *ProcessorIOSpec_InstanceResourceInputBindingSpec) GetResourceTypeUri() string {
	if x != nil {
		if x, ok := x.ResourceType.(*ProcessorIOSpec_InstanceResourceInputBindingSpec_ResourceTypeUri); ok {
			return x.ResourceTypeUri
		}
	}
	return ""
}

func (x *ProcessorIOSpec_InstanceResourceInputBindingSpec) GetName() string {
	if x != nil {
		return x.Name
	}
	return ""
}

type isProcessorIOSpec_InstanceResourceInputBindingSpec_ResourceType interface {
	isProcessorIOSpec_InstanceResourceInputBindingSpec_ResourceType()
}

type ProcessorIOSpec_InstanceResourceInputBindingSpec_ConfigTypeUri struct {
	// The configuration proto that includes the Googleapis resources. I.e.
	// type.googleapis.com/google.cloud.vision.v1.StreamWithAnnotation
	ConfigTypeUri string `protobuf:"bytes,2,opt,name=config_type_uri,json=configTypeUri,proto3,oneof"`
}

type ProcessorIOSpec_InstanceResourceInputBindingSpec_ResourceTypeUri struct {
	// The direct type url of Googleapis resource. i.e.
	// type.googleapis.com/google.cloud.vision.v1.Asset
	ResourceTypeUri string `protobuf:"bytes,3,opt,name=resource_type_uri,json=resourceTypeUri,proto3,oneof"`
}

func (*ProcessorIOSpec_InstanceResourceInputBindingSpec_ConfigTypeUri) isProcessorIOSpec_InstanceResourceInputBindingSpec_ResourceType() {
}

func (*ProcessorIOSpec_InstanceResourceInputBindingSpec_ResourceTypeUri) isProcessorIOSpec_InstanceResourceInputBindingSpec_ResourceType() {
}

type ProcessorIOSpec_InstanceResourceOutputBindingSpec struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Name of the output binding, unique within the processor.
	Name string `protobuf:"bytes,1,opt,name=name,proto3" json:"name,omitempty"`
	// The resource type uri of the acceptable output resource.
	ResourceTypeUri string `protobuf:"bytes,2,opt,name=resource_type_uri,json=resourceTypeUri,proto3" json:"resource_type_uri,omitempty"`
	// Whether the output resource needs to be explicitly set in the instance.
	// If it is false, the processor will automatically generate it if required.
	Explicit      bool `protobuf:"varint,3,opt,name=explicit,proto3" json:"explicit,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *ProcessorIOSpec_InstanceResourceOutputBindingSpec) Reset() {
	*x = ProcessorIOSpec_InstanceResourceOutputBindingSpec{}
	mi := &file_cloud_visionai_v1_data_proto_msgTypes[67]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *ProcessorIOSpec_InstanceResourceOutputBindingSpec) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*ProcessorIOSpec_InstanceResourceOutputBindingSpec) ProtoMessage() {}

func (x *ProcessorIOSpec_InstanceResourceOutputBindingSpec) ProtoReflect() protoreflect.Message {
	mi := &file_cloud_visionai_v1_data_proto_msgTypes[67]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use ProcessorIOSpec_InstanceResourceOutputBindingSpec.ProtoReflect.Descriptor instead.
func (*ProcessorIOSpec_InstanceResourceOutputBindingSpec) Descriptor() ([]byte, []int) {
	return file_cloud_visionai_v1_data_proto_rawDescGZIP(), []int{17, 3}
}

func (x *ProcessorIOSpec_InstanceResourceOutputBindingSpec) GetName() string {
	if x != nil {
		return x.Name
	}
	return ""
}

func (x *ProcessorIOSpec_InstanceResourceOutputBindingSpec) GetResourceTypeUri() string {
	if x != nil {
		return x.ResourceTypeUri
	}
	return ""
}

func (x *ProcessorIOSpec_InstanceResourceOutputBindingSpec) GetExplicit() bool {
	if x != nil {
		return x.Explicit
	}
	return false
}

// The schema is defined as an OpenAPI 3.0.2 [Schema
// Object](https://github.com/OAI/OpenAPI-Specification/blob/main/versions/3.0.2.md#schemaObject).
type CustomProcessorSourceInfo_ModelSchema struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Cloud Storage location to a YAML file that defines the format of a single
	// instance used in prediction and explanation requests.
	InstancesSchema *GcsSource `protobuf:"bytes,1,opt,name=instances_schema,json=instancesSchema,proto3" json:"instances_schema,omitempty"`
	// Cloud Storage location to a YAML file that defines the prediction and
	// explanation parameters.
	ParametersSchema *GcsSource `protobuf:"bytes,2,opt,name=parameters_schema,json=parametersSchema,proto3" json:"parameters_schema,omitempty"`
	// Cloud Storage location to a YAML file that defines the format of a single
	// prediction or explanation.
	PredictionsSchema *GcsSource `protobuf:"bytes,3,opt,name=predictions_schema,json=predictionsSchema,proto3" json:"predictions_schema,omitempty"`
	unknownFields     protoimpl.UnknownFields
	sizeCache         protoimpl.SizeCache
}

func (x *CustomProcessorSourceInfo_ModelSchema) Reset() {
	*x = CustomProcessorSourceInfo_ModelSchema{}
	mi := &file_cloud_visionai_v1_data_proto_msgTypes[68]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *CustomProcessorSourceInfo_ModelSchema) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*CustomProcessorSourceInfo_ModelSchema) ProtoMessage() {}

func (x *CustomProcessorSourceInfo_ModelSchema) ProtoReflect() protoreflect.Message {
	mi := &file_cloud_visionai_v1_data_proto_msgTypes[68]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use CustomProcessorSourceInfo_ModelSchema.ProtoReflect.Descriptor instead.
func (*CustomProcessorSourceInfo_ModelSchema) Descriptor() ([]byte, []int) {
	return file_cloud_visionai_v1_data_proto_rawDescGZIP(), []int{18, 0}
}

func (x *CustomProcessorSourceInfo_ModelSchema) GetInstancesSchema() *GcsSource {
	if x != nil {
		return x.InstancesSchema
	}
	return nil
}

func (x *CustomProcessorSourceInfo_ModelSchema) GetParametersSchema() *GcsSource {
	if x != nil {
		return x.ParametersSchema
	}
	return nil
}

func (x *CustomProcessorSourceInfo_ModelSchema) GetPredictionsSchema() *GcsSource {
	if x != nil {
		return x.PredictionsSchema
	}
	return nil
}

// Message describing annotations specific to application node.
type StreamWithAnnotation_NodeAnnotation struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// The node name of the application graph.
	Node string `protobuf:"bytes,1,opt,name=node,proto3" json:"node,omitempty"`
	// The node specific stream annotations.
	Annotations   []*StreamAnnotation `protobuf:"bytes,2,rep,name=annotations,proto3" json:"annotations,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *StreamWithAnnotation_NodeAnnotation) Reset() {
	*x = StreamWithAnnotation_NodeAnnotation{}
	mi := &file_cloud_visionai_v1_data_proto_msgTypes[70]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *StreamWithAnnotation_NodeAnnotation) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*StreamWithAnnotation_NodeAnnotation) ProtoMessage() {}

func (x *StreamWithAnnotation_NodeAnnotation) ProtoReflect() protoreflect.Message {
	mi := &file_cloud_visionai_v1_data_proto_msgTypes[70]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use StreamWithAnnotation_NodeAnnotation.ProtoReflect.Descriptor instead.
func (*StreamWithAnnotation_NodeAnnotation) Descriptor() ([]byte, []int) {
	return file_cloud_visionai_v1_data_proto_rawDescGZIP(), []int{20, 0}
}

func (x *StreamWithAnnotation_NodeAnnotation) GetNode() string {
	if x != nil {
		return x.Node
	}
	return ""
}

func (x *StreamWithAnnotation_NodeAnnotation) GetAnnotations() []*StreamAnnotation {
	if x != nil {
		return x.Annotations
	}
	return nil
}

var File_cloud_visionai_v1_data_proto protoreflect.FileDescriptor

const file_cloud_visionai_v1_data_proto_rawDesc = "" +
	"\n" +
	"\x1ccloud/visionai/v1/data.proto\x12\x1fgoogle.events.cloud.visionai.v1\x1a\x1egoogle/protobuf/duration.proto\x1a\x1cgoogle/protobuf/struct.proto\x1a\x1fgoogle/protobuf/timestamp.proto\"\xfe\x02\n" +
	"\x10StreamAnnotation\x12U\n" +
	"\vactive_zone\x18\x05 \x01(\v22.google.events.cloud.visionai.v1.NormalizedPolygonH\x00R\n" +
	"activeZone\x12Z\n" +
	"\rcrossing_line\x18\x06 \x01(\v23.google.events.cloud.visionai.v1.NormalizedPolylineH\x00R\fcrossingLine\x12\x0e\n" +
	"\x02id\x18\x01 \x01(\tR\x02id\x12!\n" +
	"\fdisplay_name\x18\x02 \x01(\tR\vdisplayName\x12#\n" +
	"\rsource_stream\x18\x03 \x01(\tR\fsourceStream\x12I\n" +
	"\x04type\x18\x04 \x01(\x0e25.google.events.cloud.visionai.v1.StreamAnnotationTypeR\x04typeB\x14\n" +
	"\x12annotation_payload\"w\n" +
	"\x11NormalizedPolygon\x12b\n" +
	"\x13normalized_vertices\x18\x01 \x03(\v21.google.events.cloud.visionai.v1.NormalizedVertexR\x12normalizedVertices\"x\n" +
	"\x12NormalizedPolyline\x12b\n" +
	"\x13normalized_vertices\x18\x01 \x03(\v21.google.events.cloud.visionai.v1.NormalizedVertexR\x12normalizedVertices\".\n" +
	"\x10NormalizedVertex\x12\f\n" +
	"\x01x\x18\x01 \x01(\x02R\x01x\x12\f\n" +
	"\x01y\x18\x02 \x01(\x02R\x01y\"\xb8\x05\n" +
	"\aCluster\x12\x12\n" +
	"\x04name\x18\x01 \x01(\tR\x04name\x12;\n" +
	"\vcreate_time\x18\x02 \x01(\v2\x1a.google.protobuf.TimestampR\n" +
	"createTime\x12;\n" +
	"\vupdate_time\x18\x03 \x01(\v2\x1a.google.protobuf.TimestampR\n" +
	"updateTime\x12L\n" +
	"\x06labels\x18\x04 \x03(\v24.google.events.cloud.visionai.v1.Cluster.LabelsEntryR\x06labels\x12[\n" +
	"\vannotations\x18\x05 \x03(\v29.google.events.cloud.visionai.v1.Cluster.AnnotationsEntryR\vannotations\x12<\n" +
	"\x1adataplane_service_endpoint\x18\x06 \x01(\tR\x18dataplaneServiceEndpoint\x12D\n" +
	"\x05state\x18\a \x01(\x0e2..google.events.cloud.visionai.v1.Cluster.StateR\x05state\x12\x1d\n" +
	"\n" +
	"psc_target\x18\b \x01(\tR\tpscTarget\x1a9\n" +
	"\vLabelsEntry\x12\x10\n" +
	"\x03key\x18\x01 \x01(\tR\x03key\x12\x14\n" +
	"\x05value\x18\x02 \x01(\tR\x05value:\x028\x01\x1a>\n" +
	"\x10AnnotationsEntry\x12\x10\n" +
	"\x03key\x18\x01 \x01(\tR\x03key\x12\x14\n" +
	"\x05value\x18\x02 \x01(\tR\x05value:\x028\x01\"V\n" +
	"\x05State\x12\x15\n" +
	"\x11STATE_UNSPECIFIED\x10\x00\x12\x10\n" +
	"\fPROVISIONING\x10\x01\x12\v\n" +
	"\aRUNNING\x10\x02\x12\f\n" +
	"\bSTOPPING\x10\x03\x12\t\n" +
	"\x05ERROR\x10\x04\"\x1f\n" +
	"\tGcsSource\x12\x12\n" +
	"\x04uris\x18\x01 \x03(\tR\x04uris\"Y\n" +
	"\x0eAttributeValue\x12\x0e\n" +
	"\x01i\x18\x01 \x01(\x03H\x00R\x01i\x12\x0e\n" +
	"\x01f\x18\x02 \x01(\x02H\x00R\x01f\x12\x0e\n" +
	"\x01b\x18\x03 \x01(\bH\x00R\x01b\x12\x0e\n" +
	"\x01s\x18\x04 \x01(\fH\x00R\x01sB\a\n" +
	"\x05value\"\xde\x05\n" +
	"\x12AnalyzerDefinition\x12\x1a\n" +
	"\banalyzer\x18\x01 \x01(\tR\banalyzer\x12\x1a\n" +
	"\boperator\x18\x02 \x01(\tR\boperator\x12W\n" +
	"\x06inputs\x18\x03 \x03(\v2?.google.events.cloud.visionai.v1.AnalyzerDefinition.StreamInputR\x06inputs\x12T\n" +
	"\x05attrs\x18\x04 \x03(\v2>.google.events.cloud.visionai.v1.AnalyzerDefinition.AttrsEntryR\x05attrs\x12e\n" +
	"\rdebug_options\x18\x05 \x01(\v2@.google.events.cloud.visionai.v1.AnalyzerDefinition.DebugOptionsR\fdebugOptions\x1a#\n" +
	"\vStreamInput\x12\x14\n" +
	"\x05input\x18\x01 \x01(\tR\x05input\x1a\xe9\x01\n" +
	"\fDebugOptions\x12\x8f\x01\n" +
	"\x15environment_variables\x18\x01 \x03(\v2Z.google.events.cloud.visionai.v1.AnalyzerDefinition.DebugOptions.EnvironmentVariablesEntryR\x14environmentVariables\x1aG\n" +
	"\x19EnvironmentVariablesEntry\x12\x10\n" +
	"\x03key\x18\x01 \x01(\tR\x03key\x12\x14\n" +
	"\x05value\x18\x02 \x01(\tR\x05value:\x028\x01\x1ai\n" +
	"\n" +
	"AttrsEntry\x12\x10\n" +
	"\x03key\x18\x01 \x01(\tR\x03key\x12E\n" +
	"\x05value\x18\x02 \x01(\v2/.google.events.cloud.visionai.v1.AttributeValueR\x05value:\x028\x01\"g\n" +
	"\x12AnalysisDefinition\x12Q\n" +
	"\tanalyzers\x18\x01 \x03(\v23.google.events.cloud.visionai.v1.AnalyzerDefinitionR\tanalyzers\"\xd2\x01\n" +
	"\tRunStatus\x12F\n" +
	"\x05state\x18\x01 \x01(\x0e20.google.events.cloud.visionai.v1.RunStatus.StateR\x05state\x12\x16\n" +
	"\x06reason\x18\x02 \x01(\tR\x06reason\"e\n" +
	"\x05State\x12\x15\n" +
	"\x11STATE_UNSPECIFIED\x10\x00\x12\x10\n" +
	"\fINITIALIZING\x10\x01\x12\v\n" +
	"\aRUNNING\x10\x02\x12\r\n" +
	"\tCOMPLETED\x10\x03\x12\n" +
	"\n" +
	"\x06FAILED\x10\x04\x12\v\n" +
	"\aPENDING\x10\x05\"\xbc\x06\n" +
	"\bAnalysis\x12\x12\n" +
	"\x04name\x18\x01 \x01(\tR\x04name\x12;\n" +
	"\vcreate_time\x18\x02 \x01(\v2\x1a.google.protobuf.TimestampR\n" +
	"createTime\x12;\n" +
	"\vupdate_time\x18\x03 \x01(\v2\x1a.google.protobuf.TimestampR\n" +
	"updateTime\x12M\n" +
	"\x06labels\x18\x04 \x03(\v25.google.events.cloud.visionai.v1.Analysis.LabelsEntryR\x06labels\x12d\n" +
	"\x13analysis_definition\x18\x05 \x01(\v23.google.events.cloud.visionai.v1.AnalysisDefinitionR\x12analysisDefinition\x12v\n" +
	"\x15input_streams_mapping\x18\x06 \x03(\v2B.google.events.cloud.visionai.v1.Analysis.InputStreamsMappingEntryR\x13inputStreamsMapping\x12y\n" +
	"\x16output_streams_mapping\x18\a \x03(\v2C.google.events.cloud.visionai.v1.Analysis.OutputStreamsMappingEntryR\x14outputStreamsMapping\x12.\n" +
	"\x13disable_event_watch\x18\b \x01(\bR\x11disableEventWatch\x1a9\n" +
	"\vLabelsEntry\x12\x10\n" +
	"\x03key\x18\x01 \x01(\tR\x03key\x12\x14\n" +
	"\x05value\x18\x02 \x01(\tR\x05value:\x028\x01\x1aF\n" +
	"\x18InputStreamsMappingEntry\x12\x10\n" +
	"\x03key\x18\x01 \x01(\tR\x03key\x12\x14\n" +
	"\x05value\x18\x02 \x01(\tR\x05value:\x028\x01\x1aG\n" +
	"\x19OutputStreamsMappingEntry\x12\x10\n" +
	"\x03key\x18\x01 \x01(\tR\x03key\x12\x14\n" +
	"\x05value\x18\x02 \x01(\tR\x05value:\x028\x01\"\xcb\x03\n" +
	"\aProcess\x12\x12\n" +
	"\x04name\x18\x01 \x01(\tR\x04name\x12;\n" +
	"\vcreate_time\x18\x02 \x01(\v2\x1a.google.protobuf.TimestampR\n" +
	"createTime\x12;\n" +
	"\vupdate_time\x18\x03 \x01(\v2\x1a.google.protobuf.TimestampR\n" +
	"updateTime\x12\x1a\n" +
	"\banalysis\x18\x04 \x01(\tR\banalysis\x12/\n" +
	"\x13attribute_overrides\x18\x05 \x03(\tR\x12attributeOverrides\x12I\n" +
	"\n" +
	"run_status\x18\x06 \x01(\v2*.google.events.cloud.visionai.v1.RunStatusR\trunStatus\x12C\n" +
	"\brun_mode\x18\a \x01(\x0e2(.google.events.cloud.visionai.v1.RunModeR\arunMode\x12\x19\n" +
	"\bevent_id\x18\b \x01(\tR\aeventId\x12\x19\n" +
	"\bbatch_id\x18\t \x01(\tR\abatchId\x12\x1f\n" +
	"\vretry_count\x18\n" +
	" \x01(\x05R\n" +
	"retryCount\"\xe5\v\n" +
	"\vApplication\x12\x12\n" +
	"\x04name\x18\x01 \x01(\tR\x04name\x12;\n" +
	"\vcreate_time\x18\x02 \x01(\v2\x1a.google.protobuf.TimestampR\n" +
	"createTime\x12;\n" +
	"\vupdate_time\x18\x03 \x01(\v2\x1a.google.protobuf.TimestampR\n" +
	"updateTime\x12P\n" +
	"\x06labels\x18\x04 \x03(\v28.google.events.cloud.visionai.v1.Application.LabelsEntryR\x06labels\x12!\n" +
	"\fdisplay_name\x18\x05 \x01(\tR\vdisplayName\x12 \n" +
	"\vdescription\x18\x06 \x01(\tR\vdescription\x12d\n" +
	"\x13application_configs\x18\a \x01(\v23.google.events.cloud.visionai.v1.ApplicationConfigsR\x12applicationConfigs\x12f\n" +
	"\fruntime_info\x18\b \x01(\v2C.google.events.cloud.visionai.v1.Application.ApplicationRuntimeInfoR\vruntimeInfo\x12H\n" +
	"\x05state\x18\t \x01(\x0e22.google.events.cloud.visionai.v1.Application.StateR\x05state\x12[\n" +
	"\fbilling_mode\x18\f \x01(\x0e28.google.events.cloud.visionai.v1.Application.BillingModeR\vbillingMode\x1a\x92\x04\n" +
	"\x16ApplicationRuntimeInfo\x12;\n" +
	"\vdeploy_time\x18\x01 \x01(\v2\x1a.google.protobuf.TimestampR\n" +
	"deployTime\x12\x90\x01\n" +
	"\x17global_output_resources\x18\x03 \x03(\v2X.google.events.cloud.visionai.v1.Application.ApplicationRuntimeInfo.GlobalOutputResourceR\x15globalOutputResources\x12\x81\x01\n" +
	"\x11monitoring_config\x18\x04 \x01(\v2T.google.events.cloud.visionai.v1.Application.ApplicationRuntimeInfo.MonitoringConfigR\x10monitoringConfig\x1av\n" +
	"\x14GlobalOutputResource\x12'\n" +
	"\x0foutput_resource\x18\x01 \x01(\tR\x0eoutputResource\x12#\n" +
	"\rproducer_node\x18\x02 \x01(\tR\fproducerNode\x12\x10\n" +
	"\x03key\x18\x03 \x01(\tR\x03key\x1a,\n" +
	"\x10MonitoringConfig\x12\x18\n" +
	"\aenabled\x18\x01 \x01(\bR\aenabled\x1a9\n" +
	"\vLabelsEntry\x12\x10\n" +
	"\x03key\x18\x01 \x01(\tR\x03key\x12\x14\n" +
	"\x05value\x18\x02 \x01(\tR\x05value:\x028\x01\"\xa7\x01\n" +
	"\x05State\x12\x15\n" +
	"\x11STATE_UNSPECIFIED\x10\x00\x12\v\n" +
	"\aCREATED\x10\x01\x12\r\n" +
	"\tDEPLOYING\x10\x02\x12\f\n" +
	"\bDEPLOYED\x10\x03\x12\x0f\n" +
	"\vUNDEPLOYING\x10\x04\x12\v\n" +
	"\aDELETED\x10\x05\x12\t\n" +
	"\x05ERROR\x10\x06\x12\f\n" +
	"\bCREATING\x10\a\x12\f\n" +
	"\bUPDATING\x10\b\x12\f\n" +
	"\bDELETING\x10\t\x12\n" +
	"\n" +
	"\x06FIXING\x10\n" +
	"\"B\n" +
	"\vBillingMode\x12\x1c\n" +
	"\x18BILLING_MODE_UNSPECIFIED\x10\x00\x12\b\n" +
	"\x04PAYG\x10\x01\x12\v\n" +
	"\aMONTHLY\x10\x02\"Q\n" +
	"\x12ApplicationConfigs\x12;\n" +
	"\x05nodes\x18\x01 \x03(\v2%.google.events.cloud.visionai.v1.NodeR\x05nodes\"\xfd\x03\n" +
	"\x04Node\x12O\n" +
	"$output_all_output_channels_to_stream\x18\x06 \x01(\bH\x00R\x1foutputAllOutputChannelsToStream\x12\x12\n" +
	"\x04name\x18\x01 \x01(\tR\x04name\x12!\n" +
	"\fdisplay_name\x18\x02 \x01(\tR\vdisplayName\x12Q\n" +
	"\vnode_config\x18\x03 \x01(\v20.google.events.cloud.visionai.v1.ProcessorConfigR\n" +
	"nodeConfig\x12\x1c\n" +
	"\tprocessor\x18\x04 \x01(\tR\tprocessor\x12I\n" +
	"\aparents\x18\x05 \x03(\v2/.google.events.cloud.visionai.v1.Node.InputEdgeR\aparents\x1a\x98\x01\n" +
	"\tInputEdge\x12\x1f\n" +
	"\vparent_node\x18\x01 \x01(\tR\n" +
	"parentNode\x122\n" +
	"\x15parent_output_channel\x18\x02 \x01(\tR\x13parentOutputChannel\x126\n" +
	"\x17connected_input_channel\x18\x03 \x01(\tR\x15connectedInputChannelB\x16\n" +
	"\x14stream_output_config\"\xd2\x03\n" +
	"\x05Draft\x12\x12\n" +
	"\x04name\x18\x01 \x01(\tR\x04name\x12;\n" +
	"\vcreate_time\x18\x02 \x01(\v2\x1a.google.protobuf.TimestampR\n" +
	"createTime\x12;\n" +
	"\vupdate_time\x18\a \x01(\v2\x1a.google.protobuf.TimestampR\n" +
	"updateTime\x12J\n" +
	"\x06labels\x18\x03 \x03(\v22.google.events.cloud.visionai.v1.Draft.LabelsEntryR\x06labels\x12!\n" +
	"\fdisplay_name\x18\x04 \x01(\tR\vdisplayName\x12 \n" +
	"\vdescription\x18\x05 \x01(\tR\vdescription\x12o\n" +
	"\x19draft_application_configs\x18\x06 \x01(\v23.google.events.cloud.visionai.v1.ApplicationConfigsR\x17draftApplicationConfigs\x1a9\n" +
	"\vLabelsEntry\x12\x10\n" +
	"\x03key\x18\x01 \x01(\tR\x03key\x12\x14\n" +
	"\x05value\x18\x02 \x01(\tR\x05value:\x028\x01\"\xe8\t\n" +
	"\tProcessor\x12\x12\n" +
	"\x04name\x18\x01 \x01(\tR\x04name\x12;\n" +
	"\vcreate_time\x18\x02 \x01(\v2\x1a.google.protobuf.TimestampR\n" +
	"createTime\x12;\n" +
	"\vupdate_time\x18\x03 \x01(\v2\x1a.google.protobuf.TimestampR\n" +
	"updateTime\x12N\n" +
	"\x06labels\x18\x04 \x03(\v26.google.events.cloud.visionai.v1.Processor.LabelsEntryR\x06labels\x12!\n" +
	"\fdisplay_name\x18\x05 \x01(\tR\vdisplayName\x12 \n" +
	"\vdescription\x18\n" +
	" \x01(\tR\vdescription\x12_\n" +
	"\x0eprocessor_type\x18\x06 \x01(\x0e28.google.events.cloud.visionai.v1.Processor.ProcessorTypeR\rprocessorType\x12I\n" +
	"\n" +
	"model_type\x18\r \x01(\x0e2*.google.events.cloud.visionai.v1.ModelTypeR\tmodelType\x12{\n" +
	"\x1ccustom_processor_source_info\x18\a \x01(\v2:.google.events.cloud.visionai.v1.CustomProcessorSourceInfoR\x19customProcessorSourceInfo\x12O\n" +
	"\x05state\x18\b \x01(\x0e29.google.events.cloud.visionai.v1.Processor.ProcessorStateR\x05state\x12\\\n" +
	"\x11processor_io_spec\x18\v \x01(\v20.google.events.cloud.visionai.v1.ProcessorIOSpecR\x0fprocessorIoSpec\x123\n" +
	"\x15configuration_typeurl\x18\x0e \x01(\tR\x14configurationTypeurl\x12s\n" +
	"\x1asupported_annotation_types\x18\x0f \x03(\x0e25.google.events.cloud.visionai.v1.StreamAnnotationTypeR\x18supportedAnnotationTypes\x128\n" +
	"\x18supports_post_processing\x18\x11 \x01(\bR\x16supportsPostProcessing\x1a9\n" +
	"\vLabelsEntry\x12\x10\n" +
	"\x03key\x18\x01 \x01(\tR\x03key\x12\x14\n" +
	"\x05value\x18\x02 \x01(\tR\x05value:\x028\x01\"Z\n" +
	"\rProcessorType\x12\x1e\n" +
	"\x1aPROCESSOR_TYPE_UNSPECIFIED\x10\x00\x12\x0e\n" +
	"\n" +
	"PRETRAINED\x10\x01\x12\n" +
	"\n" +
	"\x06CUSTOM\x10\x02\x12\r\n" +
	"\tCONNECTOR\x10\x03\"e\n" +
	"\x0eProcessorState\x12\x1f\n" +
	"\x1bPROCESSOR_STATE_UNSPECIFIED\x10\x00\x12\f\n" +
	"\bCREATING\x10\x01\x12\n" +
	"\n" +
	"\x06ACTIVE\x10\x02\x12\f\n" +
	"\bDELETING\x10\x03\x12\n" +
	"\n" +
	"\x06FAILED\x10\x04\"\xa8\n" +
	"\n" +
	"\x0fProcessorIOSpec\x12\x81\x01\n" +
	"\x19graph_input_channel_specs\x18\x03 \x03(\v2F.google.events.cloud.visionai.v1.ProcessorIOSpec.GraphInputChannelSpecR\x16graphInputChannelSpecs\x12\x84\x01\n" +
	"\x1agraph_output_channel_specs\x18\x04 \x03(\v2G.google.events.cloud.visionai.v1.ProcessorIOSpec.GraphOutputChannelSpecR\x17graphOutputChannelSpecs\x12\xa3\x01\n" +
	"%instance_resource_input_binding_specs\x18\x05 \x03(\v2Q.google.events.cloud.visionai.v1.ProcessorIOSpec.InstanceResourceInputBindingSpecR!instanceResourceInputBindingSpecs\x12\xa6\x01\n" +
	"&instance_resource_output_binding_specs\x18\x06 \x03(\v2R.google.events.cloud.visionai.v1.ProcessorIOSpec.InstanceResourceOutputBindingSpecR\"instanceResourceOutputBindingSpecs\x1a\xfc\x01\n" +
	"\x15GraphInputChannelSpec\x12\x12\n" +
	"\x04name\x18\x01 \x01(\tR\x04name\x12F\n" +
	"\tdata_type\x18\x02 \x01(\x0e2).google.events.cloud.visionai.v1.DataTypeR\bdataType\x125\n" +
	"\x17accepted_data_type_uris\x18\x05 \x03(\tR\x14acceptedDataTypeUris\x12\x1a\n" +
	"\brequired\x18\x03 \x01(\bR\brequired\x124\n" +
	"\x16max_connection_allowed\x18\x04 \x01(\x03R\x14maxConnectionAllowed\x1a\x98\x01\n" +
	"\x16GraphOutputChannelSpec\x12\x12\n" +
	"\x04name\x18\x01 \x01(\tR\x04name\x12F\n" +
	"\tdata_type\x18\x02 \x01(\x0e2).google.events.cloud.visionai.v1.DataTypeR\bdataType\x12\"\n" +
	"\rdata_type_uri\x18\x03 \x01(\tR\vdataTypeUri\x1a\x9f\x01\n" +
	" InstanceResourceInputBindingSpec\x12(\n" +
	"\x0fconfig_type_uri\x18\x02 \x01(\tH\x00R\rconfigTypeUri\x12,\n" +
	"\x11resource_type_uri\x18\x03 \x01(\tH\x00R\x0fresourceTypeUri\x12\x12\n" +
	"\x04name\x18\x01 \x01(\tR\x04nameB\x0f\n" +
	"\rresource_type\x1a\x7f\n" +
	"!InstanceResourceOutputBindingSpec\x12\x12\n" +
	"\x04name\x18\x01 \x01(\tR\x04name\x12*\n" +
	"\x11resource_type_uri\x18\x02 \x01(\tR\x0fresourceTypeUri\x12\x1a\n" +
	"\bexplicit\x18\x03 \x01(\bR\bexplicit\"\xe4\x06\n" +
	"\x19CustomProcessorSourceInfo\x12#\n" +
	"\fvertex_model\x18\x02 \x01(\tH\x00R\vvertexModel\x12f\n" +
	"\vsource_type\x18\x01 \x01(\x0e2E.google.events.cloud.visionai.v1.CustomProcessorSourceInfo.SourceTypeR\n" +
	"sourceType\x12w\n" +
	"\x0fadditional_info\x18\x04 \x03(\v2N.google.events.cloud.visionai.v1.CustomProcessorSourceInfo.AdditionalInfoEntryR\x0eadditionalInfo\x12i\n" +
	"\fmodel_schema\x18\x05 \x01(\v2F.google.events.cloud.visionai.v1.CustomProcessorSourceInfo.ModelSchemaR\vmodelSchema\x1a\x98\x02\n" +
	"\vModelSchema\x12U\n" +
	"\x10instances_schema\x18\x01 \x01(\v2*.google.events.cloud.visionai.v1.GcsSourceR\x0finstancesSchema\x12W\n" +
	"\x11parameters_schema\x18\x02 \x01(\v2*.google.events.cloud.visionai.v1.GcsSourceR\x10parametersSchema\x12Y\n" +
	"\x12predictions_schema\x18\x03 \x01(\v2*.google.events.cloud.visionai.v1.GcsSourceR\x11predictionsSchema\x1aA\n" +
	"\x13AdditionalInfoEntry\x12\x10\n" +
	"\x03key\x18\x01 \x01(\tR\x03key\x12\x14\n" +
	"\x05value\x18\x02 \x01(\tR\x05value:\x028\x01\"g\n" +
	"\n" +
	"SourceType\x12\x1b\n" +
	"\x17SOURCE_TYPE_UNSPECIFIED\x10\x00\x12\x11\n" +
	"\rVERTEX_AUTOML\x10\x01\x12\x11\n" +
	"\rVERTEX_CUSTOM\x10\x02\x12\x16\n" +
	"\x12PRODUCT_RECOGNIZER\x10\x03B\x0f\n" +
	"\rartifact_path\"\xe4\v\n" +
	"\x0fProcessorConfig\x12t\n" +
	"\x19video_stream_input_config\x18\t \x01(\v27.google.events.cloud.visionai.v1.VideoStreamInputConfigH\x00R\x16videoStreamInputConfig\x12\x84\x01\n" +
	"\x1fai_enabled_devices_input_config\x18\x14 \x01(\v2<.google.events.cloud.visionai.v1.AIEnabledDevicesInputConfigH\x00R\x1baiEnabledDevicesInputConfig\x12m\n" +
	"\x16media_warehouse_config\x18\n" +
	" \x01(\v25.google.events.cloud.visionai.v1.MediaWarehouseConfigH\x00R\x14mediaWarehouseConfig\x12a\n" +
	"\x12person_blur_config\x18\v \x01(\v21.google.events.cloud.visionai.v1.PersonBlurConfigH\x00R\x10personBlurConfig\x12m\n" +
	"\x16occupancy_count_config\x18\f \x01(\v25.google.events.cloud.visionai.v1.OccupancyCountConfigH\x00R\x14occupancyCountConfig\x12\x86\x01\n" +
	"\x1fperson_vehicle_detection_config\x18\x0f \x01(\v2=.google.events.cloud.visionai.v1.PersonVehicleDetectionConfigH\x00R\x1cpersonVehicleDetectionConfig\x12z\n" +
	"\x1bvertex_automl_vision_config\x18\r \x01(\v29.google.events.cloud.visionai.v1.VertexAutoMLVisionConfigH\x00R\x18vertexAutomlVisionConfig\x12w\n" +
	"\x1avertex_automl_video_config\x18\x0e \x01(\v28.google.events.cloud.visionai.v1.VertexAutoMLVideoConfigH\x00R\x17vertexAutomlVideoConfig\x12g\n" +
	"\x14vertex_custom_config\x18\x11 \x01(\v23.google.events.cloud.visionai.v1.VertexCustomConfigH\x00R\x12vertexCustomConfig\x12\x86\x01\n" +
	"\x1fgeneral_object_detection_config\x18\x12 \x01(\v2=.google.events.cloud.visionai.v1.GeneralObjectDetectionConfigH\x00R\x1cgeneralObjectDetectionConfig\x12[\n" +
	"\x10big_query_config\x18\x13 \x01(\v2/.google.events.cloud.visionai.v1.BigQueryConfigH\x00R\x0ebigQueryConfig\x12\xb1\x01\n" +
	".personal_protective_equipment_detection_config\x18\x16 \x01(\v2K.google.events.cloud.visionai.v1.PersonalProtectiveEquipmentDetectionConfigH\x00R*personalProtectiveEquipmentDetectionConfigB\x12\n" +
	"\x10processor_config\"\x86\x03\n" +
	"\x14StreamWithAnnotation\x12\x16\n" +
	"\x06stream\x18\x01 \x01(\tR\x06stream\x12j\n" +
	"\x17application_annotations\x18\x02 \x03(\v21.google.events.cloud.visionai.v1.StreamAnnotationR\x16applicationAnnotations\x12o\n" +
	"\x10node_annotations\x18\x03 \x03(\v2D.google.events.cloud.visionai.v1.StreamWithAnnotation.NodeAnnotationR\x0fnodeAnnotations\x1ay\n" +
	"\x0eNodeAnnotation\x12\x12\n" +
	"\x04node\x18\x01 \x01(\tR\x04node\x12S\n" +
	"\vannotations\x18\x02 \x03(\v21.google.events.cloud.visionai.v1.StreamAnnotationR\vannotations\"\xa1\x01\n" +
	"\x16VideoStreamInputConfig\x12\x18\n" +
	"\astreams\x18\x01 \x03(\tR\astreams\x12m\n" +
	"\x17streams_with_annotation\x18\x02 \x03(\v25.google.events.cloud.visionai.v1.StreamWithAnnotationR\x15streamsWithAnnotation\"\x1d\n" +
	"\x1bAIEnabledDevicesInputConfig\"s\n" +
	"\x14MediaWarehouseConfig\x12\x16\n" +
	"\x06corpus\x18\x01 \x01(\tR\x06corpus\x12\x16\n" +
	"\x06region\x18\x02 \x01(\tR\x06region\x12+\n" +
	"\x03ttl\x18\x03 \x01(\v2\x19.google.protobuf.DurationR\x03ttl\"\xf7\x01\n" +
	"\x10PersonBlurConfig\x12j\n" +
	"\x10person_blur_type\x18\x01 \x01(\x0e2@.google.events.cloud.visionai.v1.PersonBlurConfig.PersonBlurTypeR\x0epersonBlurType\x12\x1d\n" +
	"\n" +
	"faces_only\x18\x02 \x01(\bR\tfacesOnly\"X\n" +
	"\x0ePersonBlurType\x12 \n" +
	"\x1cPERSON_BLUR_TYPE_UNSPECIFIED\x10\x00\x12\x13\n" +
	"\x0fFULL_OCCULUSION\x10\x01\x12\x0f\n" +
	"\vBLUR_FILTER\x10\x02\"\xc7\x01\n" +
	"\x14OccupancyCountConfig\x124\n" +
	"\x16enable_people_counting\x18\x01 \x01(\bR\x14enablePeopleCounting\x126\n" +
	"\x17enable_vehicle_counting\x18\x02 \x01(\bR\x15enableVehicleCounting\x12A\n" +
	"\x1denable_dwelling_time_tracking\x18\x03 \x01(\bR\x1aenableDwellingTimeTracking\"\x8c\x01\n" +
	"\x1cPersonVehicleDetectionConfig\x124\n" +
	"\x16enable_people_counting\x18\x01 \x01(\bR\x14enablePeopleCounting\x126\n" +
	"\x17enable_vehicle_counting\x18\x02 \x01(\bR\x15enableVehicleCounting\"\xfd\x01\n" +
	"*PersonalProtectiveEquipmentDetectionConfig\x12C\n" +
	"\x1eenable_face_coverage_detection\x18\x01 \x01(\bR\x1benableFaceCoverageDetection\x12C\n" +
	"\x1eenable_head_coverage_detection\x18\x02 \x01(\bR\x1benableHeadCoverageDetection\x12E\n" +
	"\x1fenable_hands_coverage_detection\x18\x03 \x01(\bR\x1cenableHandsCoverageDetection\"\x1e\n" +
	"\x1cGeneralObjectDetectionConfig\"\xbb\x02\n" +
	"\x0eBigQueryConfig\x12\x14\n" +
	"\x05table\x18\x01 \x01(\tR\x05table\x12\x7f\n" +
	"\x16cloud_function_mapping\x18\x02 \x03(\v2I.google.events.cloud.visionai.v1.BigQueryConfig.CloudFunctionMappingEntryR\x14cloudFunctionMapping\x12I\n" +
	"\"create_default_table_if_not_exists\x18\x03 \x01(\bR\x1dcreateDefaultTableIfNotExists\x1aG\n" +
	"\x19CloudFunctionMappingEntry\x12\x10\n" +
	"\x03key\x18\x01 \x01(\tR\x03key\x12\x14\n" +
	"\x05value\x18\x02 \x01(\tR\x05value:\x028\x01\"v\n" +
	"\x18VertexAutoMLVisionConfig\x121\n" +
	"\x14confidence_threshold\x18\x01 \x01(\x02R\x13confidenceThreshold\x12'\n" +
	"\x0fmax_predictions\x18\x02 \x01(\x05R\x0emaxPredictions\"\xd3\x01\n" +
	"\x17VertexAutoMLVideoConfig\x121\n" +
	"\x14confidence_threshold\x18\x01 \x01(\x02R\x13confidenceThreshold\x12%\n" +
	"\x0eblocked_labels\x18\x02 \x03(\tR\rblockedLabels\x12'\n" +
	"\x0fmax_predictions\x18\x03 \x01(\x05R\x0emaxPredictions\x125\n" +
	"\x17bounding_box_size_limit\x18\x04 \x01(\x02R\x14boundingBoxSizeLimit\"\xad\x02\n" +
	"\x12VertexCustomConfig\x12,\n" +
	"\x12max_prediction_fps\x18\x01 \x01(\x05R\x10maxPredictionFps\x12d\n" +
	"\x13dedicated_resources\x18\x02 \x01(\v23.google.events.cloud.visionai.v1.DedicatedResourcesR\x12dedicatedResources\x12C\n" +
	"\x1epost_processing_cloud_function\x18\x03 \x01(\tR\x1bpostProcessingCloudFunction\x12>\n" +
	"\x1battach_application_metadata\x18\x04 \x01(\bR\x19attachApplicationMetadata\"\xba\x01\n" +
	"\vMachineSpec\x12!\n" +
	"\fmachine_type\x18\x01 \x01(\tR\vmachineType\x12[\n" +
	"\x10accelerator_type\x18\x02 \x01(\x0e20.google.events.cloud.visionai.v1.AcceleratorTypeR\x0facceleratorType\x12+\n" +
	"\x11accelerator_count\x18\x03 \x01(\x05R\x10acceleratorCount\"P\n" +
	"\x15AutoscalingMetricSpec\x12\x1f\n" +
	"\vmetric_name\x18\x01 \x01(\tR\n" +
	"metricName\x12\x16\n" +
	"\x06target\x18\x02 \x01(\x05R\x06target\"\xaf\x02\n" +
	"\x12DedicatedResources\x12O\n" +
	"\fmachine_spec\x18\x01 \x01(\v2,.google.events.cloud.visionai.v1.MachineSpecR\vmachineSpec\x12*\n" +
	"\x11min_replica_count\x18\x02 \x01(\x05R\x0fminReplicaCount\x12*\n" +
	"\x11max_replica_count\x18\x03 \x01(\x05R\x0fmaxReplicaCount\x12p\n" +
	"\x18autoscaling_metric_specs\x18\x04 \x03(\v26.google.events.cloud.visionai.v1.AutoscalingMetricSpecR\x16autoscalingMetricSpecs\"\xc1\x04\n" +
	"\x06Stream\x12\x12\n" +
	"\x04name\x18\x01 \x01(\tR\x04name\x12;\n" +
	"\vcreate_time\x18\x02 \x01(\v2\x1a.google.protobuf.TimestampR\n" +
	"createTime\x12;\n" +
	"\vupdate_time\x18\x03 \x01(\v2\x1a.google.protobuf.TimestampR\n" +
	"updateTime\x12K\n" +
	"\x06labels\x18\x04 \x03(\v23.google.events.cloud.visionai.v1.Stream.LabelsEntryR\x06labels\x12Z\n" +
	"\vannotations\x18\x05 \x03(\v28.google.events.cloud.visionai.v1.Stream.AnnotationsEntryR\vannotations\x12!\n" +
	"\fdisplay_name\x18\x06 \x01(\tR\vdisplayName\x12.\n" +
	"\x13enable_hls_playback\x18\a \x01(\bR\x11enableHlsPlayback\x122\n" +
	"\x15media_warehouse_asset\x18\b \x01(\tR\x13mediaWarehouseAsset\x1a9\n" +
	"\vLabelsEntry\x12\x10\n" +
	"\x03key\x18\x01 \x01(\tR\x03key\x12\x14\n" +
	"\x05value\x18\x02 \x01(\tR\x05value:\x028\x01\x1a>\n" +
	"\x10AnnotationsEntry\x12\x10\n" +
	"\x03key\x18\x01 \x01(\tR\x03key\x12\x14\n" +
	"\x05value\x18\x02 \x01(\tR\x05value:\x028\x01\"\x85\x05\n" +
	"\x05Event\x12\x12\n" +
	"\x04name\x18\x01 \x01(\tR\x04name\x12;\n" +
	"\vcreate_time\x18\x02 \x01(\v2\x1a.google.protobuf.TimestampR\n" +
	"createTime\x12;\n" +
	"\vupdate_time\x18\x03 \x01(\v2\x1a.google.protobuf.TimestampR\n" +
	"updateTime\x12J\n" +
	"\x06labels\x18\x04 \x03(\v22.google.events.cloud.visionai.v1.Event.LabelsEntryR\x06labels\x12Y\n" +
	"\vannotations\x18\x05 \x03(\v27.google.events.cloud.visionai.v1.Event.AnnotationsEntryR\vannotations\x12U\n" +
	"\x0falignment_clock\x18\x06 \x01(\x0e2,.google.events.cloud.visionai.v1.Event.ClockR\x0ealignmentClock\x12<\n" +
	"\fgrace_period\x18\a \x01(\v2\x19.google.protobuf.DurationR\vgracePeriod\x1a9\n" +
	"\vLabelsEntry\x12\x10\n" +
	"\x03key\x18\x01 \x01(\tR\x03key\x12\x14\n" +
	"\x05value\x18\x02 \x01(\tR\x05value:\x028\x01\x1a>\n" +
	"\x10AnnotationsEntry\x12\x10\n" +
	"\x03key\x18\x01 \x01(\tR\x03key\x12\x14\n" +
	"\x05value\x18\x02 \x01(\tR\x05value:\x028\x01\"7\n" +
	"\x05Clock\x12\x15\n" +
	"\x11CLOCK_UNSPECIFIED\x10\x00\x12\v\n" +
	"\aCAPTURE\x10\x01\x12\n" +
	"\n" +
	"\x06INGEST\x10\x02\"\xe8\x03\n" +
	"\x06Series\x12\x12\n" +
	"\x04name\x18\x01 \x01(\tR\x04name\x12;\n" +
	"\vcreate_time\x18\x02 \x01(\v2\x1a.google.protobuf.TimestampR\n" +
	"createTime\x12;\n" +
	"\vupdate_time\x18\x03 \x01(\v2\x1a.google.protobuf.TimestampR\n" +
	"updateTime\x12K\n" +
	"\x06labels\x18\x04 \x03(\v23.google.events.cloud.visionai.v1.Series.LabelsEntryR\x06labels\x12Z\n" +
	"\vannotations\x18\x05 \x03(\v28.google.events.cloud.visionai.v1.Series.AnnotationsEntryR\vannotations\x12\x16\n" +
	"\x06stream\x18\x06 \x01(\tR\x06stream\x12\x14\n" +
	"\x05event\x18\a \x01(\tR\x05event\x1a9\n" +
	"\vLabelsEntry\x12\x10\n" +
	"\x03key\x18\x01 \x01(\tR\x03key\x12\x14\n" +
	"\x05value\x18\x02 \x01(\tR\x05value:\x028\x01\x1a>\n" +
	"\x10AnnotationsEntry\x12\x10\n" +
	"\x03key\x18\x01 \x01(\tR\x03key\x12\x14\n" +
	"\x05value\x18\x02 \x01(\tR\x05value:\x028\x01\"e\n" +
	"\x0fSeriesEventData\x12F\n" +
	"\apayload\x18\x01 \x01(\v2'.google.events.cloud.visionai.v1.SeriesH\x00R\apayload\x88\x01\x01B\n" +
	"\n" +
	"\b_payload\"c\n" +
	"\x0eDraftEventData\x12E\n" +
	"\apayload\x18\x01 \x01(\v2&.google.events.cloud.visionai.v1.DraftH\x00R\apayload\x88\x01\x01B\n" +
	"\n" +
	"\b_payload\"k\n" +
	"\x12ProcessorEventData\x12I\n" +
	"\apayload\x18\x01 \x01(\v2*.google.events.cloud.visionai.v1.ProcessorH\x00R\apayload\x88\x01\x01B\n" +
	"\n" +
	"\b_payload\"i\n" +
	"\x11AnalysisEventData\x12H\n" +
	"\apayload\x18\x01 \x01(\v2).google.events.cloud.visionai.v1.AnalysisH\x00R\apayload\x88\x01\x01B\n" +
	"\n" +
	"\b_payload\"g\n" +
	"\x10ClusterEventData\x12G\n" +
	"\apayload\x18\x01 \x01(\v2(.google.events.cloud.visionai.v1.ClusterH\x00R\apayload\x88\x01\x01B\n" +
	"\n" +
	"\b_payload\"c\n" +
	"\x0eEventEventData\x12E\n" +
	"\apayload\x18\x01 \x01(\v2&.google.events.cloud.visionai.v1.EventH\x00R\apayload\x88\x01\x01B\n" +
	"\n" +
	"\b_payload\"g\n" +
	"\x10ProcessEventData\x12G\n" +
	"\apayload\x18\x01 \x01(\v2(.google.events.cloud.visionai.v1.ProcessH\x00R\apayload\x88\x01\x01B\n" +
	"\n" +
	"\b_payload\"e\n" +
	"\x0fStreamEventData\x12F\n" +
	"\apayload\x18\x01 \x01(\v2'.google.events.cloud.visionai.v1.StreamH\x00R\apayload\x88\x01\x01B\n" +
	"\n" +
	"\b_payload\"o\n" +
	"\x14ApplicationEventData\x12K\n" +
	"\apayload\x18\x01 \x01(\v2,.google.events.cloud.visionai.v1.ApplicationH\x00R\apayload\x88\x01\x01B\n" +
	"\n" +
	"\b_payload*\x90\x01\n" +
	"\x14StreamAnnotationType\x12&\n" +
	"\"STREAM_ANNOTATION_TYPE_UNSPECIFIED\x10\x00\x12&\n" +
	"\"STREAM_ANNOTATION_TYPE_ACTIVE_ZONE\x10\x01\x12(\n" +
	"$STREAM_ANNOTATION_TYPE_CROSSING_LINE\x10\x02*=\n" +
	"\aRunMode\x12\x18\n" +
	"\x14RUN_MODE_UNSPECIFIED\x10\x00\x12\b\n" +
	"\x04LIVE\x10\x01\x12\x0e\n" +
	"\n" +
	"SUBMISSION\x10\x02*\xe6\x01\n" +
	"\tModelType\x12\x1a\n" +
	"\x16MODEL_TYPE_UNSPECIFIED\x10\x00\x12\x18\n" +
	"\x14IMAGE_CLASSIFICATION\x10\x01\x12\x14\n" +
	"\x10OBJECT_DETECTION\x10\x02\x12\x18\n" +
	"\x14VIDEO_CLASSIFICATION\x10\x03\x12\x19\n" +
	"\x15VIDEO_OBJECT_TRACKING\x10\x04\x12\x1c\n" +
	"\x18VIDEO_ACTION_RECOGNITION\x10\x05\x12\x16\n" +
	"\x12OCCUPANCY_COUNTING\x10\x06\x12\x0f\n" +
	"\vPERSON_BLUR\x10\a\x12\x11\n" +
	"\rVERTEX_CUSTOM\x10\b*\xd0\x01\n" +
	"\x0fAcceleratorType\x12 \n" +
	"\x1cACCELERATOR_TYPE_UNSPECIFIED\x10\x00\x12\x14\n" +
	"\x10NVIDIA_TESLA_K80\x10\x01\x12\x15\n" +
	"\x11NVIDIA_TESLA_P100\x10\x02\x12\x15\n" +
	"\x11NVIDIA_TESLA_V100\x10\x03\x12\x13\n" +
	"\x0fNVIDIA_TESLA_P4\x10\x04\x12\x13\n" +
	"\x0fNVIDIA_TESLA_T4\x10\x05\x12\x15\n" +
	"\x11NVIDIA_TESLA_A100\x10\b\x12\n" +
	"\n" +
	"\x06TPU_V2\x10\x06\x12\n" +
	"\n" +
	"\x06TPU_V3\x10\a*F\n" +
	"\bDataType\x12\x19\n" +
	"\x15DATA_TYPE_UNSPECIFIED\x10\x00\x12\t\n" +
	"\x05VIDEO\x10\x01\x12\t\n" +
	"\x05IMAGE\x10\x03\x12\t\n" +
	"\x05PROTO\x10\x02Bs\xaa\x02(Google.Events.Protobuf.Cloud.VisionAI.V1\xca\x02\x1fGoogle\\Events\\Cloud\\VisionAI\\V1\xea\x02#Google::Events::Cloud::VisionAI::V1b\x06proto3"

var (
	file_cloud_visionai_v1_data_proto_rawDescOnce sync.Once
	file_cloud_visionai_v1_data_proto_rawDescData []byte
)

func file_cloud_visionai_v1_data_proto_rawDescGZIP() []byte {
	file_cloud_visionai_v1_data_proto_rawDescOnce.Do(func() {
		file_cloud_visionai_v1_data_proto_rawDescData = protoimpl.X.CompressGZIP(unsafe.Slice(unsafe.StringData(file_cloud_visionai_v1_data_proto_rawDesc), len(file_cloud_visionai_v1_data_proto_rawDesc)))
	})
	return file_cloud_visionai_v1_data_proto_rawDescData
}

var file_cloud_visionai_v1_data_proto_enumTypes = make([]protoimpl.EnumInfo, 14)
var file_cloud_visionai_v1_data_proto_msgTypes = make([]protoimpl.MessageInfo, 78)
var file_cloud_visionai_v1_data_proto_goTypes = []any{
	(StreamAnnotationType)(0),                          // 0: google.events.cloud.visionai.v1.StreamAnnotationType
	(RunMode)(0),                                       // 1: google.events.cloud.visionai.v1.RunMode
	(ModelType)(0),                                     // 2: google.events.cloud.visionai.v1.ModelType
	(AcceleratorType)(0),                               // 3: google.events.cloud.visionai.v1.AcceleratorType
	(DataType)(0),                                      // 4: google.events.cloud.visionai.v1.DataType
	(Cluster_State)(0),                                 // 5: google.events.cloud.visionai.v1.Cluster.State
	(RunStatus_State)(0),                               // 6: google.events.cloud.visionai.v1.RunStatus.State
	(Application_State)(0),                             // 7: google.events.cloud.visionai.v1.Application.State
	(Application_BillingMode)(0),                       // 8: google.events.cloud.visionai.v1.Application.BillingMode
	(Processor_ProcessorType)(0),                       // 9: google.events.cloud.visionai.v1.Processor.ProcessorType
	(Processor_ProcessorState)(0),                      // 10: google.events.cloud.visionai.v1.Processor.ProcessorState
	(CustomProcessorSourceInfo_SourceType)(0),          // 11: google.events.cloud.visionai.v1.CustomProcessorSourceInfo.SourceType
	(PersonBlurConfig_PersonBlurType)(0),               // 12: google.events.cloud.visionai.v1.PersonBlurConfig.PersonBlurType
	(Event_Clock)(0),                                   // 13: google.events.cloud.visionai.v1.Event.Clock
	(*StreamAnnotation)(nil),                           // 14: google.events.cloud.visionai.v1.StreamAnnotation
	(*NormalizedPolygon)(nil),                          // 15: google.events.cloud.visionai.v1.NormalizedPolygon
	(*NormalizedPolyline)(nil),                         // 16: google.events.cloud.visionai.v1.NormalizedPolyline
	(*NormalizedVertex)(nil),                           // 17: google.events.cloud.visionai.v1.NormalizedVertex
	(*Cluster)(nil),                                    // 18: google.events.cloud.visionai.v1.Cluster
	(*GcsSource)(nil),                                  // 19: google.events.cloud.visionai.v1.GcsSource
	(*AttributeValue)(nil),                             // 20: google.events.cloud.visionai.v1.AttributeValue
	(*AnalyzerDefinition)(nil),                         // 21: google.events.cloud.visionai.v1.AnalyzerDefinition
	(*AnalysisDefinition)(nil),                         // 22: google.events.cloud.visionai.v1.AnalysisDefinition
	(*RunStatus)(nil),                                  // 23: google.events.cloud.visionai.v1.RunStatus
	(*Analysis)(nil),                                   // 24: google.events.cloud.visionai.v1.Analysis
	(*Process)(nil),                                    // 25: google.events.cloud.visionai.v1.Process
	(*Application)(nil),                                // 26: google.events.cloud.visionai.v1.Application
	(*ApplicationConfigs)(nil),                         // 27: google.events.cloud.visionai.v1.ApplicationConfigs
	(*Node)(nil),                                       // 28: google.events.cloud.visionai.v1.Node
	(*Draft)(nil),                                      // 29: google.events.cloud.visionai.v1.Draft
	(*Processor)(nil),                                  // 30: google.events.cloud.visionai.v1.Processor
	(*ProcessorIOSpec)(nil),                            // 31: google.events.cloud.visionai.v1.ProcessorIOSpec
	(*CustomProcessorSourceInfo)(nil),                  // 32: google.events.cloud.visionai.v1.CustomProcessorSourceInfo
	(*ProcessorConfig)(nil),                            // 33: google.events.cloud.visionai.v1.ProcessorConfig
	(*StreamWithAnnotation)(nil),                       // 34: google.events.cloud.visionai.v1.StreamWithAnnotation
	(*VideoStreamInputConfig)(nil),                     // 35: google.events.cloud.visionai.v1.VideoStreamInputConfig
	(*AIEnabledDevicesInputConfig)(nil),                // 36: google.events.cloud.visionai.v1.AIEnabledDevicesInputConfig
	(*MediaWarehouseConfig)(nil),                       // 37: google.events.cloud.visionai.v1.MediaWarehouseConfig
	(*PersonBlurConfig)(nil),                           // 38: google.events.cloud.visionai.v1.PersonBlurConfig
	(*OccupancyCountConfig)(nil),                       // 39: google.events.cloud.visionai.v1.OccupancyCountConfig
	(*PersonVehicleDetectionConfig)(nil),               // 40: google.events.cloud.visionai.v1.PersonVehicleDetectionConfig
	(*PersonalProtectiveEquipmentDetectionConfig)(nil), // 41: google.events.cloud.visionai.v1.PersonalProtectiveEquipmentDetectionConfig
	(*GeneralObjectDetectionConfig)(nil),               // 42: google.events.cloud.visionai.v1.GeneralObjectDetectionConfig
	(*BigQueryConfig)(nil),                             // 43: google.events.cloud.visionai.v1.BigQueryConfig
	(*VertexAutoMLVisionConfig)(nil),                   // 44: google.events.cloud.visionai.v1.VertexAutoMLVisionConfig
	(*VertexAutoMLVideoConfig)(nil),                    // 45: google.events.cloud.visionai.v1.VertexAutoMLVideoConfig
	(*VertexCustomConfig)(nil),                         // 46: google.events.cloud.visionai.v1.VertexCustomConfig
	(*MachineSpec)(nil),                                // 47: google.events.cloud.visionai.v1.MachineSpec
	(*AutoscalingMetricSpec)(nil),                      // 48: google.events.cloud.visionai.v1.AutoscalingMetricSpec
	(*DedicatedResources)(nil),                         // 49: google.events.cloud.visionai.v1.DedicatedResources
	(*Stream)(nil),                                     // 50: google.events.cloud.visionai.v1.Stream
	(*Event)(nil),                                      // 51: google.events.cloud.visionai.v1.Event
	(*Series)(nil),                                     // 52: google.events.cloud.visionai.v1.Series
	(*SeriesEventData)(nil),                            // 53: google.events.cloud.visionai.v1.SeriesEventData
	(*DraftEventData)(nil),                             // 54: google.events.cloud.visionai.v1.DraftEventData
	(*ProcessorEventData)(nil),                         // 55: google.events.cloud.visionai.v1.ProcessorEventData
	(*AnalysisEventData)(nil),                          // 56: google.events.cloud.visionai.v1.AnalysisEventData
	(*ClusterEventData)(nil),                           // 57: google.events.cloud.visionai.v1.ClusterEventData
	(*EventEventData)(nil),                             // 58: google.events.cloud.visionai.v1.EventEventData
	(*ProcessEventData)(nil),                           // 59: google.events.cloud.visionai.v1.ProcessEventData
	(*StreamEventData)(nil),                            // 60: google.events.cloud.visionai.v1.StreamEventData
	(*ApplicationEventData)(nil),                       // 61: google.events.cloud.visionai.v1.ApplicationEventData
	nil,                                                // 62: google.events.cloud.visionai.v1.Cluster.LabelsEntry
	nil,                                                // 63: google.events.cloud.visionai.v1.Cluster.AnnotationsEntry
	(*AnalyzerDefinition_StreamInput)(nil),             // 64: google.events.cloud.visionai.v1.AnalyzerDefinition.StreamInput
	(*AnalyzerDefinition_DebugOptions)(nil),            // 65: google.events.cloud.visionai.v1.AnalyzerDefinition.DebugOptions
	nil,                                                // 66: google.events.cloud.visionai.v1.AnalyzerDefinition.AttrsEntry
	nil,                                                // 67: google.events.cloud.visionai.v1.AnalyzerDefinition.DebugOptions.EnvironmentVariablesEntry
	nil,                                                // 68: google.events.cloud.visionai.v1.Analysis.LabelsEntry
	nil,                                                // 69: google.events.cloud.visionai.v1.Analysis.InputStreamsMappingEntry
	nil,                                                // 70: google.events.cloud.visionai.v1.Analysis.OutputStreamsMappingEntry
	(*Application_ApplicationRuntimeInfo)(nil),         // 71: google.events.cloud.visionai.v1.Application.ApplicationRuntimeInfo
	nil, // 72: google.events.cloud.visionai.v1.Application.LabelsEntry
	(*Application_ApplicationRuntimeInfo_GlobalOutputResource)(nil), // 73: google.events.cloud.visionai.v1.Application.ApplicationRuntimeInfo.GlobalOutputResource
	(*Application_ApplicationRuntimeInfo_MonitoringConfig)(nil),     // 74: google.events.cloud.visionai.v1.Application.ApplicationRuntimeInfo.MonitoringConfig
	(*Node_InputEdge)(nil), // 75: google.events.cloud.visionai.v1.Node.InputEdge
	nil,                    // 76: google.events.cloud.visionai.v1.Draft.LabelsEntry
	nil,                    // 77: google.events.cloud.visionai.v1.Processor.LabelsEntry
	(*ProcessorIOSpec_GraphInputChannelSpec)(nil),             // 78: google.events.cloud.visionai.v1.ProcessorIOSpec.GraphInputChannelSpec
	(*ProcessorIOSpec_GraphOutputChannelSpec)(nil),            // 79: google.events.cloud.visionai.v1.ProcessorIOSpec.GraphOutputChannelSpec
	(*ProcessorIOSpec_InstanceResourceInputBindingSpec)(nil),  // 80: google.events.cloud.visionai.v1.ProcessorIOSpec.InstanceResourceInputBindingSpec
	(*ProcessorIOSpec_InstanceResourceOutputBindingSpec)(nil), // 81: google.events.cloud.visionai.v1.ProcessorIOSpec.InstanceResourceOutputBindingSpec
	(*CustomProcessorSourceInfo_ModelSchema)(nil),             // 82: google.events.cloud.visionai.v1.CustomProcessorSourceInfo.ModelSchema
	nil, // 83: google.events.cloud.visionai.v1.CustomProcessorSourceInfo.AdditionalInfoEntry
	(*StreamWithAnnotation_NodeAnnotation)(nil), // 84: google.events.cloud.visionai.v1.StreamWithAnnotation.NodeAnnotation
	nil,                           // 85: google.events.cloud.visionai.v1.BigQueryConfig.CloudFunctionMappingEntry
	nil,                           // 86: google.events.cloud.visionai.v1.Stream.LabelsEntry
	nil,                           // 87: google.events.cloud.visionai.v1.Stream.AnnotationsEntry
	nil,                           // 88: google.events.cloud.visionai.v1.Event.LabelsEntry
	nil,                           // 89: google.events.cloud.visionai.v1.Event.AnnotationsEntry
	nil,                           // 90: google.events.cloud.visionai.v1.Series.LabelsEntry
	nil,                           // 91: google.events.cloud.visionai.v1.Series.AnnotationsEntry
	(*timestamppb.Timestamp)(nil), // 92: google.protobuf.Timestamp
	(*durationpb.Duration)(nil),   // 93: google.protobuf.Duration
}
var file_cloud_visionai_v1_data_proto_depIdxs = []int32{
	15,  // 0: google.events.cloud.visionai.v1.StreamAnnotation.active_zone:type_name -> google.events.cloud.visionai.v1.NormalizedPolygon
	16,  // 1: google.events.cloud.visionai.v1.StreamAnnotation.crossing_line:type_name -> google.events.cloud.visionai.v1.NormalizedPolyline
	0,   // 2: google.events.cloud.visionai.v1.StreamAnnotation.type:type_name -> google.events.cloud.visionai.v1.StreamAnnotationType
	17,  // 3: google.events.cloud.visionai.v1.NormalizedPolygon.normalized_vertices:type_name -> google.events.cloud.visionai.v1.NormalizedVertex
	17,  // 4: google.events.cloud.visionai.v1.NormalizedPolyline.normalized_vertices:type_name -> google.events.cloud.visionai.v1.NormalizedVertex
	92,  // 5: google.events.cloud.visionai.v1.Cluster.create_time:type_name -> google.protobuf.Timestamp
	92,  // 6: google.events.cloud.visionai.v1.Cluster.update_time:type_name -> google.protobuf.Timestamp
	62,  // 7: google.events.cloud.visionai.v1.Cluster.labels:type_name -> google.events.cloud.visionai.v1.Cluster.LabelsEntry
	63,  // 8: google.events.cloud.visionai.v1.Cluster.annotations:type_name -> google.events.cloud.visionai.v1.Cluster.AnnotationsEntry
	5,   // 9: google.events.cloud.visionai.v1.Cluster.state:type_name -> google.events.cloud.visionai.v1.Cluster.State
	64,  // 10: google.events.cloud.visionai.v1.AnalyzerDefinition.inputs:type_name -> google.events.cloud.visionai.v1.AnalyzerDefinition.StreamInput
	66,  // 11: google.events.cloud.visionai.v1.AnalyzerDefinition.attrs:type_name -> google.events.cloud.visionai.v1.AnalyzerDefinition.AttrsEntry
	65,  // 12: google.events.cloud.visionai.v1.AnalyzerDefinition.debug_options:type_name -> google.events.cloud.visionai.v1.AnalyzerDefinition.DebugOptions
	21,  // 13: google.events.cloud.visionai.v1.AnalysisDefinition.analyzers:type_name -> google.events.cloud.visionai.v1.AnalyzerDefinition
	6,   // 14: google.events.cloud.visionai.v1.RunStatus.state:type_name -> google.events.cloud.visionai.v1.RunStatus.State
	92,  // 15: google.events.cloud.visionai.v1.Analysis.create_time:type_name -> google.protobuf.Timestamp
	92,  // 16: google.events.cloud.visionai.v1.Analysis.update_time:type_name -> google.protobuf.Timestamp
	68,  // 17: google.events.cloud.visionai.v1.Analysis.labels:type_name -> google.events.cloud.visionai.v1.Analysis.LabelsEntry
	22,  // 18: google.events.cloud.visionai.v1.Analysis.analysis_definition:type_name -> google.events.cloud.visionai.v1.AnalysisDefinition
	69,  // 19: google.events.cloud.visionai.v1.Analysis.input_streams_mapping:type_name -> google.events.cloud.visionai.v1.Analysis.InputStreamsMappingEntry
	70,  // 20: google.events.cloud.visionai.v1.Analysis.output_streams_mapping:type_name -> google.events.cloud.visionai.v1.Analysis.OutputStreamsMappingEntry
	92,  // 21: google.events.cloud.visionai.v1.Process.create_time:type_name -> google.protobuf.Timestamp
	92,  // 22: google.events.cloud.visionai.v1.Process.update_time:type_name -> google.protobuf.Timestamp
	23,  // 23: google.events.cloud.visionai.v1.Process.run_status:type_name -> google.events.cloud.visionai.v1.RunStatus
	1,   // 24: google.events.cloud.visionai.v1.Process.run_mode:type_name -> google.events.cloud.visionai.v1.RunMode
	92,  // 25: google.events.cloud.visionai.v1.Application.create_time:type_name -> google.protobuf.Timestamp
	92,  // 26: google.events.cloud.visionai.v1.Application.update_time:type_name -> google.protobuf.Timestamp
	72,  // 27: google.events.cloud.visionai.v1.Application.labels:type_name -> google.events.cloud.visionai.v1.Application.LabelsEntry
	27,  // 28: google.events.cloud.visionai.v1.Application.application_configs:type_name -> google.events.cloud.visionai.v1.ApplicationConfigs
	71,  // 29: google.events.cloud.visionai.v1.Application.runtime_info:type_name -> google.events.cloud.visionai.v1.Application.ApplicationRuntimeInfo
	7,   // 30: google.events.cloud.visionai.v1.Application.state:type_name -> google.events.cloud.visionai.v1.Application.State
	8,   // 31: google.events.cloud.visionai.v1.Application.billing_mode:type_name -> google.events.cloud.visionai.v1.Application.BillingMode
	28,  // 32: google.events.cloud.visionai.v1.ApplicationConfigs.nodes:type_name -> google.events.cloud.visionai.v1.Node
	33,  // 33: google.events.cloud.visionai.v1.Node.node_config:type_name -> google.events.cloud.visionai.v1.ProcessorConfig
	75,  // 34: google.events.cloud.visionai.v1.Node.parents:type_name -> google.events.cloud.visionai.v1.Node.InputEdge
	92,  // 35: google.events.cloud.visionai.v1.Draft.create_time:type_name -> google.protobuf.Timestamp
	92,  // 36: google.events.cloud.visionai.v1.Draft.update_time:type_name -> google.protobuf.Timestamp
	76,  // 37: google.events.cloud.visionai.v1.Draft.labels:type_name -> google.events.cloud.visionai.v1.Draft.LabelsEntry
	27,  // 38: google.events.cloud.visionai.v1.Draft.draft_application_configs:type_name -> google.events.cloud.visionai.v1.ApplicationConfigs
	92,  // 39: google.events.cloud.visionai.v1.Processor.create_time:type_name -> google.protobuf.Timestamp
	92,  // 40: google.events.cloud.visionai.v1.Processor.update_time:type_name -> google.protobuf.Timestamp
	77,  // 41: google.events.cloud.visionai.v1.Processor.labels:type_name -> google.events.cloud.visionai.v1.Processor.LabelsEntry
	9,   // 42: google.events.cloud.visionai.v1.Processor.processor_type:type_name -> google.events.cloud.visionai.v1.Processor.ProcessorType
	2,   // 43: google.events.cloud.visionai.v1.Processor.model_type:type_name -> google.events.cloud.visionai.v1.ModelType
	32,  // 44: google.events.cloud.visionai.v1.Processor.custom_processor_source_info:type_name -> google.events.cloud.visionai.v1.CustomProcessorSourceInfo
	10,  // 45: google.events.cloud.visionai.v1.Processor.state:type_name -> google.events.cloud.visionai.v1.Processor.ProcessorState
	31,  // 46: google.events.cloud.visionai.v1.Processor.processor_io_spec:type_name -> google.events.cloud.visionai.v1.ProcessorIOSpec
	0,   // 47: google.events.cloud.visionai.v1.Processor.supported_annotation_types:type_name -> google.events.cloud.visionai.v1.StreamAnnotationType
	78,  // 48: google.events.cloud.visionai.v1.ProcessorIOSpec.graph_input_channel_specs:type_name -> google.events.cloud.visionai.v1.ProcessorIOSpec.GraphInputChannelSpec
	79,  // 49: google.events.cloud.visionai.v1.ProcessorIOSpec.graph_output_channel_specs:type_name -> google.events.cloud.visionai.v1.ProcessorIOSpec.GraphOutputChannelSpec
	80,  // 50: google.events.cloud.visionai.v1.ProcessorIOSpec.instance_resource_input_binding_specs:type_name -> google.events.cloud.visionai.v1.ProcessorIOSpec.InstanceResourceInputBindingSpec
	81,  // 51: google.events.cloud.visionai.v1.ProcessorIOSpec.instance_resource_output_binding_specs:type_name -> google.events.cloud.visionai.v1.ProcessorIOSpec.InstanceResourceOutputBindingSpec
	11,  // 52: google.events.cloud.visionai.v1.CustomProcessorSourceInfo.source_type:type_name -> google.events.cloud.visionai.v1.CustomProcessorSourceInfo.SourceType
	83,  // 53: google.events.cloud.visionai.v1.CustomProcessorSourceInfo.additional_info:type_name -> google.events.cloud.visionai.v1.CustomProcessorSourceInfo.AdditionalInfoEntry
	82,  // 54: google.events.cloud.visionai.v1.CustomProcessorSourceInfo.model_schema:type_name -> google.events.cloud.visionai.v1.CustomProcessorSourceInfo.ModelSchema
	35,  // 55: google.events.cloud.visionai.v1.ProcessorConfig.video_stream_input_config:type_name -> google.events.cloud.visionai.v1.VideoStreamInputConfig
	36,  // 56: google.events.cloud.visionai.v1.ProcessorConfig.ai_enabled_devices_input_config:type_name -> google.events.cloud.visionai.v1.AIEnabledDevicesInputConfig
	37,  // 57: google.events.cloud.visionai.v1.ProcessorConfig.media_warehouse_config:type_name -> google.events.cloud.visionai.v1.MediaWarehouseConfig
	38,  // 58: google.events.cloud.visionai.v1.ProcessorConfig.person_blur_config:type_name -> google.events.cloud.visionai.v1.PersonBlurConfig
	39,  // 59: google.events.cloud.visionai.v1.ProcessorConfig.occupancy_count_config:type_name -> google.events.cloud.visionai.v1.OccupancyCountConfig
	40,  // 60: google.events.cloud.visionai.v1.ProcessorConfig.person_vehicle_detection_config:type_name -> google.events.cloud.visionai.v1.PersonVehicleDetectionConfig
	44,  // 61: google.events.cloud.visionai.v1.ProcessorConfig.vertex_automl_vision_config:type_name -> google.events.cloud.visionai.v1.VertexAutoMLVisionConfig
	45,  // 62: google.events.cloud.visionai.v1.ProcessorConfig.vertex_automl_video_config:type_name -> google.events.cloud.visionai.v1.VertexAutoMLVideoConfig
	46,  // 63: google.events.cloud.visionai.v1.ProcessorConfig.vertex_custom_config:type_name -> google.events.cloud.visionai.v1.VertexCustomConfig
	42,  // 64: google.events.cloud.visionai.v1.ProcessorConfig.general_object_detection_config:type_name -> google.events.cloud.visionai.v1.GeneralObjectDetectionConfig
	43,  // 65: google.events.cloud.visionai.v1.ProcessorConfig.big_query_config:type_name -> google.events.cloud.visionai.v1.BigQueryConfig
	41,  // 66: google.events.cloud.visionai.v1.ProcessorConfig.personal_protective_equipment_detection_config:type_name -> google.events.cloud.visionai.v1.PersonalProtectiveEquipmentDetectionConfig
	14,  // 67: google.events.cloud.visionai.v1.StreamWithAnnotation.application_annotations:type_name -> google.events.cloud.visionai.v1.StreamAnnotation
	84,  // 68: google.events.cloud.visionai.v1.StreamWithAnnotation.node_annotations:type_name -> google.events.cloud.visionai.v1.StreamWithAnnotation.NodeAnnotation
	34,  // 69: google.events.cloud.visionai.v1.VideoStreamInputConfig.streams_with_annotation:type_name -> google.events.cloud.visionai.v1.StreamWithAnnotation
	93,  // 70: google.events.cloud.visionai.v1.MediaWarehouseConfig.ttl:type_name -> google.protobuf.Duration
	12,  // 71: google.events.cloud.visionai.v1.PersonBlurConfig.person_blur_type:type_name -> google.events.cloud.visionai.v1.PersonBlurConfig.PersonBlurType
	85,  // 72: google.events.cloud.visionai.v1.BigQueryConfig.cloud_function_mapping:type_name -> google.events.cloud.visionai.v1.BigQueryConfig.CloudFunctionMappingEntry
	49,  // 73: google.events.cloud.visionai.v1.VertexCustomConfig.dedicated_resources:type_name -> google.events.cloud.visionai.v1.DedicatedResources
	3,   // 74: google.events.cloud.visionai.v1.MachineSpec.accelerator_type:type_name -> google.events.cloud.visionai.v1.AcceleratorType
	47,  // 75: google.events.cloud.visionai.v1.DedicatedResources.machine_spec:type_name -> google.events.cloud.visionai.v1.MachineSpec
	48,  // 76: google.events.cloud.visionai.v1.DedicatedResources.autoscaling_metric_specs:type_name -> google.events.cloud.visionai.v1.AutoscalingMetricSpec
	92,  // 77: google.events.cloud.visionai.v1.Stream.create_time:type_name -> google.protobuf.Timestamp
	92,  // 78: google.events.cloud.visionai.v1.Stream.update_time:type_name -> google.protobuf.Timestamp
	86,  // 79: google.events.cloud.visionai.v1.Stream.labels:type_name -> google.events.cloud.visionai.v1.Stream.LabelsEntry
	87,  // 80: google.events.cloud.visionai.v1.Stream.annotations:type_name -> google.events.cloud.visionai.v1.Stream.AnnotationsEntry
	92,  // 81: google.events.cloud.visionai.v1.Event.create_time:type_name -> google.protobuf.Timestamp
	92,  // 82: google.events.cloud.visionai.v1.Event.update_time:type_name -> google.protobuf.Timestamp
	88,  // 83: google.events.cloud.visionai.v1.Event.labels:type_name -> google.events.cloud.visionai.v1.Event.LabelsEntry
	89,  // 84: google.events.cloud.visionai.v1.Event.annotations:type_name -> google.events.cloud.visionai.v1.Event.AnnotationsEntry
	13,  // 85: google.events.cloud.visionai.v1.Event.alignment_clock:type_name -> google.events.cloud.visionai.v1.Event.Clock
	93,  // 86: google.events.cloud.visionai.v1.Event.grace_period:type_name -> google.protobuf.Duration
	92,  // 87: google.events.cloud.visionai.v1.Series.create_time:type_name -> google.protobuf.Timestamp
	92,  // 88: google.events.cloud.visionai.v1.Series.update_time:type_name -> google.protobuf.Timestamp
	90,  // 89: google.events.cloud.visionai.v1.Series.labels:type_name -> google.events.cloud.visionai.v1.Series.LabelsEntry
	91,  // 90: google.events.cloud.visionai.v1.Series.annotations:type_name -> google.events.cloud.visionai.v1.Series.AnnotationsEntry
	52,  // 91: google.events.cloud.visionai.v1.SeriesEventData.payload:type_name -> google.events.cloud.visionai.v1.Series
	29,  // 92: google.events.cloud.visionai.v1.DraftEventData.payload:type_name -> google.events.cloud.visionai.v1.Draft
	30,  // 93: google.events.cloud.visionai.v1.ProcessorEventData.payload:type_name -> google.events.cloud.visionai.v1.Processor
	24,  // 94: google.events.cloud.visionai.v1.AnalysisEventData.payload:type_name -> google.events.cloud.visionai.v1.Analysis
	18,  // 95: google.events.cloud.visionai.v1.ClusterEventData.payload:type_name -> google.events.cloud.visionai.v1.Cluster
	51,  // 96: google.events.cloud.visionai.v1.EventEventData.payload:type_name -> google.events.cloud.visionai.v1.Event
	25,  // 97: google.events.cloud.visionai.v1.ProcessEventData.payload:type_name -> google.events.cloud.visionai.v1.Process
	50,  // 98: google.events.cloud.visionai.v1.StreamEventData.payload:type_name -> google.events.cloud.visionai.v1.Stream
	26,  // 99: google.events.cloud.visionai.v1.ApplicationEventData.payload:type_name -> google.events.cloud.visionai.v1.Application
	67,  // 100: google.events.cloud.visionai.v1.AnalyzerDefinition.DebugOptions.environment_variables:type_name -> google.events.cloud.visionai.v1.AnalyzerDefinition.DebugOptions.EnvironmentVariablesEntry
	20,  // 101: google.events.cloud.visionai.v1.AnalyzerDefinition.AttrsEntry.value:type_name -> google.events.cloud.visionai.v1.AttributeValue
	92,  // 102: google.events.cloud.visionai.v1.Application.ApplicationRuntimeInfo.deploy_time:type_name -> google.protobuf.Timestamp
	73,  // 103: google.events.cloud.visionai.v1.Application.ApplicationRuntimeInfo.global_output_resources:type_name -> google.events.cloud.visionai.v1.Application.ApplicationRuntimeInfo.GlobalOutputResource
	74,  // 104: google.events.cloud.visionai.v1.Application.ApplicationRuntimeInfo.monitoring_config:type_name -> google.events.cloud.visionai.v1.Application.ApplicationRuntimeInfo.MonitoringConfig
	4,   // 105: google.events.cloud.visionai.v1.ProcessorIOSpec.GraphInputChannelSpec.data_type:type_name -> google.events.cloud.visionai.v1.DataType
	4,   // 106: google.events.cloud.visionai.v1.ProcessorIOSpec.GraphOutputChannelSpec.data_type:type_name -> google.events.cloud.visionai.v1.DataType
	19,  // 107: google.events.cloud.visionai.v1.CustomProcessorSourceInfo.ModelSchema.instances_schema:type_name -> google.events.cloud.visionai.v1.GcsSource
	19,  // 108: google.events.cloud.visionai.v1.CustomProcessorSourceInfo.ModelSchema.parameters_schema:type_name -> google.events.cloud.visionai.v1.GcsSource
	19,  // 109: google.events.cloud.visionai.v1.CustomProcessorSourceInfo.ModelSchema.predictions_schema:type_name -> google.events.cloud.visionai.v1.GcsSource
	14,  // 110: google.events.cloud.visionai.v1.StreamWithAnnotation.NodeAnnotation.annotations:type_name -> google.events.cloud.visionai.v1.StreamAnnotation
	111, // [111:111] is the sub-list for method output_type
	111, // [111:111] is the sub-list for method input_type
	111, // [111:111] is the sub-list for extension type_name
	111, // [111:111] is the sub-list for extension extendee
	0,   // [0:111] is the sub-list for field type_name
}

func init() { file_cloud_visionai_v1_data_proto_init() }
func file_cloud_visionai_v1_data_proto_init() {
	if File_cloud_visionai_v1_data_proto != nil {
		return
	}
	file_cloud_visionai_v1_data_proto_msgTypes[0].OneofWrappers = []any{
		(*StreamAnnotation_ActiveZone)(nil),
		(*StreamAnnotation_CrossingLine)(nil),
	}
	file_cloud_visionai_v1_data_proto_msgTypes[6].OneofWrappers = []any{
		(*AttributeValue_I)(nil),
		(*AttributeValue_F)(nil),
		(*AttributeValue_B)(nil),
		(*AttributeValue_S)(nil),
	}
	file_cloud_visionai_v1_data_proto_msgTypes[14].OneofWrappers = []any{
		(*Node_OutputAllOutputChannelsToStream)(nil),
	}
	file_cloud_visionai_v1_data_proto_msgTypes[18].OneofWrappers = []any{
		(*CustomProcessorSourceInfo_VertexModel)(nil),
	}
	file_cloud_visionai_v1_data_proto_msgTypes[19].OneofWrappers = []any{
		(*ProcessorConfig_VideoStreamInputConfig)(nil),
		(*ProcessorConfig_AiEnabledDevicesInputConfig)(nil),
		(*ProcessorConfig_MediaWarehouseConfig)(nil),
		(*ProcessorConfig_PersonBlurConfig)(nil),
		(*ProcessorConfig_OccupancyCountConfig)(nil),
		(*ProcessorConfig_PersonVehicleDetectionConfig)(nil),
		(*ProcessorConfig_VertexAutomlVisionConfig)(nil),
		(*ProcessorConfig_VertexAutomlVideoConfig)(nil),
		(*ProcessorConfig_VertexCustomConfig)(nil),
		(*ProcessorConfig_GeneralObjectDetectionConfig)(nil),
		(*ProcessorConfig_BigQueryConfig)(nil),
		(*ProcessorConfig_PersonalProtectiveEquipmentDetectionConfig)(nil),
	}
	file_cloud_visionai_v1_data_proto_msgTypes[39].OneofWrappers = []any{}
	file_cloud_visionai_v1_data_proto_msgTypes[40].OneofWrappers = []any{}
	file_cloud_visionai_v1_data_proto_msgTypes[41].OneofWrappers = []any{}
	file_cloud_visionai_v1_data_proto_msgTypes[42].OneofWrappers = []any{}
	file_cloud_visionai_v1_data_proto_msgTypes[43].OneofWrappers = []any{}
	file_cloud_visionai_v1_data_proto_msgTypes[44].OneofWrappers = []any{}
	file_cloud_visionai_v1_data_proto_msgTypes[45].OneofWrappers = []any{}
	file_cloud_visionai_v1_data_proto_msgTypes[46].OneofWrappers = []any{}
	file_cloud_visionai_v1_data_proto_msgTypes[47].OneofWrappers = []any{}
	file_cloud_visionai_v1_data_proto_msgTypes[66].OneofWrappers = []any{
		(*ProcessorIOSpec_InstanceResourceInputBindingSpec_ConfigTypeUri)(nil),
		(*ProcessorIOSpec_InstanceResourceInputBindingSpec_ResourceTypeUri)(nil),
	}
	type x struct{}
	out := protoimpl.TypeBuilder{
		File: protoimpl.DescBuilder{
			GoPackagePath: reflect.TypeOf(x{}).PkgPath(),
			RawDescriptor: unsafe.Slice(unsafe.StringData(file_cloud_visionai_v1_data_proto_rawDesc), len(file_cloud_visionai_v1_data_proto_rawDesc)),
			NumEnums:      14,
			NumMessages:   78,
			NumExtensions: 0,
			NumServices:   0,
		},
		GoTypes:           file_cloud_visionai_v1_data_proto_goTypes,
		DependencyIndexes: file_cloud_visionai_v1_data_proto_depIdxs,
		EnumInfos:         file_cloud_visionai_v1_data_proto_enumTypes,
		MessageInfos:      file_cloud_visionai_v1_data_proto_msgTypes,
	}.Build()
	File_cloud_visionai_v1_data_proto = out.File
	file_cloud_visionai_v1_data_proto_goTypes = nil
	file_cloud_visionai_v1_data_proto_depIdxs = nil
}
