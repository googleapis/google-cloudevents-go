// Copyright 2023 Google LLC
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     https://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

// Code generated by protoc-gen-go. DO NOT EDIT.
// versions:
// 	protoc-gen-go v1.36.6
// 	protoc        v3.21.6
// source: cloud/dataflow/v1beta3/data.proto

package dataflowdatav1beta3

import (
	protoreflect "google.golang.org/protobuf/reflect/protoreflect"
	protoimpl "google.golang.org/protobuf/runtime/protoimpl"
	structpb "google.golang.org/protobuf/types/known/structpb"
	timestamppb "google.golang.org/protobuf/types/known/timestamppb"
	reflect "reflect"
	sync "sync"
	unsafe "unsafe"
)

const (
	// Verify that this generated code is sufficiently up-to-date.
	_ = protoimpl.EnforceVersion(20 - protoimpl.MinVersion)
	// Verify that runtime/protoimpl is sufficiently up-to-date.
	_ = protoimpl.EnforceVersion(protoimpl.MaxVersion - 20)
)

// Specifies the processing model used by a
// [google.dataflow.v1beta3.Job], which determines the way the Job is
// managed by the Cloud Dataflow service (how workers are scheduled, how
// inputs are sharded, etc).
type JobType int32

const (
	// The type of the job is unspecified, or unknown.
	JobType_JOB_TYPE_UNKNOWN JobType = 0
	// A batch job with a well-defined end point: data is read, data is
	// processed, data is written, and the job is done.
	JobType_JOB_TYPE_BATCH JobType = 1
	// A continuously streaming job with no end: data is read,
	// processed, and written continuously.
	JobType_JOB_TYPE_STREAMING JobType = 2
)

// Enum value maps for JobType.
var (
	JobType_name = map[int32]string{
		0: "JOB_TYPE_UNKNOWN",
		1: "JOB_TYPE_BATCH",
		2: "JOB_TYPE_STREAMING",
	}
	JobType_value = map[string]int32{
		"JOB_TYPE_UNKNOWN":   0,
		"JOB_TYPE_BATCH":     1,
		"JOB_TYPE_STREAMING": 2,
	}
)

func (x JobType) Enum() *JobType {
	p := new(JobType)
	*p = x
	return p
}

func (x JobType) String() string {
	return protoimpl.X.EnumStringOf(x.Descriptor(), protoreflect.EnumNumber(x))
}

func (JobType) Descriptor() protoreflect.EnumDescriptor {
	return file_cloud_dataflow_v1beta3_data_proto_enumTypes[0].Descriptor()
}

func (JobType) Type() protoreflect.EnumType {
	return &file_cloud_dataflow_v1beta3_data_proto_enumTypes[0]
}

func (x JobType) Number() protoreflect.EnumNumber {
	return protoreflect.EnumNumber(x)
}

// Deprecated: Use JobType.Descriptor instead.
func (JobType) EnumDescriptor() ([]byte, []int) {
	return file_cloud_dataflow_v1beta3_data_proto_rawDescGZIP(), []int{0}
}

// Specifies the resource to optimize for in Flexible Resource Scheduling.
type FlexResourceSchedulingGoal int32

const (
	// Run in the default mode.
	FlexResourceSchedulingGoal_FLEXRS_UNSPECIFIED FlexResourceSchedulingGoal = 0
	// Optimize for lower execution time.
	FlexResourceSchedulingGoal_FLEXRS_SPEED_OPTIMIZED FlexResourceSchedulingGoal = 1
	// Optimize for lower cost.
	FlexResourceSchedulingGoal_FLEXRS_COST_OPTIMIZED FlexResourceSchedulingGoal = 2
)

// Enum value maps for FlexResourceSchedulingGoal.
var (
	FlexResourceSchedulingGoal_name = map[int32]string{
		0: "FLEXRS_UNSPECIFIED",
		1: "FLEXRS_SPEED_OPTIMIZED",
		2: "FLEXRS_COST_OPTIMIZED",
	}
	FlexResourceSchedulingGoal_value = map[string]int32{
		"FLEXRS_UNSPECIFIED":     0,
		"FLEXRS_SPEED_OPTIMIZED": 1,
		"FLEXRS_COST_OPTIMIZED":  2,
	}
)

func (x FlexResourceSchedulingGoal) Enum() *FlexResourceSchedulingGoal {
	p := new(FlexResourceSchedulingGoal)
	*p = x
	return p
}

func (x FlexResourceSchedulingGoal) String() string {
	return protoimpl.X.EnumStringOf(x.Descriptor(), protoreflect.EnumNumber(x))
}

func (FlexResourceSchedulingGoal) Descriptor() protoreflect.EnumDescriptor {
	return file_cloud_dataflow_v1beta3_data_proto_enumTypes[1].Descriptor()
}

func (FlexResourceSchedulingGoal) Type() protoreflect.EnumType {
	return &file_cloud_dataflow_v1beta3_data_proto_enumTypes[1]
}

func (x FlexResourceSchedulingGoal) Number() protoreflect.EnumNumber {
	return protoreflect.EnumNumber(x)
}

// Deprecated: Use FlexResourceSchedulingGoal.Descriptor instead.
func (FlexResourceSchedulingGoal) EnumDescriptor() ([]byte, []int) {
	return file_cloud_dataflow_v1beta3_data_proto_rawDescGZIP(), []int{1}
}

// Specifies what happens to a resource when a Cloud Dataflow
// [google.dataflow.v1beta3.Job][google.dataflow.v1beta3.Job] has completed.
type TeardownPolicy int32

const (
	// The teardown policy isn't specified, or is unknown.
	TeardownPolicy_TEARDOWN_POLICY_UNKNOWN TeardownPolicy = 0
	// Always teardown the resource.
	TeardownPolicy_TEARDOWN_ALWAYS TeardownPolicy = 1
	// Teardown the resource on success. This is useful for debugging
	// failures.
	TeardownPolicy_TEARDOWN_ON_SUCCESS TeardownPolicy = 2
	// Never teardown the resource. This is useful for debugging and
	// development.
	TeardownPolicy_TEARDOWN_NEVER TeardownPolicy = 3
)

// Enum value maps for TeardownPolicy.
var (
	TeardownPolicy_name = map[int32]string{
		0: "TEARDOWN_POLICY_UNKNOWN",
		1: "TEARDOWN_ALWAYS",
		2: "TEARDOWN_ON_SUCCESS",
		3: "TEARDOWN_NEVER",
	}
	TeardownPolicy_value = map[string]int32{
		"TEARDOWN_POLICY_UNKNOWN": 0,
		"TEARDOWN_ALWAYS":         1,
		"TEARDOWN_ON_SUCCESS":     2,
		"TEARDOWN_NEVER":          3,
	}
)

func (x TeardownPolicy) Enum() *TeardownPolicy {
	p := new(TeardownPolicy)
	*p = x
	return p
}

func (x TeardownPolicy) String() string {
	return protoimpl.X.EnumStringOf(x.Descriptor(), protoreflect.EnumNumber(x))
}

func (TeardownPolicy) Descriptor() protoreflect.EnumDescriptor {
	return file_cloud_dataflow_v1beta3_data_proto_enumTypes[2].Descriptor()
}

func (TeardownPolicy) Type() protoreflect.EnumType {
	return &file_cloud_dataflow_v1beta3_data_proto_enumTypes[2]
}

func (x TeardownPolicy) Number() protoreflect.EnumNumber {
	return protoreflect.EnumNumber(x)
}

// Deprecated: Use TeardownPolicy.Descriptor instead.
func (TeardownPolicy) EnumDescriptor() ([]byte, []int) {
	return file_cloud_dataflow_v1beta3_data_proto_rawDescGZIP(), []int{2}
}

// The default set of packages to be staged on a pool of workers.
type DefaultPackageSet int32

const (
	// The default set of packages to stage is unknown, or unspecified.
	DefaultPackageSet_DEFAULT_PACKAGE_SET_UNKNOWN DefaultPackageSet = 0
	// Indicates that no packages should be staged at the worker unless
	// explicitly specified by the job.
	DefaultPackageSet_DEFAULT_PACKAGE_SET_NONE DefaultPackageSet = 1
	// Stage packages typically useful to workers written in Java.
	DefaultPackageSet_DEFAULT_PACKAGE_SET_JAVA DefaultPackageSet = 2
	// Stage packages typically useful to workers written in Python.
	DefaultPackageSet_DEFAULT_PACKAGE_SET_PYTHON DefaultPackageSet = 3
)

// Enum value maps for DefaultPackageSet.
var (
	DefaultPackageSet_name = map[int32]string{
		0: "DEFAULT_PACKAGE_SET_UNKNOWN",
		1: "DEFAULT_PACKAGE_SET_NONE",
		2: "DEFAULT_PACKAGE_SET_JAVA",
		3: "DEFAULT_PACKAGE_SET_PYTHON",
	}
	DefaultPackageSet_value = map[string]int32{
		"DEFAULT_PACKAGE_SET_UNKNOWN": 0,
		"DEFAULT_PACKAGE_SET_NONE":    1,
		"DEFAULT_PACKAGE_SET_JAVA":    2,
		"DEFAULT_PACKAGE_SET_PYTHON":  3,
	}
)

func (x DefaultPackageSet) Enum() *DefaultPackageSet {
	p := new(DefaultPackageSet)
	*p = x
	return p
}

func (x DefaultPackageSet) String() string {
	return protoimpl.X.EnumStringOf(x.Descriptor(), protoreflect.EnumNumber(x))
}

func (DefaultPackageSet) Descriptor() protoreflect.EnumDescriptor {
	return file_cloud_dataflow_v1beta3_data_proto_enumTypes[3].Descriptor()
}

func (DefaultPackageSet) Type() protoreflect.EnumType {
	return &file_cloud_dataflow_v1beta3_data_proto_enumTypes[3]
}

func (x DefaultPackageSet) Number() protoreflect.EnumNumber {
	return protoreflect.EnumNumber(x)
}

// Deprecated: Use DefaultPackageSet.Descriptor instead.
func (DefaultPackageSet) EnumDescriptor() ([]byte, []int) {
	return file_cloud_dataflow_v1beta3_data_proto_rawDescGZIP(), []int{3}
}

// Specifies the algorithm used to determine the number of worker
// processes to run at any given point in time, based on the amount of
// data left to process, the number of workers, and how quickly
// existing workers are processing data.
type AutoscalingAlgorithm int32

const (
	// The algorithm is unknown, or unspecified.
	AutoscalingAlgorithm_AUTOSCALING_ALGORITHM_UNKNOWN AutoscalingAlgorithm = 0
	// Disable autoscaling.
	AutoscalingAlgorithm_AUTOSCALING_ALGORITHM_NONE AutoscalingAlgorithm = 1
	// Increase worker count over time to reduce job execution time.
	AutoscalingAlgorithm_AUTOSCALING_ALGORITHM_BASIC AutoscalingAlgorithm = 2
)

// Enum value maps for AutoscalingAlgorithm.
var (
	AutoscalingAlgorithm_name = map[int32]string{
		0: "AUTOSCALING_ALGORITHM_UNKNOWN",
		1: "AUTOSCALING_ALGORITHM_NONE",
		2: "AUTOSCALING_ALGORITHM_BASIC",
	}
	AutoscalingAlgorithm_value = map[string]int32{
		"AUTOSCALING_ALGORITHM_UNKNOWN": 0,
		"AUTOSCALING_ALGORITHM_NONE":    1,
		"AUTOSCALING_ALGORITHM_BASIC":   2,
	}
)

func (x AutoscalingAlgorithm) Enum() *AutoscalingAlgorithm {
	p := new(AutoscalingAlgorithm)
	*p = x
	return p
}

func (x AutoscalingAlgorithm) String() string {
	return protoimpl.X.EnumStringOf(x.Descriptor(), protoreflect.EnumNumber(x))
}

func (AutoscalingAlgorithm) Descriptor() protoreflect.EnumDescriptor {
	return file_cloud_dataflow_v1beta3_data_proto_enumTypes[4].Descriptor()
}

func (AutoscalingAlgorithm) Type() protoreflect.EnumType {
	return &file_cloud_dataflow_v1beta3_data_proto_enumTypes[4]
}

func (x AutoscalingAlgorithm) Number() protoreflect.EnumNumber {
	return protoreflect.EnumNumber(x)
}

// Deprecated: Use AutoscalingAlgorithm.Descriptor instead.
func (AutoscalingAlgorithm) EnumDescriptor() ([]byte, []int) {
	return file_cloud_dataflow_v1beta3_data_proto_rawDescGZIP(), []int{4}
}

// Specifies how IP addresses should be allocated to the worker machines.
type WorkerIPAddressConfiguration int32

const (
	// The configuration is unknown, or unspecified.
	WorkerIPAddressConfiguration_WORKER_IP_UNSPECIFIED WorkerIPAddressConfiguration = 0
	// Workers should have public IP addresses.
	WorkerIPAddressConfiguration_WORKER_IP_PUBLIC WorkerIPAddressConfiguration = 1
	// Workers should have private IP addresses.
	WorkerIPAddressConfiguration_WORKER_IP_PRIVATE WorkerIPAddressConfiguration = 2
)

// Enum value maps for WorkerIPAddressConfiguration.
var (
	WorkerIPAddressConfiguration_name = map[int32]string{
		0: "WORKER_IP_UNSPECIFIED",
		1: "WORKER_IP_PUBLIC",
		2: "WORKER_IP_PRIVATE",
	}
	WorkerIPAddressConfiguration_value = map[string]int32{
		"WORKER_IP_UNSPECIFIED": 0,
		"WORKER_IP_PUBLIC":      1,
		"WORKER_IP_PRIVATE":     2,
	}
)

func (x WorkerIPAddressConfiguration) Enum() *WorkerIPAddressConfiguration {
	p := new(WorkerIPAddressConfiguration)
	*p = x
	return p
}

func (x WorkerIPAddressConfiguration) String() string {
	return protoimpl.X.EnumStringOf(x.Descriptor(), protoreflect.EnumNumber(x))
}

func (WorkerIPAddressConfiguration) Descriptor() protoreflect.EnumDescriptor {
	return file_cloud_dataflow_v1beta3_data_proto_enumTypes[5].Descriptor()
}

func (WorkerIPAddressConfiguration) Type() protoreflect.EnumType {
	return &file_cloud_dataflow_v1beta3_data_proto_enumTypes[5]
}

func (x WorkerIPAddressConfiguration) Number() protoreflect.EnumNumber {
	return protoreflect.EnumNumber(x)
}

// Deprecated: Use WorkerIPAddressConfiguration.Descriptor instead.
func (WorkerIPAddressConfiguration) EnumDescriptor() ([]byte, []int) {
	return file_cloud_dataflow_v1beta3_data_proto_rawDescGZIP(), []int{5}
}

// Specifies the shuffle mode used by a
// [google.dataflow.v1beta3.Job], which determines the approach data is shuffled
// during processing. More details in:
// https://cloud.google.com/dataflow/docs/guides/deploying-a-pipeline#dataflow-shuffle
type ShuffleMode int32

const (
	// Shuffle mode information is not available.
	ShuffleMode_SHUFFLE_MODE_UNSPECIFIED ShuffleMode = 0
	// Shuffle is done on the worker VMs.
	ShuffleMode_VM_BASED ShuffleMode = 1
	// Shuffle is done on the service side.
	ShuffleMode_SERVICE_BASED ShuffleMode = 2
)

// Enum value maps for ShuffleMode.
var (
	ShuffleMode_name = map[int32]string{
		0: "SHUFFLE_MODE_UNSPECIFIED",
		1: "VM_BASED",
		2: "SERVICE_BASED",
	}
	ShuffleMode_value = map[string]int32{
		"SHUFFLE_MODE_UNSPECIFIED": 0,
		"VM_BASED":                 1,
		"SERVICE_BASED":            2,
	}
)

func (x ShuffleMode) Enum() *ShuffleMode {
	p := new(ShuffleMode)
	*p = x
	return p
}

func (x ShuffleMode) String() string {
	return protoimpl.X.EnumStringOf(x.Descriptor(), protoreflect.EnumNumber(x))
}

func (ShuffleMode) Descriptor() protoreflect.EnumDescriptor {
	return file_cloud_dataflow_v1beta3_data_proto_enumTypes[6].Descriptor()
}

func (ShuffleMode) Type() protoreflect.EnumType {
	return &file_cloud_dataflow_v1beta3_data_proto_enumTypes[6]
}

func (x ShuffleMode) Number() protoreflect.EnumNumber {
	return protoreflect.EnumNumber(x)
}

// Deprecated: Use ShuffleMode.Descriptor instead.
func (ShuffleMode) EnumDescriptor() ([]byte, []int) {
	return file_cloud_dataflow_v1beta3_data_proto_rawDescGZIP(), []int{6}
}

// Describes the overall state of a
// [google.dataflow.v1beta3.Job][google.dataflow.v1beta3.Job].
type JobState int32

const (
	// The job's run state isn't specified.
	JobState_JOB_STATE_UNKNOWN JobState = 0
	// `JOB_STATE_STOPPED` indicates that the job has not
	// yet started to run.
	JobState_JOB_STATE_STOPPED JobState = 1
	// `JOB_STATE_RUNNING` indicates that the job is currently running.
	JobState_JOB_STATE_RUNNING JobState = 2
	// `JOB_STATE_DONE` indicates that the job has successfully completed.
	// This is a terminal job state.  This state may be set by the Cloud Dataflow
	// service, as a transition from `JOB_STATE_RUNNING`. It may also be set via a
	// Cloud Dataflow `UpdateJob` call, if the job has not yet reached a terminal
	// state.
	JobState_JOB_STATE_DONE JobState = 3
	// `JOB_STATE_FAILED` indicates that the job has failed.  This is a
	// terminal job state.  This state may only be set by the Cloud Dataflow
	// service, and only as a transition from `JOB_STATE_RUNNING`.
	JobState_JOB_STATE_FAILED JobState = 4
	// `JOB_STATE_CANCELLED` indicates that the job has been explicitly
	// cancelled. This is a terminal job state. This state may only be
	// set via a Cloud Dataflow `UpdateJob` call, and only if the job has not
	// yet reached another terminal state.
	JobState_JOB_STATE_CANCELLED JobState = 5
	// `JOB_STATE_UPDATED` indicates that the job was successfully updated,
	// meaning that this job was stopped and another job was started, inheriting
	// state from this one. This is a terminal job state. This state may only be
	// set by the Cloud Dataflow service, and only as a transition from
	// `JOB_STATE_RUNNING`.
	JobState_JOB_STATE_UPDATED JobState = 6
	// `JOB_STATE_DRAINING` indicates that the job is in the process of draining.
	// A draining job has stopped pulling from its input sources and is processing
	// any data that remains in-flight. This state may be set via a Cloud Dataflow
	// `UpdateJob` call, but only as a transition from `JOB_STATE_RUNNING`. Jobs
	// that are draining may only transition to `JOB_STATE_DRAINED`,
	// `JOB_STATE_CANCELLED`, or `JOB_STATE_FAILED`.
	JobState_JOB_STATE_DRAINING JobState = 7
	// `JOB_STATE_DRAINED` indicates that the job has been drained.
	// A drained job terminated by stopping pulling from its input sources and
	// processing any data that remained in-flight when draining was requested.
	// This state is a terminal state, may only be set by the Cloud Dataflow
	// service, and only as a transition from `JOB_STATE_DRAINING`.
	JobState_JOB_STATE_DRAINED JobState = 8
	// `JOB_STATE_PENDING` indicates that the job has been created but is not yet
	// running.  Jobs that are pending may only transition to `JOB_STATE_RUNNING`,
	// or `JOB_STATE_FAILED`.
	JobState_JOB_STATE_PENDING JobState = 9
	// `JOB_STATE_CANCELLING` indicates that the job has been explicitly cancelled
	// and is in the process of stopping.  Jobs that are cancelling may only
	// transition to `JOB_STATE_CANCELLED` or `JOB_STATE_FAILED`.
	JobState_JOB_STATE_CANCELLING JobState = 10
	// `JOB_STATE_QUEUED` indicates that the job has been created but is being
	// delayed until launch. Jobs that are queued may only transition to
	// `JOB_STATE_PENDING` or `JOB_STATE_CANCELLED`.
	JobState_JOB_STATE_QUEUED JobState = 11
	// `JOB_STATE_RESOURCE_CLEANING_UP` indicates that the batch job's associated
	// resources are currently being cleaned up after a successful run.
	// Currently, this is an opt-in feature, please reach out to Cloud support
	// team if you are interested.
	JobState_JOB_STATE_RESOURCE_CLEANING_UP JobState = 12
)

// Enum value maps for JobState.
var (
	JobState_name = map[int32]string{
		0:  "JOB_STATE_UNKNOWN",
		1:  "JOB_STATE_STOPPED",
		2:  "JOB_STATE_RUNNING",
		3:  "JOB_STATE_DONE",
		4:  "JOB_STATE_FAILED",
		5:  "JOB_STATE_CANCELLED",
		6:  "JOB_STATE_UPDATED",
		7:  "JOB_STATE_DRAINING",
		8:  "JOB_STATE_DRAINED",
		9:  "JOB_STATE_PENDING",
		10: "JOB_STATE_CANCELLING",
		11: "JOB_STATE_QUEUED",
		12: "JOB_STATE_RESOURCE_CLEANING_UP",
	}
	JobState_value = map[string]int32{
		"JOB_STATE_UNKNOWN":              0,
		"JOB_STATE_STOPPED":              1,
		"JOB_STATE_RUNNING":              2,
		"JOB_STATE_DONE":                 3,
		"JOB_STATE_FAILED":               4,
		"JOB_STATE_CANCELLED":            5,
		"JOB_STATE_UPDATED":              6,
		"JOB_STATE_DRAINING":             7,
		"JOB_STATE_DRAINED":              8,
		"JOB_STATE_PENDING":              9,
		"JOB_STATE_CANCELLING":           10,
		"JOB_STATE_QUEUED":               11,
		"JOB_STATE_RESOURCE_CLEANING_UP": 12,
	}
)

func (x JobState) Enum() *JobState {
	p := new(JobState)
	*p = x
	return p
}

func (x JobState) String() string {
	return protoimpl.X.EnumStringOf(x.Descriptor(), protoreflect.EnumNumber(x))
}

func (JobState) Descriptor() protoreflect.EnumDescriptor {
	return file_cloud_dataflow_v1beta3_data_proto_enumTypes[7].Descriptor()
}

func (JobState) Type() protoreflect.EnumType {
	return &file_cloud_dataflow_v1beta3_data_proto_enumTypes[7]
}

func (x JobState) Number() protoreflect.EnumNumber {
	return protoreflect.EnumNumber(x)
}

// Deprecated: Use JobState.Descriptor instead.
func (JobState) EnumDescriptor() ([]byte, []int) {
	return file_cloud_dataflow_v1beta3_data_proto_rawDescGZIP(), []int{7}
}

// The support status of the SDK used to run the job.
type SdkVersion_SdkSupportStatus int32

const (
	// Cloud Dataflow is unaware of this version.
	SdkVersion_UNKNOWN SdkVersion_SdkSupportStatus = 0
	// This is a known version of an SDK, and is supported.
	SdkVersion_SUPPORTED SdkVersion_SdkSupportStatus = 1
	// A newer version of the SDK family exists, and an update is recommended.
	SdkVersion_STALE SdkVersion_SdkSupportStatus = 2
	// This version of the SDK is deprecated and will eventually be
	// unsupported.
	SdkVersion_DEPRECATED SdkVersion_SdkSupportStatus = 3
	// Support for this SDK version has ended and it should no longer be used.
	SdkVersion_UNSUPPORTED SdkVersion_SdkSupportStatus = 4
)

// Enum value maps for SdkVersion_SdkSupportStatus.
var (
	SdkVersion_SdkSupportStatus_name = map[int32]string{
		0: "UNKNOWN",
		1: "SUPPORTED",
		2: "STALE",
		3: "DEPRECATED",
		4: "UNSUPPORTED",
	}
	SdkVersion_SdkSupportStatus_value = map[string]int32{
		"UNKNOWN":     0,
		"SUPPORTED":   1,
		"STALE":       2,
		"DEPRECATED":  3,
		"UNSUPPORTED": 4,
	}
)

func (x SdkVersion_SdkSupportStatus) Enum() *SdkVersion_SdkSupportStatus {
	p := new(SdkVersion_SdkSupportStatus)
	*p = x
	return p
}

func (x SdkVersion_SdkSupportStatus) String() string {
	return protoimpl.X.EnumStringOf(x.Descriptor(), protoreflect.EnumNumber(x))
}

func (SdkVersion_SdkSupportStatus) Descriptor() protoreflect.EnumDescriptor {
	return file_cloud_dataflow_v1beta3_data_proto_enumTypes[8].Descriptor()
}

func (SdkVersion_SdkSupportStatus) Type() protoreflect.EnumType {
	return &file_cloud_dataflow_v1beta3_data_proto_enumTypes[8]
}

func (x SdkVersion_SdkSupportStatus) Number() protoreflect.EnumNumber {
	return protoreflect.EnumNumber(x)
}

// Deprecated: Use SdkVersion_SdkSupportStatus.Descriptor instead.
func (SdkVersion_SdkSupportStatus) EnumDescriptor() ([]byte, []int) {
	return file_cloud_dataflow_v1beta3_data_proto_rawDescGZIP(), []int{13, 0}
}

// Describes the environment in which a Dataflow Job runs.
type Environment struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// The prefix of the resources the system should use for temporary
	// storage.  The system will append the suffix "/temp-{JOBNAME} to
	// this resource prefix, where {JOBNAME} is the value of the
	// job_name field.  The resulting bucket and object prefix is used
	// as the prefix of the resources used to store temporary data
	// needed during the job execution.  NOTE: This will override the
	// value in taskrunner_settings.
	// The supported resource type is:
	//
	// Google Cloud Storage:
	//
	//	storage.googleapis.com/{bucket}/{object}
	//	bucket.storage.googleapis.com/{object}
	TempStoragePrefix string `protobuf:"bytes,1,opt,name=temp_storage_prefix,json=tempStoragePrefix,proto3" json:"temp_storage_prefix,omitempty"`
	// The type of cluster manager API to use.  If unknown or
	// unspecified, the service will attempt to choose a reasonable
	// default.  This should be in the form of the API service name,
	// e.g. "compute.googleapis.com".
	ClusterManagerApiService string `protobuf:"bytes,2,opt,name=cluster_manager_api_service,json=clusterManagerApiService,proto3" json:"cluster_manager_api_service,omitempty"`
	// The list of experiments to enable. This field should be used for SDK
	// related experiments and not for service related experiments. The proper
	// field for service related experiments is service_options.
	Experiments []string `protobuf:"bytes,3,rep,name=experiments,proto3" json:"experiments,omitempty"`
	// The list of service options to enable. This field should be used for
	// service related experiments only. These experiments, when graduating to GA,
	// should be replaced by dedicated fields or become default (i.e. always on).
	ServiceOptions []string `protobuf:"bytes,16,rep,name=service_options,json=serviceOptions,proto3" json:"service_options,omitempty"`
	// If set, contains the Cloud KMS key identifier used to encrypt data
	// at rest, AKA a Customer Managed Encryption Key (CMEK).
	//
	// Format:
	//
	//	projects/PROJECT_ID/locations/LOCATION/keyRings/KEY_RING/cryptoKeys/KEY
	ServiceKmsKeyName string `protobuf:"bytes,12,opt,name=service_kms_key_name,json=serviceKmsKeyName,proto3" json:"service_kms_key_name,omitempty"`
	// The worker pools. At least one "harness" worker pool must be
	// specified in order for the job to have workers.
	WorkerPools []*WorkerPool `protobuf:"bytes,4,rep,name=worker_pools,json=workerPools,proto3" json:"worker_pools,omitempty"`
	// A description of the process that generated the request.
	UserAgent *structpb.Struct `protobuf:"bytes,5,opt,name=user_agent,json=userAgent,proto3" json:"user_agent,omitempty"`
	// A structure describing which components and their versions of the service
	// are required in order to run the job.
	Version *structpb.Struct `protobuf:"bytes,6,opt,name=version,proto3" json:"version,omitempty"`
	// The dataset for the current project where various workflow
	// related tables are stored.
	//
	// The supported resource type is:
	//
	// Google BigQuery:
	//
	//	bigquery.googleapis.com/{dataset}
	Dataset string `protobuf:"bytes,7,opt,name=dataset,proto3" json:"dataset,omitempty"`
	// The Cloud Dataflow SDK pipeline options specified by the user. These
	// options are passed through the service and are used to recreate the
	// SDK pipeline options on the worker in a language agnostic and platform
	// independent way.
	SdkPipelineOptions *structpb.Struct `protobuf:"bytes,8,opt,name=sdk_pipeline_options,json=sdkPipelineOptions,proto3" json:"sdk_pipeline_options,omitempty"`
	// Identity to run virtual machines as. Defaults to the default account.
	ServiceAccountEmail string `protobuf:"bytes,10,opt,name=service_account_email,json=serviceAccountEmail,proto3" json:"service_account_email,omitempty"`
	// Which Flexible Resource Scheduling mode to run in.
	FlexResourceSchedulingGoal FlexResourceSchedulingGoal `protobuf:"varint,11,opt,name=flex_resource_scheduling_goal,json=flexResourceSchedulingGoal,proto3,enum=google.events.cloud.dataflow.v1beta3.FlexResourceSchedulingGoal" json:"flex_resource_scheduling_goal,omitempty"`
	// The Compute Engine region
	// (https://cloud.google.com/compute/docs/regions-zones/regions-zones) in
	// which worker processing should occur, e.g. "us-west1". Mutually exclusive
	// with worker_zone. If neither worker_region nor worker_zone is specified,
	// default to the control plane's region.
	WorkerRegion string `protobuf:"bytes,13,opt,name=worker_region,json=workerRegion,proto3" json:"worker_region,omitempty"`
	// The Compute Engine zone
	// (https://cloud.google.com/compute/docs/regions-zones/regions-zones) in
	// which worker processing should occur, e.g. "us-west1-a". Mutually exclusive
	// with worker_region. If neither worker_region nor worker_zone is specified,
	// a zone in the control plane's region is chosen based on available capacity.
	WorkerZone string `protobuf:"bytes,14,opt,name=worker_zone,json=workerZone,proto3" json:"worker_zone,omitempty"`
	// Output only. The shuffle mode used for the job.
	ShuffleMode ShuffleMode `protobuf:"varint,15,opt,name=shuffle_mode,json=shuffleMode,proto3,enum=google.events.cloud.dataflow.v1beta3.ShuffleMode" json:"shuffle_mode,omitempty"`
	// Any debugging options to be supplied to the job.
	DebugOptions  *DebugOptions `protobuf:"bytes,17,opt,name=debug_options,json=debugOptions,proto3" json:"debug_options,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *Environment) Reset() {
	*x = Environment{}
	mi := &file_cloud_dataflow_v1beta3_data_proto_msgTypes[0]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *Environment) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*Environment) ProtoMessage() {}

func (x *Environment) ProtoReflect() protoreflect.Message {
	mi := &file_cloud_dataflow_v1beta3_data_proto_msgTypes[0]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use Environment.ProtoReflect.Descriptor instead.
func (*Environment) Descriptor() ([]byte, []int) {
	return file_cloud_dataflow_v1beta3_data_proto_rawDescGZIP(), []int{0}
}

func (x *Environment) GetTempStoragePrefix() string {
	if x != nil {
		return x.TempStoragePrefix
	}
	return ""
}

func (x *Environment) GetClusterManagerApiService() string {
	if x != nil {
		return x.ClusterManagerApiService
	}
	return ""
}

func (x *Environment) GetExperiments() []string {
	if x != nil {
		return x.Experiments
	}
	return nil
}

func (x *Environment) GetServiceOptions() []string {
	if x != nil {
		return x.ServiceOptions
	}
	return nil
}

func (x *Environment) GetServiceKmsKeyName() string {
	if x != nil {
		return x.ServiceKmsKeyName
	}
	return ""
}

func (x *Environment) GetWorkerPools() []*WorkerPool {
	if x != nil {
		return x.WorkerPools
	}
	return nil
}

func (x *Environment) GetUserAgent() *structpb.Struct {
	if x != nil {
		return x.UserAgent
	}
	return nil
}

func (x *Environment) GetVersion() *structpb.Struct {
	if x != nil {
		return x.Version
	}
	return nil
}

func (x *Environment) GetDataset() string {
	if x != nil {
		return x.Dataset
	}
	return ""
}

func (x *Environment) GetSdkPipelineOptions() *structpb.Struct {
	if x != nil {
		return x.SdkPipelineOptions
	}
	return nil
}

func (x *Environment) GetServiceAccountEmail() string {
	if x != nil {
		return x.ServiceAccountEmail
	}
	return ""
}

func (x *Environment) GetFlexResourceSchedulingGoal() FlexResourceSchedulingGoal {
	if x != nil {
		return x.FlexResourceSchedulingGoal
	}
	return FlexResourceSchedulingGoal_FLEXRS_UNSPECIFIED
}

func (x *Environment) GetWorkerRegion() string {
	if x != nil {
		return x.WorkerRegion
	}
	return ""
}

func (x *Environment) GetWorkerZone() string {
	if x != nil {
		return x.WorkerZone
	}
	return ""
}

func (x *Environment) GetShuffleMode() ShuffleMode {
	if x != nil {
		return x.ShuffleMode
	}
	return ShuffleMode_SHUFFLE_MODE_UNSPECIFIED
}

func (x *Environment) GetDebugOptions() *DebugOptions {
	if x != nil {
		return x.DebugOptions
	}
	return nil
}

// The packages that must be installed in order for a worker to run the
// steps of the Cloud Dataflow job that will be assigned to its worker
// pool.
//
// This is the mechanism by which the Cloud Dataflow SDK causes code to
// be loaded onto the workers. For example, the Cloud Dataflow Java SDK
// might use this to install jars containing the user's code and all of the
// various dependencies (libraries, data files, etc.) required in order
// for that code to run.
type Package struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// The name of the package.
	Name string `protobuf:"bytes,1,opt,name=name,proto3" json:"name,omitempty"`
	// The resource to read the package from. The supported resource type is:
	//
	// Google Cloud Storage:
	//
	//	storage.googleapis.com/{bucket}
	//	bucket.storage.googleapis.com/
	Location      string `protobuf:"bytes,2,opt,name=location,proto3" json:"location,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *Package) Reset() {
	*x = Package{}
	mi := &file_cloud_dataflow_v1beta3_data_proto_msgTypes[1]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *Package) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*Package) ProtoMessage() {}

func (x *Package) ProtoReflect() protoreflect.Message {
	mi := &file_cloud_dataflow_v1beta3_data_proto_msgTypes[1]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use Package.ProtoReflect.Descriptor instead.
func (*Package) Descriptor() ([]byte, []int) {
	return file_cloud_dataflow_v1beta3_data_proto_rawDescGZIP(), []int{1}
}

func (x *Package) GetName() string {
	if x != nil {
		return x.Name
	}
	return ""
}

func (x *Package) GetLocation() string {
	if x != nil {
		return x.Location
	}
	return ""
}

// Settings for WorkerPool autoscaling.
type AutoscalingSettings struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// The algorithm to use for autoscaling.
	Algorithm AutoscalingAlgorithm `protobuf:"varint,1,opt,name=algorithm,proto3,enum=google.events.cloud.dataflow.v1beta3.AutoscalingAlgorithm" json:"algorithm,omitempty"`
	// The maximum number of workers to cap scaling at.
	MaxNumWorkers int32 `protobuf:"varint,2,opt,name=max_num_workers,json=maxNumWorkers,proto3" json:"max_num_workers,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *AutoscalingSettings) Reset() {
	*x = AutoscalingSettings{}
	mi := &file_cloud_dataflow_v1beta3_data_proto_msgTypes[2]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *AutoscalingSettings) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*AutoscalingSettings) ProtoMessage() {}

func (x *AutoscalingSettings) ProtoReflect() protoreflect.Message {
	mi := &file_cloud_dataflow_v1beta3_data_proto_msgTypes[2]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use AutoscalingSettings.ProtoReflect.Descriptor instead.
func (*AutoscalingSettings) Descriptor() ([]byte, []int) {
	return file_cloud_dataflow_v1beta3_data_proto_rawDescGZIP(), []int{2}
}

func (x *AutoscalingSettings) GetAlgorithm() AutoscalingAlgorithm {
	if x != nil {
		return x.Algorithm
	}
	return AutoscalingAlgorithm_AUTOSCALING_ALGORITHM_UNKNOWN
}

func (x *AutoscalingSettings) GetMaxNumWorkers() int32 {
	if x != nil {
		return x.MaxNumWorkers
	}
	return 0
}

// Defines an SDK harness container for executing Dataflow pipelines.
type SdkHarnessContainerImage struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// A docker container image that resides in Google Container Registry.
	ContainerImage string `protobuf:"bytes,1,opt,name=container_image,json=containerImage,proto3" json:"container_image,omitempty"`
	// If true, recommends the Dataflow service to use only one core per SDK
	// container instance with this image. If false (or unset) recommends using
	// more than one core per SDK container instance with this image for
	// efficiency. Note that Dataflow service may choose to override this property
	// if needed.
	UseSingleCorePerContainer bool `protobuf:"varint,2,opt,name=use_single_core_per_container,json=useSingleCorePerContainer,proto3" json:"use_single_core_per_container,omitempty"`
	// Environment ID for the Beam runner API proto Environment that corresponds
	// to the current SDK Harness.
	EnvironmentId string `protobuf:"bytes,3,opt,name=environment_id,json=environmentId,proto3" json:"environment_id,omitempty"`
	// The set of capabilities enumerated in the above Environment proto. See also
	// [beam_runner_api.proto](https://github.com/apache/beam/blob/master/model/pipeline/src/main/proto/org/apache/beam/model/pipeline/v1/beam_runner_api.proto)
	Capabilities  []string `protobuf:"bytes,4,rep,name=capabilities,proto3" json:"capabilities,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *SdkHarnessContainerImage) Reset() {
	*x = SdkHarnessContainerImage{}
	mi := &file_cloud_dataflow_v1beta3_data_proto_msgTypes[3]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *SdkHarnessContainerImage) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*SdkHarnessContainerImage) ProtoMessage() {}

func (x *SdkHarnessContainerImage) ProtoReflect() protoreflect.Message {
	mi := &file_cloud_dataflow_v1beta3_data_proto_msgTypes[3]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use SdkHarnessContainerImage.ProtoReflect.Descriptor instead.
func (*SdkHarnessContainerImage) Descriptor() ([]byte, []int) {
	return file_cloud_dataflow_v1beta3_data_proto_rawDescGZIP(), []int{3}
}

func (x *SdkHarnessContainerImage) GetContainerImage() string {
	if x != nil {
		return x.ContainerImage
	}
	return ""
}

func (x *SdkHarnessContainerImage) GetUseSingleCorePerContainer() bool {
	if x != nil {
		return x.UseSingleCorePerContainer
	}
	return false
}

func (x *SdkHarnessContainerImage) GetEnvironmentId() string {
	if x != nil {
		return x.EnvironmentId
	}
	return ""
}

func (x *SdkHarnessContainerImage) GetCapabilities() []string {
	if x != nil {
		return x.Capabilities
	}
	return nil
}

// Describes one particular pool of Cloud Dataflow workers to be
// instantiated by the Cloud Dataflow service in order to perform the
// computations required by a job.  Note that a workflow job may use
// multiple pools, in order to match the various computational
// requirements of the various stages of the job.
type WorkerPool struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// The kind of the worker pool; currently only `harness` and `shuffle`
	// are supported.
	Kind string `protobuf:"bytes,1,opt,name=kind,proto3" json:"kind,omitempty"`
	// Number of Google Compute Engine workers in this pool needed to
	// execute the job.  If zero or unspecified, the service will
	// attempt to choose a reasonable default.
	NumWorkers int32 `protobuf:"varint,2,opt,name=num_workers,json=numWorkers,proto3" json:"num_workers,omitempty"`
	// Packages to be installed on workers.
	Packages []*Package `protobuf:"bytes,3,rep,name=packages,proto3" json:"packages,omitempty"`
	// The default package set to install.  This allows the service to
	// select a default set of packages which are useful to worker
	// harnesses written in a particular language.
	DefaultPackageSet DefaultPackageSet `protobuf:"varint,4,opt,name=default_package_set,json=defaultPackageSet,proto3,enum=google.events.cloud.dataflow.v1beta3.DefaultPackageSet" json:"default_package_set,omitempty"`
	// Machine type (e.g. "n1-standard-1").  If empty or unspecified, the
	// service will attempt to choose a reasonable default.
	MachineType string `protobuf:"bytes,5,opt,name=machine_type,json=machineType,proto3" json:"machine_type,omitempty"`
	// Sets the policy for determining when to turndown worker pool.
	// Allowed values are: `TEARDOWN_ALWAYS`, `TEARDOWN_ON_SUCCESS`, and
	// `TEARDOWN_NEVER`.
	// `TEARDOWN_ALWAYS` means workers are always torn down regardless of whether
	// the job succeeds. `TEARDOWN_ON_SUCCESS` means workers are torn down
	// if the job succeeds. `TEARDOWN_NEVER` means the workers are never torn
	// down.
	//
	// If the workers are not torn down by the service, they will
	// continue to run and use Google Compute Engine VM resources in the
	// user's project until they are explicitly terminated by the user.
	// Because of this, Google recommends using the `TEARDOWN_ALWAYS`
	// policy except for small, manually supervised test jobs.
	//
	// If unknown or unspecified, the service will attempt to choose a reasonable
	// default.
	TeardownPolicy TeardownPolicy `protobuf:"varint,6,opt,name=teardown_policy,json=teardownPolicy,proto3,enum=google.events.cloud.dataflow.v1beta3.TeardownPolicy" json:"teardown_policy,omitempty"`
	// Size of root disk for VMs, in GB.  If zero or unspecified, the service will
	// attempt to choose a reasonable default.
	DiskSizeGb int32 `protobuf:"varint,7,opt,name=disk_size_gb,json=diskSizeGb,proto3" json:"disk_size_gb,omitempty"`
	// Type of root disk for VMs.  If empty or unspecified, the service will
	// attempt to choose a reasonable default.
	DiskType string `protobuf:"bytes,16,opt,name=disk_type,json=diskType,proto3" json:"disk_type,omitempty"`
	// Fully qualified source image for disks.
	DiskSourceImage string `protobuf:"bytes,8,opt,name=disk_source_image,json=diskSourceImage,proto3" json:"disk_source_image,omitempty"`
	// Zone to run the worker pools in.  If empty or unspecified, the service
	// will attempt to choose a reasonable default.
	Zone string `protobuf:"bytes,9,opt,name=zone,proto3" json:"zone,omitempty"`
	// The action to take on host maintenance, as defined by the Google
	// Compute Engine API.
	OnHostMaintenance string `protobuf:"bytes,11,opt,name=on_host_maintenance,json=onHostMaintenance,proto3" json:"on_host_maintenance,omitempty"`
	// Metadata to set on the Google Compute Engine VMs.
	Metadata map[string]string `protobuf:"bytes,13,rep,name=metadata,proto3" json:"metadata,omitempty" protobuf_key:"bytes,1,opt,name=key" protobuf_val:"bytes,2,opt,name=value"`
	// Settings for autoscaling of this WorkerPool.
	AutoscalingSettings *AutoscalingSettings `protobuf:"bytes,14,opt,name=autoscaling_settings,json=autoscalingSettings,proto3" json:"autoscaling_settings,omitempty"`
	// Network to which VMs will be assigned.  If empty or unspecified,
	// the service will use the network "default".
	Network string `protobuf:"bytes,17,opt,name=network,proto3" json:"network,omitempty"`
	// Subnetwork to which VMs will be assigned, if desired.  Expected to be of
	// the form "regions/REGION/subnetworks/SUBNETWORK".
	Subnetwork string `protobuf:"bytes,19,opt,name=subnetwork,proto3" json:"subnetwork,omitempty"`
	// Required. Docker container image that executes the Cloud Dataflow worker
	// harness, residing in Google Container Registry.
	//
	// Deprecated for the Fn API path. Use sdk_harness_container_images instead.
	WorkerHarnessContainerImage string `protobuf:"bytes,18,opt,name=worker_harness_container_image,json=workerHarnessContainerImage,proto3" json:"worker_harness_container_image,omitempty"`
	// The number of threads per worker harness. If empty or unspecified, the
	// service will choose a number of threads (according to the number of cores
	// on the selected machine type for batch, or 1 by convention for streaming).
	NumThreadsPerWorker int32 `protobuf:"varint,20,opt,name=num_threads_per_worker,json=numThreadsPerWorker,proto3" json:"num_threads_per_worker,omitempty"`
	// Configuration for VM IPs.
	IpConfiguration WorkerIPAddressConfiguration `protobuf:"varint,21,opt,name=ip_configuration,json=ipConfiguration,proto3,enum=google.events.cloud.dataflow.v1beta3.WorkerIPAddressConfiguration" json:"ip_configuration,omitempty"`
	// Set of SDK harness containers needed to execute this pipeline. This will
	// only be set in the Fn API path. For non-cross-language pipelines this
	// should have only one entry. Cross-language pipelines will have two or more
	// entries.
	SdkHarnessContainerImages []*SdkHarnessContainerImage `protobuf:"bytes,22,rep,name=sdk_harness_container_images,json=sdkHarnessContainerImages,proto3" json:"sdk_harness_container_images,omitempty"`
	unknownFields             protoimpl.UnknownFields
	sizeCache                 protoimpl.SizeCache
}

func (x *WorkerPool) Reset() {
	*x = WorkerPool{}
	mi := &file_cloud_dataflow_v1beta3_data_proto_msgTypes[4]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *WorkerPool) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*WorkerPool) ProtoMessage() {}

func (x *WorkerPool) ProtoReflect() protoreflect.Message {
	mi := &file_cloud_dataflow_v1beta3_data_proto_msgTypes[4]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use WorkerPool.ProtoReflect.Descriptor instead.
func (*WorkerPool) Descriptor() ([]byte, []int) {
	return file_cloud_dataflow_v1beta3_data_proto_rawDescGZIP(), []int{4}
}

func (x *WorkerPool) GetKind() string {
	if x != nil {
		return x.Kind
	}
	return ""
}

func (x *WorkerPool) GetNumWorkers() int32 {
	if x != nil {
		return x.NumWorkers
	}
	return 0
}

func (x *WorkerPool) GetPackages() []*Package {
	if x != nil {
		return x.Packages
	}
	return nil
}

func (x *WorkerPool) GetDefaultPackageSet() DefaultPackageSet {
	if x != nil {
		return x.DefaultPackageSet
	}
	return DefaultPackageSet_DEFAULT_PACKAGE_SET_UNKNOWN
}

func (x *WorkerPool) GetMachineType() string {
	if x != nil {
		return x.MachineType
	}
	return ""
}

func (x *WorkerPool) GetTeardownPolicy() TeardownPolicy {
	if x != nil {
		return x.TeardownPolicy
	}
	return TeardownPolicy_TEARDOWN_POLICY_UNKNOWN
}

func (x *WorkerPool) GetDiskSizeGb() int32 {
	if x != nil {
		return x.DiskSizeGb
	}
	return 0
}

func (x *WorkerPool) GetDiskType() string {
	if x != nil {
		return x.DiskType
	}
	return ""
}

func (x *WorkerPool) GetDiskSourceImage() string {
	if x != nil {
		return x.DiskSourceImage
	}
	return ""
}

func (x *WorkerPool) GetZone() string {
	if x != nil {
		return x.Zone
	}
	return ""
}

func (x *WorkerPool) GetOnHostMaintenance() string {
	if x != nil {
		return x.OnHostMaintenance
	}
	return ""
}

func (x *WorkerPool) GetMetadata() map[string]string {
	if x != nil {
		return x.Metadata
	}
	return nil
}

func (x *WorkerPool) GetAutoscalingSettings() *AutoscalingSettings {
	if x != nil {
		return x.AutoscalingSettings
	}
	return nil
}

func (x *WorkerPool) GetNetwork() string {
	if x != nil {
		return x.Network
	}
	return ""
}

func (x *WorkerPool) GetSubnetwork() string {
	if x != nil {
		return x.Subnetwork
	}
	return ""
}

func (x *WorkerPool) GetWorkerHarnessContainerImage() string {
	if x != nil {
		return x.WorkerHarnessContainerImage
	}
	return ""
}

func (x *WorkerPool) GetNumThreadsPerWorker() int32 {
	if x != nil {
		return x.NumThreadsPerWorker
	}
	return 0
}

func (x *WorkerPool) GetIpConfiguration() WorkerIPAddressConfiguration {
	if x != nil {
		return x.IpConfiguration
	}
	return WorkerIPAddressConfiguration_WORKER_IP_UNSPECIFIED
}

func (x *WorkerPool) GetSdkHarnessContainerImages() []*SdkHarnessContainerImage {
	if x != nil {
		return x.SdkHarnessContainerImages
	}
	return nil
}

// Describes any options that have an effect on the debugging of pipelines.
type DebugOptions struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// When true, enables the logging of the literal hot key to the user's Cloud
	// Logging.
	EnableHotKeyLogging bool `protobuf:"varint,1,opt,name=enable_hot_key_logging,json=enableHotKeyLogging,proto3" json:"enable_hot_key_logging,omitempty"`
	unknownFields       protoimpl.UnknownFields
	sizeCache           protoimpl.SizeCache
}

func (x *DebugOptions) Reset() {
	*x = DebugOptions{}
	mi := &file_cloud_dataflow_v1beta3_data_proto_msgTypes[5]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *DebugOptions) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*DebugOptions) ProtoMessage() {}

func (x *DebugOptions) ProtoReflect() protoreflect.Message {
	mi := &file_cloud_dataflow_v1beta3_data_proto_msgTypes[5]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use DebugOptions.ProtoReflect.Descriptor instead.
func (*DebugOptions) Descriptor() ([]byte, []int) {
	return file_cloud_dataflow_v1beta3_data_proto_rawDescGZIP(), []int{5}
}

func (x *DebugOptions) GetEnableHotKeyLogging() bool {
	if x != nil {
		return x.EnableHotKeyLogging
	}
	return false
}

// Defines a job to be run by the Cloud Dataflow service. Do not enter
// confidential information when you supply string values using the API.
// Fields stripped from source Job proto:
// - steps
// - pipeline_description
// - transform_name_mapping
type Job struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// The unique ID of this job.
	//
	// This field is set by the Cloud Dataflow service when the Job is
	// created, and is immutable for the life of the job.
	Id string `protobuf:"bytes,1,opt,name=id,proto3" json:"id,omitempty"`
	// The ID of the Cloud Platform project that the job belongs to.
	ProjectId string `protobuf:"bytes,2,opt,name=project_id,json=projectId,proto3" json:"project_id,omitempty"`
	// The user-specified Cloud Dataflow job name.
	//
	// Only one Job with a given name can exist in a project within one region at
	// any given time. Jobs in different regions can have the same name.
	// If a caller attempts to create a Job with the same
	// name as an already-existing Job, the attempt returns the
	// existing Job.
	//
	// The name must match the regular expression
	// `[a-z]([-a-z0-9]{0,1022}[a-z0-9])?`
	Name string `protobuf:"bytes,3,opt,name=name,proto3" json:"name,omitempty"`
	// The type of Cloud Dataflow job.
	Type JobType `protobuf:"varint,4,opt,name=type,proto3,enum=google.events.cloud.dataflow.v1beta3.JobType" json:"type,omitempty"`
	// The environment for the job.
	Environment *Environment `protobuf:"bytes,5,opt,name=environment,proto3" json:"environment,omitempty"`
	// The Cloud Storage location where the steps are stored.
	StepsLocation string `protobuf:"bytes,24,opt,name=steps_location,json=stepsLocation,proto3" json:"steps_location,omitempty"`
	// The current state of the job.
	//
	// Jobs are created in the `JOB_STATE_STOPPED` state unless otherwise
	// specified.
	//
	// A job in the `JOB_STATE_RUNNING` state may asynchronously enter a
	// terminal state. After a job has reached a terminal state, no
	// further state updates may be made.
	//
	// This field may be mutated by the Cloud Dataflow service;
	// callers cannot mutate it.
	CurrentState JobState `protobuf:"varint,7,opt,name=current_state,json=currentState,proto3,enum=google.events.cloud.dataflow.v1beta3.JobState" json:"current_state,omitempty"`
	// The timestamp associated with the current state.
	CurrentStateTime *timestamppb.Timestamp `protobuf:"bytes,8,opt,name=current_state_time,json=currentStateTime,proto3" json:"current_state_time,omitempty"`
	// The job's requested state.
	//
	// `UpdateJob` may be used to switch between the `JOB_STATE_STOPPED` and
	// `JOB_STATE_RUNNING` states, by setting requested_state.  `UpdateJob` may
	// also be used to directly set a job's requested state to
	// `JOB_STATE_CANCELLED` or `JOB_STATE_DONE`, irrevocably terminating the
	// job if it has not already reached a terminal state.
	RequestedState JobState `protobuf:"varint,9,opt,name=requested_state,json=requestedState,proto3,enum=google.events.cloud.dataflow.v1beta3.JobState" json:"requested_state,omitempty"`
	// Deprecated.
	ExecutionInfo *JobExecutionInfo `protobuf:"bytes,10,opt,name=execution_info,json=executionInfo,proto3" json:"execution_info,omitempty"`
	// The timestamp when the job was initially created. Immutable and set by the
	// Cloud Dataflow service.
	CreateTime *timestamppb.Timestamp `protobuf:"bytes,11,opt,name=create_time,json=createTime,proto3" json:"create_time,omitempty"`
	// If this job is an update of an existing job, this field is the job ID
	// of the job it replaced.
	//
	// When sending a `CreateJobRequest`, you can update a job by specifying it
	// here. The job named here is stopped, and its intermediate state is
	// transferred to this job.
	ReplaceJobId string `protobuf:"bytes,12,opt,name=replace_job_id,json=replaceJobId,proto3" json:"replace_job_id,omitempty"`
	// The client's unique identifier of the job, re-used across retried attempts.
	// If this field is set, the service will ensure its uniqueness.
	// The request to create a job will fail if the service has knowledge of a
	// previously submitted job with the same client's ID and job name.
	// The caller may use this field to ensure idempotence of job
	// creation across retried attempts to create a job.
	// By default, the field is empty and, in that case, the service ignores it.
	ClientRequestId string `protobuf:"bytes,14,opt,name=client_request_id,json=clientRequestId,proto3" json:"client_request_id,omitempty"`
	// If another job is an update of this job (and thus, this job is in
	// `JOB_STATE_UPDATED`), this field contains the ID of that job.
	ReplacedByJobId string `protobuf:"bytes,15,opt,name=replaced_by_job_id,json=replacedByJobId,proto3" json:"replaced_by_job_id,omitempty"`
	// A set of files the system should be aware of that are used
	// for temporary storage. These temporary files will be
	// removed on job completion.
	// No duplicates are allowed.
	// No file patterns are supported.
	//
	// The supported files are:
	//
	// Google Cloud Storage:
	//
	//	storage.googleapis.com/{bucket}/{object}
	//	bucket.storage.googleapis.com/{object}
	TempFiles []string `protobuf:"bytes,16,rep,name=temp_files,json=tempFiles,proto3" json:"temp_files,omitempty"`
	// User-defined labels for this job.
	//
	// The labels map can contain no more than 64 entries.  Entries of the labels
	// map are UTF8 strings that comply with the following restrictions:
	//
	// * Keys must conform to regexp:  [\p{Ll}\p{Lo}][\p{Ll}\p{Lo}\p{N}_-]{0,62}
	// * Values must conform to regexp:  [\p{Ll}\p{Lo}\p{N}_-]{0,63}
	// * Both keys and values are additionally constrained to be <= 128 bytes in
	// size.
	Labels map[string]string `protobuf:"bytes,17,rep,name=labels,proto3" json:"labels,omitempty" protobuf_key:"bytes,1,opt,name=key" protobuf_val:"bytes,2,opt,name=value"`
	// The [regional endpoint]
	// (https://cloud.google.com/dataflow/docs/concepts/regional-endpoints) that
	// contains this job.
	Location string `protobuf:"bytes,18,opt,name=location,proto3" json:"location,omitempty"`
	// This field may be mutated by the Cloud Dataflow service;
	// callers cannot mutate it.
	StageStates []*ExecutionStageState `protobuf:"bytes,20,rep,name=stage_states,json=stageStates,proto3" json:"stage_states,omitempty"`
	// This field is populated by the Dataflow service to support filtering jobs
	// by the metadata values provided here. Populated for ListJobs and all GetJob
	// views SUMMARY and higher.
	JobMetadata *JobMetadata `protobuf:"bytes,21,opt,name=job_metadata,json=jobMetadata,proto3" json:"job_metadata,omitempty"`
	// The timestamp when the job was started (transitioned to JOB_STATE_PENDING).
	// Flexible resource scheduling jobs are started with some delay after job
	// creation, so start_time is unset before start and is updated when the
	// job is started by the Cloud Dataflow service. For other jobs, start_time
	// always equals to create_time and is immutable and set by the Cloud Dataflow
	// service.
	StartTime *timestamppb.Timestamp `protobuf:"bytes,22,opt,name=start_time,json=startTime,proto3" json:"start_time,omitempty"`
	// If this is specified, the job's initial state is populated from the given
	// snapshot.
	CreatedFromSnapshotId string `protobuf:"bytes,23,opt,name=created_from_snapshot_id,json=createdFromSnapshotId,proto3" json:"created_from_snapshot_id,omitempty"`
	// Reserved for future use. This field is set only in responses from the
	// server; it is ignored if it is set in any requests.
	SatisfiesPzs  bool `protobuf:"varint,25,opt,name=satisfies_pzs,json=satisfiesPzs,proto3" json:"satisfies_pzs,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *Job) Reset() {
	*x = Job{}
	mi := &file_cloud_dataflow_v1beta3_data_proto_msgTypes[6]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *Job) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*Job) ProtoMessage() {}

func (x *Job) ProtoReflect() protoreflect.Message {
	mi := &file_cloud_dataflow_v1beta3_data_proto_msgTypes[6]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use Job.ProtoReflect.Descriptor instead.
func (*Job) Descriptor() ([]byte, []int) {
	return file_cloud_dataflow_v1beta3_data_proto_rawDescGZIP(), []int{6}
}

func (x *Job) GetId() string {
	if x != nil {
		return x.Id
	}
	return ""
}

func (x *Job) GetProjectId() string {
	if x != nil {
		return x.ProjectId
	}
	return ""
}

func (x *Job) GetName() string {
	if x != nil {
		return x.Name
	}
	return ""
}

func (x *Job) GetType() JobType {
	if x != nil {
		return x.Type
	}
	return JobType_JOB_TYPE_UNKNOWN
}

func (x *Job) GetEnvironment() *Environment {
	if x != nil {
		return x.Environment
	}
	return nil
}

func (x *Job) GetStepsLocation() string {
	if x != nil {
		return x.StepsLocation
	}
	return ""
}

func (x *Job) GetCurrentState() JobState {
	if x != nil {
		return x.CurrentState
	}
	return JobState_JOB_STATE_UNKNOWN
}

func (x *Job) GetCurrentStateTime() *timestamppb.Timestamp {
	if x != nil {
		return x.CurrentStateTime
	}
	return nil
}

func (x *Job) GetRequestedState() JobState {
	if x != nil {
		return x.RequestedState
	}
	return JobState_JOB_STATE_UNKNOWN
}

func (x *Job) GetExecutionInfo() *JobExecutionInfo {
	if x != nil {
		return x.ExecutionInfo
	}
	return nil
}

func (x *Job) GetCreateTime() *timestamppb.Timestamp {
	if x != nil {
		return x.CreateTime
	}
	return nil
}

func (x *Job) GetReplaceJobId() string {
	if x != nil {
		return x.ReplaceJobId
	}
	return ""
}

func (x *Job) GetClientRequestId() string {
	if x != nil {
		return x.ClientRequestId
	}
	return ""
}

func (x *Job) GetReplacedByJobId() string {
	if x != nil {
		return x.ReplacedByJobId
	}
	return ""
}

func (x *Job) GetTempFiles() []string {
	if x != nil {
		return x.TempFiles
	}
	return nil
}

func (x *Job) GetLabels() map[string]string {
	if x != nil {
		return x.Labels
	}
	return nil
}

func (x *Job) GetLocation() string {
	if x != nil {
		return x.Location
	}
	return ""
}

func (x *Job) GetStageStates() []*ExecutionStageState {
	if x != nil {
		return x.StageStates
	}
	return nil
}

func (x *Job) GetJobMetadata() *JobMetadata {
	if x != nil {
		return x.JobMetadata
	}
	return nil
}

func (x *Job) GetStartTime() *timestamppb.Timestamp {
	if x != nil {
		return x.StartTime
	}
	return nil
}

func (x *Job) GetCreatedFromSnapshotId() string {
	if x != nil {
		return x.CreatedFromSnapshotId
	}
	return ""
}

func (x *Job) GetSatisfiesPzs() bool {
	if x != nil {
		return x.SatisfiesPzs
	}
	return false
}

// Metadata for a Datastore connector used by the job.
type DatastoreIODetails struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Namespace used in the connection.
	Namespace string `protobuf:"bytes,1,opt,name=namespace,proto3" json:"namespace,omitempty"`
	// ProjectId accessed in the connection.
	ProjectId     string `protobuf:"bytes,2,opt,name=project_id,json=projectId,proto3" json:"project_id,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *DatastoreIODetails) Reset() {
	*x = DatastoreIODetails{}
	mi := &file_cloud_dataflow_v1beta3_data_proto_msgTypes[7]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *DatastoreIODetails) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*DatastoreIODetails) ProtoMessage() {}

func (x *DatastoreIODetails) ProtoReflect() protoreflect.Message {
	mi := &file_cloud_dataflow_v1beta3_data_proto_msgTypes[7]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use DatastoreIODetails.ProtoReflect.Descriptor instead.
func (*DatastoreIODetails) Descriptor() ([]byte, []int) {
	return file_cloud_dataflow_v1beta3_data_proto_rawDescGZIP(), []int{7}
}

func (x *DatastoreIODetails) GetNamespace() string {
	if x != nil {
		return x.Namespace
	}
	return ""
}

func (x *DatastoreIODetails) GetProjectId() string {
	if x != nil {
		return x.ProjectId
	}
	return ""
}

// Metadata for a Pub/Sub connector used by the job.
type PubSubIODetails struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Topic accessed in the connection.
	Topic string `protobuf:"bytes,1,opt,name=topic,proto3" json:"topic,omitempty"`
	// Subscription used in the connection.
	Subscription  string `protobuf:"bytes,2,opt,name=subscription,proto3" json:"subscription,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *PubSubIODetails) Reset() {
	*x = PubSubIODetails{}
	mi := &file_cloud_dataflow_v1beta3_data_proto_msgTypes[8]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *PubSubIODetails) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*PubSubIODetails) ProtoMessage() {}

func (x *PubSubIODetails) ProtoReflect() protoreflect.Message {
	mi := &file_cloud_dataflow_v1beta3_data_proto_msgTypes[8]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use PubSubIODetails.ProtoReflect.Descriptor instead.
func (*PubSubIODetails) Descriptor() ([]byte, []int) {
	return file_cloud_dataflow_v1beta3_data_proto_rawDescGZIP(), []int{8}
}

func (x *PubSubIODetails) GetTopic() string {
	if x != nil {
		return x.Topic
	}
	return ""
}

func (x *PubSubIODetails) GetSubscription() string {
	if x != nil {
		return x.Subscription
	}
	return ""
}

// Metadata for a File connector used by the job.
type FileIODetails struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// File Pattern used to access files by the connector.
	FilePattern   string `protobuf:"bytes,1,opt,name=file_pattern,json=filePattern,proto3" json:"file_pattern,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *FileIODetails) Reset() {
	*x = FileIODetails{}
	mi := &file_cloud_dataflow_v1beta3_data_proto_msgTypes[9]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *FileIODetails) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*FileIODetails) ProtoMessage() {}

func (x *FileIODetails) ProtoReflect() protoreflect.Message {
	mi := &file_cloud_dataflow_v1beta3_data_proto_msgTypes[9]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use FileIODetails.ProtoReflect.Descriptor instead.
func (*FileIODetails) Descriptor() ([]byte, []int) {
	return file_cloud_dataflow_v1beta3_data_proto_rawDescGZIP(), []int{9}
}

func (x *FileIODetails) GetFilePattern() string {
	if x != nil {
		return x.FilePattern
	}
	return ""
}

// Metadata for a Cloud Bigtable connector used by the job.
type BigTableIODetails struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// ProjectId accessed in the connection.
	ProjectId string `protobuf:"bytes,1,opt,name=project_id,json=projectId,proto3" json:"project_id,omitempty"`
	// InstanceId accessed in the connection.
	InstanceId string `protobuf:"bytes,2,opt,name=instance_id,json=instanceId,proto3" json:"instance_id,omitempty"`
	// TableId accessed in the connection.
	TableId       string `protobuf:"bytes,3,opt,name=table_id,json=tableId,proto3" json:"table_id,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *BigTableIODetails) Reset() {
	*x = BigTableIODetails{}
	mi := &file_cloud_dataflow_v1beta3_data_proto_msgTypes[10]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *BigTableIODetails) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*BigTableIODetails) ProtoMessage() {}

func (x *BigTableIODetails) ProtoReflect() protoreflect.Message {
	mi := &file_cloud_dataflow_v1beta3_data_proto_msgTypes[10]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use BigTableIODetails.ProtoReflect.Descriptor instead.
func (*BigTableIODetails) Descriptor() ([]byte, []int) {
	return file_cloud_dataflow_v1beta3_data_proto_rawDescGZIP(), []int{10}
}

func (x *BigTableIODetails) GetProjectId() string {
	if x != nil {
		return x.ProjectId
	}
	return ""
}

func (x *BigTableIODetails) GetInstanceId() string {
	if x != nil {
		return x.InstanceId
	}
	return ""
}

func (x *BigTableIODetails) GetTableId() string {
	if x != nil {
		return x.TableId
	}
	return ""
}

// Metadata for a BigQuery connector used by the job.
type BigQueryIODetails struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Table accessed in the connection.
	Table string `protobuf:"bytes,1,opt,name=table,proto3" json:"table,omitempty"`
	// Dataset accessed in the connection.
	Dataset string `protobuf:"bytes,2,opt,name=dataset,proto3" json:"dataset,omitempty"`
	// Project accessed in the connection.
	ProjectId string `protobuf:"bytes,3,opt,name=project_id,json=projectId,proto3" json:"project_id,omitempty"`
	// Query used to access data in the connection.
	Query         string `protobuf:"bytes,4,opt,name=query,proto3" json:"query,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *BigQueryIODetails) Reset() {
	*x = BigQueryIODetails{}
	mi := &file_cloud_dataflow_v1beta3_data_proto_msgTypes[11]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *BigQueryIODetails) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*BigQueryIODetails) ProtoMessage() {}

func (x *BigQueryIODetails) ProtoReflect() protoreflect.Message {
	mi := &file_cloud_dataflow_v1beta3_data_proto_msgTypes[11]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use BigQueryIODetails.ProtoReflect.Descriptor instead.
func (*BigQueryIODetails) Descriptor() ([]byte, []int) {
	return file_cloud_dataflow_v1beta3_data_proto_rawDescGZIP(), []int{11}
}

func (x *BigQueryIODetails) GetTable() string {
	if x != nil {
		return x.Table
	}
	return ""
}

func (x *BigQueryIODetails) GetDataset() string {
	if x != nil {
		return x.Dataset
	}
	return ""
}

func (x *BigQueryIODetails) GetProjectId() string {
	if x != nil {
		return x.ProjectId
	}
	return ""
}

func (x *BigQueryIODetails) GetQuery() string {
	if x != nil {
		return x.Query
	}
	return ""
}

// Metadata for a Spanner connector used by the job.
type SpannerIODetails struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// ProjectId accessed in the connection.
	ProjectId string `protobuf:"bytes,1,opt,name=project_id,json=projectId,proto3" json:"project_id,omitempty"`
	// InstanceId accessed in the connection.
	InstanceId string `protobuf:"bytes,2,opt,name=instance_id,json=instanceId,proto3" json:"instance_id,omitempty"`
	// DatabaseId accessed in the connection.
	DatabaseId    string `protobuf:"bytes,3,opt,name=database_id,json=databaseId,proto3" json:"database_id,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *SpannerIODetails) Reset() {
	*x = SpannerIODetails{}
	mi := &file_cloud_dataflow_v1beta3_data_proto_msgTypes[12]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *SpannerIODetails) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*SpannerIODetails) ProtoMessage() {}

func (x *SpannerIODetails) ProtoReflect() protoreflect.Message {
	mi := &file_cloud_dataflow_v1beta3_data_proto_msgTypes[12]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use SpannerIODetails.ProtoReflect.Descriptor instead.
func (*SpannerIODetails) Descriptor() ([]byte, []int) {
	return file_cloud_dataflow_v1beta3_data_proto_rawDescGZIP(), []int{12}
}

func (x *SpannerIODetails) GetProjectId() string {
	if x != nil {
		return x.ProjectId
	}
	return ""
}

func (x *SpannerIODetails) GetInstanceId() string {
	if x != nil {
		return x.InstanceId
	}
	return ""
}

func (x *SpannerIODetails) GetDatabaseId() string {
	if x != nil {
		return x.DatabaseId
	}
	return ""
}

// The version of the SDK used to run the job.
type SdkVersion struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// The version of the SDK used to run the job.
	Version string `protobuf:"bytes,1,opt,name=version,proto3" json:"version,omitempty"`
	// A readable string describing the version of the SDK.
	VersionDisplayName string `protobuf:"bytes,2,opt,name=version_display_name,json=versionDisplayName,proto3" json:"version_display_name,omitempty"`
	// The support status for this SDK version.
	SdkSupportStatus SdkVersion_SdkSupportStatus `protobuf:"varint,3,opt,name=sdk_support_status,json=sdkSupportStatus,proto3,enum=google.events.cloud.dataflow.v1beta3.SdkVersion_SdkSupportStatus" json:"sdk_support_status,omitempty"`
	unknownFields    protoimpl.UnknownFields
	sizeCache        protoimpl.SizeCache
}

func (x *SdkVersion) Reset() {
	*x = SdkVersion{}
	mi := &file_cloud_dataflow_v1beta3_data_proto_msgTypes[13]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *SdkVersion) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*SdkVersion) ProtoMessage() {}

func (x *SdkVersion) ProtoReflect() protoreflect.Message {
	mi := &file_cloud_dataflow_v1beta3_data_proto_msgTypes[13]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use SdkVersion.ProtoReflect.Descriptor instead.
func (*SdkVersion) Descriptor() ([]byte, []int) {
	return file_cloud_dataflow_v1beta3_data_proto_rawDescGZIP(), []int{13}
}

func (x *SdkVersion) GetVersion() string {
	if x != nil {
		return x.Version
	}
	return ""
}

func (x *SdkVersion) GetVersionDisplayName() string {
	if x != nil {
		return x.VersionDisplayName
	}
	return ""
}

func (x *SdkVersion) GetSdkSupportStatus() SdkVersion_SdkSupportStatus {
	if x != nil {
		return x.SdkSupportStatus
	}
	return SdkVersion_UNKNOWN
}

// Metadata available primarily for filtering jobs. Will be included in the
// ListJob response and Job SUMMARY view.
type JobMetadata struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// The SDK version used to run the job.
	SdkVersion *SdkVersion `protobuf:"bytes,1,opt,name=sdk_version,json=sdkVersion,proto3" json:"sdk_version,omitempty"`
	// Identification of a Spanner source used in the Dataflow job.
	SpannerDetails []*SpannerIODetails `protobuf:"bytes,2,rep,name=spanner_details,json=spannerDetails,proto3" json:"spanner_details,omitempty"`
	// Identification of a BigQuery source used in the Dataflow job.
	BigqueryDetails []*BigQueryIODetails `protobuf:"bytes,3,rep,name=bigquery_details,json=bigqueryDetails,proto3" json:"bigquery_details,omitempty"`
	// Identification of a Cloud Bigtable source used in the Dataflow job.
	BigTableDetails []*BigTableIODetails `protobuf:"bytes,4,rep,name=big_table_details,json=bigTableDetails,proto3" json:"big_table_details,omitempty"`
	// Identification of a Pub/Sub source used in the Dataflow job.
	PubsubDetails []*PubSubIODetails `protobuf:"bytes,5,rep,name=pubsub_details,json=pubsubDetails,proto3" json:"pubsub_details,omitempty"`
	// Identification of a File source used in the Dataflow job.
	FileDetails []*FileIODetails `protobuf:"bytes,6,rep,name=file_details,json=fileDetails,proto3" json:"file_details,omitempty"`
	// Identification of a Datastore source used in the Dataflow job.
	DatastoreDetails []*DatastoreIODetails `protobuf:"bytes,7,rep,name=datastore_details,json=datastoreDetails,proto3" json:"datastore_details,omitempty"`
	unknownFields    protoimpl.UnknownFields
	sizeCache        protoimpl.SizeCache
}

func (x *JobMetadata) Reset() {
	*x = JobMetadata{}
	mi := &file_cloud_dataflow_v1beta3_data_proto_msgTypes[14]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *JobMetadata) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*JobMetadata) ProtoMessage() {}

func (x *JobMetadata) ProtoReflect() protoreflect.Message {
	mi := &file_cloud_dataflow_v1beta3_data_proto_msgTypes[14]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use JobMetadata.ProtoReflect.Descriptor instead.
func (*JobMetadata) Descriptor() ([]byte, []int) {
	return file_cloud_dataflow_v1beta3_data_proto_rawDescGZIP(), []int{14}
}

func (x *JobMetadata) GetSdkVersion() *SdkVersion {
	if x != nil {
		return x.SdkVersion
	}
	return nil
}

func (x *JobMetadata) GetSpannerDetails() []*SpannerIODetails {
	if x != nil {
		return x.SpannerDetails
	}
	return nil
}

func (x *JobMetadata) GetBigqueryDetails() []*BigQueryIODetails {
	if x != nil {
		return x.BigqueryDetails
	}
	return nil
}

func (x *JobMetadata) GetBigTableDetails() []*BigTableIODetails {
	if x != nil {
		return x.BigTableDetails
	}
	return nil
}

func (x *JobMetadata) GetPubsubDetails() []*PubSubIODetails {
	if x != nil {
		return x.PubsubDetails
	}
	return nil
}

func (x *JobMetadata) GetFileDetails() []*FileIODetails {
	if x != nil {
		return x.FileDetails
	}
	return nil
}

func (x *JobMetadata) GetDatastoreDetails() []*DatastoreIODetails {
	if x != nil {
		return x.DatastoreDetails
	}
	return nil
}

// A message describing the state of a particular execution stage.
type ExecutionStageState struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// The name of the execution stage.
	ExecutionStageName string `protobuf:"bytes,1,opt,name=execution_stage_name,json=executionStageName,proto3" json:"execution_stage_name,omitempty"`
	// Executions stage states allow the same set of values as JobState.
	ExecutionStageState JobState `protobuf:"varint,2,opt,name=execution_stage_state,json=executionStageState,proto3,enum=google.events.cloud.dataflow.v1beta3.JobState" json:"execution_stage_state,omitempty"`
	// The time at which the stage transitioned to this state.
	CurrentStateTime *timestamppb.Timestamp `protobuf:"bytes,3,opt,name=current_state_time,json=currentStateTime,proto3" json:"current_state_time,omitempty"`
	unknownFields    protoimpl.UnknownFields
	sizeCache        protoimpl.SizeCache
}

func (x *ExecutionStageState) Reset() {
	*x = ExecutionStageState{}
	mi := &file_cloud_dataflow_v1beta3_data_proto_msgTypes[15]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *ExecutionStageState) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*ExecutionStageState) ProtoMessage() {}

func (x *ExecutionStageState) ProtoReflect() protoreflect.Message {
	mi := &file_cloud_dataflow_v1beta3_data_proto_msgTypes[15]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use ExecutionStageState.ProtoReflect.Descriptor instead.
func (*ExecutionStageState) Descriptor() ([]byte, []int) {
	return file_cloud_dataflow_v1beta3_data_proto_rawDescGZIP(), []int{15}
}

func (x *ExecutionStageState) GetExecutionStageName() string {
	if x != nil {
		return x.ExecutionStageName
	}
	return ""
}

func (x *ExecutionStageState) GetExecutionStageState() JobState {
	if x != nil {
		return x.ExecutionStageState
	}
	return JobState_JOB_STATE_UNKNOWN
}

func (x *ExecutionStageState) GetCurrentStateTime() *timestamppb.Timestamp {
	if x != nil {
		return x.CurrentStateTime
	}
	return nil
}

// Additional information about how a Cloud Dataflow job will be executed that
// isn't contained in the submitted job.
type JobExecutionInfo struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// A mapping from each stage to the information about that stage.
	Stages        map[string]*JobExecutionStageInfo `protobuf:"bytes,1,rep,name=stages,proto3" json:"stages,omitempty" protobuf_key:"bytes,1,opt,name=key" protobuf_val:"bytes,2,opt,name=value"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *JobExecutionInfo) Reset() {
	*x = JobExecutionInfo{}
	mi := &file_cloud_dataflow_v1beta3_data_proto_msgTypes[16]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *JobExecutionInfo) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*JobExecutionInfo) ProtoMessage() {}

func (x *JobExecutionInfo) ProtoReflect() protoreflect.Message {
	mi := &file_cloud_dataflow_v1beta3_data_proto_msgTypes[16]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use JobExecutionInfo.ProtoReflect.Descriptor instead.
func (*JobExecutionInfo) Descriptor() ([]byte, []int) {
	return file_cloud_dataflow_v1beta3_data_proto_rawDescGZIP(), []int{16}
}

func (x *JobExecutionInfo) GetStages() map[string]*JobExecutionStageInfo {
	if x != nil {
		return x.Stages
	}
	return nil
}

// Contains information about how a particular
// [google.dataflow.v1beta3.Step][google.dataflow.v1beta3.Step] will be
// executed.
type JobExecutionStageInfo struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// The steps associated with the execution stage.
	// Note that stages may have several steps, and that a given step
	// might be run by more than one stage.
	StepName      []string `protobuf:"bytes,1,rep,name=step_name,json=stepName,proto3" json:"step_name,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *JobExecutionStageInfo) Reset() {
	*x = JobExecutionStageInfo{}
	mi := &file_cloud_dataflow_v1beta3_data_proto_msgTypes[17]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *JobExecutionStageInfo) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*JobExecutionStageInfo) ProtoMessage() {}

func (x *JobExecutionStageInfo) ProtoReflect() protoreflect.Message {
	mi := &file_cloud_dataflow_v1beta3_data_proto_msgTypes[17]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use JobExecutionStageInfo.ProtoReflect.Descriptor instead.
func (*JobExecutionStageInfo) Descriptor() ([]byte, []int) {
	return file_cloud_dataflow_v1beta3_data_proto_rawDescGZIP(), []int{17}
}

func (x *JobExecutionStageInfo) GetStepName() []string {
	if x != nil {
		return x.StepName
	}
	return nil
}

// The data within all Job events.
type JobEventData struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// The Job event payload.
	Payload       *Job `protobuf:"bytes,1,opt,name=payload,proto3" json:"payload,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *JobEventData) Reset() {
	*x = JobEventData{}
	mi := &file_cloud_dataflow_v1beta3_data_proto_msgTypes[18]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *JobEventData) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*JobEventData) ProtoMessage() {}

func (x *JobEventData) ProtoReflect() protoreflect.Message {
	mi := &file_cloud_dataflow_v1beta3_data_proto_msgTypes[18]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use JobEventData.ProtoReflect.Descriptor instead.
func (*JobEventData) Descriptor() ([]byte, []int) {
	return file_cloud_dataflow_v1beta3_data_proto_rawDescGZIP(), []int{18}
}

func (x *JobEventData) GetPayload() *Job {
	if x != nil {
		return x.Payload
	}
	return nil
}

var File_cloud_dataflow_v1beta3_data_proto protoreflect.FileDescriptor

const file_cloud_dataflow_v1beta3_data_proto_rawDesc = "" +
	"\n" +
	"!cloud/dataflow/v1beta3/data.proto\x12$google.events.cloud.dataflow.v1beta3\x1a\x1cgoogle/protobuf/struct.proto\x1a\x1fgoogle/protobuf/timestamp.proto\"\xcc\a\n" +
	"\vEnvironment\x12.\n" +
	"\x13temp_storage_prefix\x18\x01 \x01(\tR\x11tempStoragePrefix\x12=\n" +
	"\x1bcluster_manager_api_service\x18\x02 \x01(\tR\x18clusterManagerApiService\x12 \n" +
	"\vexperiments\x18\x03 \x03(\tR\vexperiments\x12'\n" +
	"\x0fservice_options\x18\x10 \x03(\tR\x0eserviceOptions\x12/\n" +
	"\x14service_kms_key_name\x18\f \x01(\tR\x11serviceKmsKeyName\x12S\n" +
	"\fworker_pools\x18\x04 \x03(\v20.google.events.cloud.dataflow.v1beta3.WorkerPoolR\vworkerPools\x126\n" +
	"\n" +
	"user_agent\x18\x05 \x01(\v2\x17.google.protobuf.StructR\tuserAgent\x121\n" +
	"\aversion\x18\x06 \x01(\v2\x17.google.protobuf.StructR\aversion\x12\x18\n" +
	"\adataset\x18\a \x01(\tR\adataset\x12I\n" +
	"\x14sdk_pipeline_options\x18\b \x01(\v2\x17.google.protobuf.StructR\x12sdkPipelineOptions\x122\n" +
	"\x15service_account_email\x18\n" +
	" \x01(\tR\x13serviceAccountEmail\x12\x83\x01\n" +
	"\x1dflex_resource_scheduling_goal\x18\v \x01(\x0e2@.google.events.cloud.dataflow.v1beta3.FlexResourceSchedulingGoalR\x1aflexResourceSchedulingGoal\x12#\n" +
	"\rworker_region\x18\r \x01(\tR\fworkerRegion\x12\x1f\n" +
	"\vworker_zone\x18\x0e \x01(\tR\n" +
	"workerZone\x12T\n" +
	"\fshuffle_mode\x18\x0f \x01(\x0e21.google.events.cloud.dataflow.v1beta3.ShuffleModeR\vshuffleMode\x12W\n" +
	"\rdebug_options\x18\x11 \x01(\v22.google.events.cloud.dataflow.v1beta3.DebugOptionsR\fdebugOptions\"9\n" +
	"\aPackage\x12\x12\n" +
	"\x04name\x18\x01 \x01(\tR\x04name\x12\x1a\n" +
	"\blocation\x18\x02 \x01(\tR\blocation\"\x97\x01\n" +
	"\x13AutoscalingSettings\x12X\n" +
	"\talgorithm\x18\x01 \x01(\x0e2:.google.events.cloud.dataflow.v1beta3.AutoscalingAlgorithmR\talgorithm\x12&\n" +
	"\x0fmax_num_workers\x18\x02 \x01(\x05R\rmaxNumWorkers\"\xd0\x01\n" +
	"\x18SdkHarnessContainerImage\x12'\n" +
	"\x0fcontainer_image\x18\x01 \x01(\tR\x0econtainerImage\x12@\n" +
	"\x1duse_single_core_per_container\x18\x02 \x01(\bR\x19useSingleCorePerContainer\x12%\n" +
	"\x0eenvironment_id\x18\x03 \x01(\tR\renvironmentId\x12\"\n" +
	"\fcapabilities\x18\x04 \x03(\tR\fcapabilities\"\xd1\t\n" +
	"\n" +
	"WorkerPool\x12\x12\n" +
	"\x04kind\x18\x01 \x01(\tR\x04kind\x12\x1f\n" +
	"\vnum_workers\x18\x02 \x01(\x05R\n" +
	"numWorkers\x12I\n" +
	"\bpackages\x18\x03 \x03(\v2-.google.events.cloud.dataflow.v1beta3.PackageR\bpackages\x12g\n" +
	"\x13default_package_set\x18\x04 \x01(\x0e27.google.events.cloud.dataflow.v1beta3.DefaultPackageSetR\x11defaultPackageSet\x12!\n" +
	"\fmachine_type\x18\x05 \x01(\tR\vmachineType\x12]\n" +
	"\x0fteardown_policy\x18\x06 \x01(\x0e24.google.events.cloud.dataflow.v1beta3.TeardownPolicyR\x0eteardownPolicy\x12 \n" +
	"\fdisk_size_gb\x18\a \x01(\x05R\n" +
	"diskSizeGb\x12\x1b\n" +
	"\tdisk_type\x18\x10 \x01(\tR\bdiskType\x12*\n" +
	"\x11disk_source_image\x18\b \x01(\tR\x0fdiskSourceImage\x12\x12\n" +
	"\x04zone\x18\t \x01(\tR\x04zone\x12.\n" +
	"\x13on_host_maintenance\x18\v \x01(\tR\x11onHostMaintenance\x12Z\n" +
	"\bmetadata\x18\r \x03(\v2>.google.events.cloud.dataflow.v1beta3.WorkerPool.MetadataEntryR\bmetadata\x12l\n" +
	"\x14autoscaling_settings\x18\x0e \x01(\v29.google.events.cloud.dataflow.v1beta3.AutoscalingSettingsR\x13autoscalingSettings\x12\x18\n" +
	"\anetwork\x18\x11 \x01(\tR\anetwork\x12\x1e\n" +
	"\n" +
	"subnetwork\x18\x13 \x01(\tR\n" +
	"subnetwork\x12C\n" +
	"\x1eworker_harness_container_image\x18\x12 \x01(\tR\x1bworkerHarnessContainerImage\x123\n" +
	"\x16num_threads_per_worker\x18\x14 \x01(\x05R\x13numThreadsPerWorker\x12m\n" +
	"\x10ip_configuration\x18\x15 \x01(\x0e2B.google.events.cloud.dataflow.v1beta3.WorkerIPAddressConfigurationR\x0fipConfiguration\x12\x7f\n" +
	"\x1csdk_harness_container_images\x18\x16 \x03(\v2>.google.events.cloud.dataflow.v1beta3.SdkHarnessContainerImageR\x19sdkHarnessContainerImages\x1a;\n" +
	"\rMetadataEntry\x12\x10\n" +
	"\x03key\x18\x01 \x01(\tR\x03key\x12\x14\n" +
	"\x05value\x18\x02 \x01(\tR\x05value:\x028\x01\"C\n" +
	"\fDebugOptions\x123\n" +
	"\x16enable_hot_key_logging\x18\x01 \x01(\bR\x13enableHotKeyLogging\"\xac\n" +
	"\n" +
	"\x03Job\x12\x0e\n" +
	"\x02id\x18\x01 \x01(\tR\x02id\x12\x1d\n" +
	"\n" +
	"project_id\x18\x02 \x01(\tR\tprojectId\x12\x12\n" +
	"\x04name\x18\x03 \x01(\tR\x04name\x12A\n" +
	"\x04type\x18\x04 \x01(\x0e2-.google.events.cloud.dataflow.v1beta3.JobTypeR\x04type\x12S\n" +
	"\venvironment\x18\x05 \x01(\v21.google.events.cloud.dataflow.v1beta3.EnvironmentR\venvironment\x12%\n" +
	"\x0esteps_location\x18\x18 \x01(\tR\rstepsLocation\x12S\n" +
	"\rcurrent_state\x18\a \x01(\x0e2..google.events.cloud.dataflow.v1beta3.JobStateR\fcurrentState\x12H\n" +
	"\x12current_state_time\x18\b \x01(\v2\x1a.google.protobuf.TimestampR\x10currentStateTime\x12W\n" +
	"\x0frequested_state\x18\t \x01(\x0e2..google.events.cloud.dataflow.v1beta3.JobStateR\x0erequestedState\x12]\n" +
	"\x0eexecution_info\x18\n" +
	" \x01(\v26.google.events.cloud.dataflow.v1beta3.JobExecutionInfoR\rexecutionInfo\x12;\n" +
	"\vcreate_time\x18\v \x01(\v2\x1a.google.protobuf.TimestampR\n" +
	"createTime\x12$\n" +
	"\x0ereplace_job_id\x18\f \x01(\tR\freplaceJobId\x12*\n" +
	"\x11client_request_id\x18\x0e \x01(\tR\x0fclientRequestId\x12+\n" +
	"\x12replaced_by_job_id\x18\x0f \x01(\tR\x0freplacedByJobId\x12\x1d\n" +
	"\n" +
	"temp_files\x18\x10 \x03(\tR\ttempFiles\x12M\n" +
	"\x06labels\x18\x11 \x03(\v25.google.events.cloud.dataflow.v1beta3.Job.LabelsEntryR\x06labels\x12\x1a\n" +
	"\blocation\x18\x12 \x01(\tR\blocation\x12\\\n" +
	"\fstage_states\x18\x14 \x03(\v29.google.events.cloud.dataflow.v1beta3.ExecutionStageStateR\vstageStates\x12T\n" +
	"\fjob_metadata\x18\x15 \x01(\v21.google.events.cloud.dataflow.v1beta3.JobMetadataR\vjobMetadata\x129\n" +
	"\n" +
	"start_time\x18\x16 \x01(\v2\x1a.google.protobuf.TimestampR\tstartTime\x127\n" +
	"\x18created_from_snapshot_id\x18\x17 \x01(\tR\x15createdFromSnapshotId\x12#\n" +
	"\rsatisfies_pzs\x18\x19 \x01(\bR\fsatisfiesPzs\x1a9\n" +
	"\vLabelsEntry\x12\x10\n" +
	"\x03key\x18\x01 \x01(\tR\x03key\x12\x14\n" +
	"\x05value\x18\x02 \x01(\tR\x05value:\x028\x01\"Q\n" +
	"\x12DatastoreIODetails\x12\x1c\n" +
	"\tnamespace\x18\x01 \x01(\tR\tnamespace\x12\x1d\n" +
	"\n" +
	"project_id\x18\x02 \x01(\tR\tprojectId\"K\n" +
	"\x0fPubSubIODetails\x12\x14\n" +
	"\x05topic\x18\x01 \x01(\tR\x05topic\x12\"\n" +
	"\fsubscription\x18\x02 \x01(\tR\fsubscription\"2\n" +
	"\rFileIODetails\x12!\n" +
	"\ffile_pattern\x18\x01 \x01(\tR\vfilePattern\"n\n" +
	"\x11BigTableIODetails\x12\x1d\n" +
	"\n" +
	"project_id\x18\x01 \x01(\tR\tprojectId\x12\x1f\n" +
	"\vinstance_id\x18\x02 \x01(\tR\n" +
	"instanceId\x12\x19\n" +
	"\btable_id\x18\x03 \x01(\tR\atableId\"x\n" +
	"\x11BigQueryIODetails\x12\x14\n" +
	"\x05table\x18\x01 \x01(\tR\x05table\x12\x18\n" +
	"\adataset\x18\x02 \x01(\tR\adataset\x12\x1d\n" +
	"\n" +
	"project_id\x18\x03 \x01(\tR\tprojectId\x12\x14\n" +
	"\x05query\x18\x04 \x01(\tR\x05query\"s\n" +
	"\x10SpannerIODetails\x12\x1d\n" +
	"\n" +
	"project_id\x18\x01 \x01(\tR\tprojectId\x12\x1f\n" +
	"\vinstance_id\x18\x02 \x01(\tR\n" +
	"instanceId\x12\x1f\n" +
	"\vdatabase_id\x18\x03 \x01(\tR\n" +
	"databaseId\"\xa5\x02\n" +
	"\n" +
	"SdkVersion\x12\x18\n" +
	"\aversion\x18\x01 \x01(\tR\aversion\x120\n" +
	"\x14version_display_name\x18\x02 \x01(\tR\x12versionDisplayName\x12o\n" +
	"\x12sdk_support_status\x18\x03 \x01(\x0e2A.google.events.cloud.dataflow.v1beta3.SdkVersion.SdkSupportStatusR\x10sdkSupportStatus\"Z\n" +
	"\x10SdkSupportStatus\x12\v\n" +
	"\aUNKNOWN\x10\x00\x12\r\n" +
	"\tSUPPORTED\x10\x01\x12\t\n" +
	"\x05STALE\x10\x02\x12\x0e\n" +
	"\n" +
	"DEPRECATED\x10\x03\x12\x0f\n" +
	"\vUNSUPPORTED\x10\x04\"\xa7\x05\n" +
	"\vJobMetadata\x12Q\n" +
	"\vsdk_version\x18\x01 \x01(\v20.google.events.cloud.dataflow.v1beta3.SdkVersionR\n" +
	"sdkVersion\x12_\n" +
	"\x0fspanner_details\x18\x02 \x03(\v26.google.events.cloud.dataflow.v1beta3.SpannerIODetailsR\x0espannerDetails\x12b\n" +
	"\x10bigquery_details\x18\x03 \x03(\v27.google.events.cloud.dataflow.v1beta3.BigQueryIODetailsR\x0fbigqueryDetails\x12c\n" +
	"\x11big_table_details\x18\x04 \x03(\v27.google.events.cloud.dataflow.v1beta3.BigTableIODetailsR\x0fbigTableDetails\x12\\\n" +
	"\x0epubsub_details\x18\x05 \x03(\v25.google.events.cloud.dataflow.v1beta3.PubSubIODetailsR\rpubsubDetails\x12V\n" +
	"\ffile_details\x18\x06 \x03(\v23.google.events.cloud.dataflow.v1beta3.FileIODetailsR\vfileDetails\x12e\n" +
	"\x11datastore_details\x18\a \x03(\v28.google.events.cloud.dataflow.v1beta3.DatastoreIODetailsR\x10datastoreDetails\"\xf5\x01\n" +
	"\x13ExecutionStageState\x120\n" +
	"\x14execution_stage_name\x18\x01 \x01(\tR\x12executionStageName\x12b\n" +
	"\x15execution_stage_state\x18\x02 \x01(\x0e2..google.events.cloud.dataflow.v1beta3.JobStateR\x13executionStageState\x12H\n" +
	"\x12current_state_time\x18\x03 \x01(\v2\x1a.google.protobuf.TimestampR\x10currentStateTime\"\xe6\x01\n" +
	"\x10JobExecutionInfo\x12Z\n" +
	"\x06stages\x18\x01 \x03(\v2B.google.events.cloud.dataflow.v1beta3.JobExecutionInfo.StagesEntryR\x06stages\x1av\n" +
	"\vStagesEntry\x12\x10\n" +
	"\x03key\x18\x01 \x01(\tR\x03key\x12Q\n" +
	"\x05value\x18\x02 \x01(\v2;.google.events.cloud.dataflow.v1beta3.JobExecutionStageInfoR\x05value:\x028\x01\"4\n" +
	"\x15JobExecutionStageInfo\x12\x1b\n" +
	"\tstep_name\x18\x01 \x03(\tR\bstepName\"S\n" +
	"\fJobEventData\x12C\n" +
	"\apayload\x18\x01 \x01(\v2).google.events.cloud.dataflow.v1beta3.JobR\apayload*K\n" +
	"\aJobType\x12\x14\n" +
	"\x10JOB_TYPE_UNKNOWN\x10\x00\x12\x12\n" +
	"\x0eJOB_TYPE_BATCH\x10\x01\x12\x16\n" +
	"\x12JOB_TYPE_STREAMING\x10\x02*k\n" +
	"\x1aFlexResourceSchedulingGoal\x12\x16\n" +
	"\x12FLEXRS_UNSPECIFIED\x10\x00\x12\x1a\n" +
	"\x16FLEXRS_SPEED_OPTIMIZED\x10\x01\x12\x19\n" +
	"\x15FLEXRS_COST_OPTIMIZED\x10\x02*o\n" +
	"\x0eTeardownPolicy\x12\x1b\n" +
	"\x17TEARDOWN_POLICY_UNKNOWN\x10\x00\x12\x13\n" +
	"\x0fTEARDOWN_ALWAYS\x10\x01\x12\x17\n" +
	"\x13TEARDOWN_ON_SUCCESS\x10\x02\x12\x12\n" +
	"\x0eTEARDOWN_NEVER\x10\x03*\x90\x01\n" +
	"\x11DefaultPackageSet\x12\x1f\n" +
	"\x1bDEFAULT_PACKAGE_SET_UNKNOWN\x10\x00\x12\x1c\n" +
	"\x18DEFAULT_PACKAGE_SET_NONE\x10\x01\x12\x1c\n" +
	"\x18DEFAULT_PACKAGE_SET_JAVA\x10\x02\x12\x1e\n" +
	"\x1aDEFAULT_PACKAGE_SET_PYTHON\x10\x03*z\n" +
	"\x14AutoscalingAlgorithm\x12!\n" +
	"\x1dAUTOSCALING_ALGORITHM_UNKNOWN\x10\x00\x12\x1e\n" +
	"\x1aAUTOSCALING_ALGORITHM_NONE\x10\x01\x12\x1f\n" +
	"\x1bAUTOSCALING_ALGORITHM_BASIC\x10\x02*f\n" +
	"\x1cWorkerIPAddressConfiguration\x12\x19\n" +
	"\x15WORKER_IP_UNSPECIFIED\x10\x00\x12\x14\n" +
	"\x10WORKER_IP_PUBLIC\x10\x01\x12\x15\n" +
	"\x11WORKER_IP_PRIVATE\x10\x02*L\n" +
	"\vShuffleMode\x12\x1c\n" +
	"\x18SHUFFLE_MODE_UNSPECIFIED\x10\x00\x12\f\n" +
	"\bVM_BASED\x10\x01\x12\x11\n" +
	"\rSERVICE_BASED\x10\x02*\xc3\x02\n" +
	"\bJobState\x12\x15\n" +
	"\x11JOB_STATE_UNKNOWN\x10\x00\x12\x15\n" +
	"\x11JOB_STATE_STOPPED\x10\x01\x12\x15\n" +
	"\x11JOB_STATE_RUNNING\x10\x02\x12\x12\n" +
	"\x0eJOB_STATE_DONE\x10\x03\x12\x14\n" +
	"\x10JOB_STATE_FAILED\x10\x04\x12\x17\n" +
	"\x13JOB_STATE_CANCELLED\x10\x05\x12\x15\n" +
	"\x11JOB_STATE_UPDATED\x10\x06\x12\x16\n" +
	"\x12JOB_STATE_DRAINING\x10\a\x12\x15\n" +
	"\x11JOB_STATE_DRAINED\x10\b\x12\x15\n" +
	"\x11JOB_STATE_PENDING\x10\t\x12\x18\n" +
	"\x14JOB_STATE_CANCELLING\x10\n" +
	"\x12\x14\n" +
	"\x10JOB_STATE_QUEUED\x10\v\x12\"\n" +
	"\x1eJOB_STATE_RESOURCE_CLEANING_UP\x10\fB\x82\x01\xaa\x02-Google.Events.Protobuf.Cloud.Dataflow.V1Beta3\xca\x02$Google\\Events\\Cloud\\Dataflow\\V1beta3\xea\x02(Google::Events::Cloud::Dataflow::V1beta3b\x06proto3"

var (
	file_cloud_dataflow_v1beta3_data_proto_rawDescOnce sync.Once
	file_cloud_dataflow_v1beta3_data_proto_rawDescData []byte
)

func file_cloud_dataflow_v1beta3_data_proto_rawDescGZIP() []byte {
	file_cloud_dataflow_v1beta3_data_proto_rawDescOnce.Do(func() {
		file_cloud_dataflow_v1beta3_data_proto_rawDescData = protoimpl.X.CompressGZIP(unsafe.Slice(unsafe.StringData(file_cloud_dataflow_v1beta3_data_proto_rawDesc), len(file_cloud_dataflow_v1beta3_data_proto_rawDesc)))
	})
	return file_cloud_dataflow_v1beta3_data_proto_rawDescData
}

var file_cloud_dataflow_v1beta3_data_proto_enumTypes = make([]protoimpl.EnumInfo, 9)
var file_cloud_dataflow_v1beta3_data_proto_msgTypes = make([]protoimpl.MessageInfo, 22)
var file_cloud_dataflow_v1beta3_data_proto_goTypes = []any{
	(JobType)(0),                      // 0: google.events.cloud.dataflow.v1beta3.JobType
	(FlexResourceSchedulingGoal)(0),   // 1: google.events.cloud.dataflow.v1beta3.FlexResourceSchedulingGoal
	(TeardownPolicy)(0),               // 2: google.events.cloud.dataflow.v1beta3.TeardownPolicy
	(DefaultPackageSet)(0),            // 3: google.events.cloud.dataflow.v1beta3.DefaultPackageSet
	(AutoscalingAlgorithm)(0),         // 4: google.events.cloud.dataflow.v1beta3.AutoscalingAlgorithm
	(WorkerIPAddressConfiguration)(0), // 5: google.events.cloud.dataflow.v1beta3.WorkerIPAddressConfiguration
	(ShuffleMode)(0),                  // 6: google.events.cloud.dataflow.v1beta3.ShuffleMode
	(JobState)(0),                     // 7: google.events.cloud.dataflow.v1beta3.JobState
	(SdkVersion_SdkSupportStatus)(0),  // 8: google.events.cloud.dataflow.v1beta3.SdkVersion.SdkSupportStatus
	(*Environment)(nil),               // 9: google.events.cloud.dataflow.v1beta3.Environment
	(*Package)(nil),                   // 10: google.events.cloud.dataflow.v1beta3.Package
	(*AutoscalingSettings)(nil),       // 11: google.events.cloud.dataflow.v1beta3.AutoscalingSettings
	(*SdkHarnessContainerImage)(nil),  // 12: google.events.cloud.dataflow.v1beta3.SdkHarnessContainerImage
	(*WorkerPool)(nil),                // 13: google.events.cloud.dataflow.v1beta3.WorkerPool
	(*DebugOptions)(nil),              // 14: google.events.cloud.dataflow.v1beta3.DebugOptions
	(*Job)(nil),                       // 15: google.events.cloud.dataflow.v1beta3.Job
	(*DatastoreIODetails)(nil),        // 16: google.events.cloud.dataflow.v1beta3.DatastoreIODetails
	(*PubSubIODetails)(nil),           // 17: google.events.cloud.dataflow.v1beta3.PubSubIODetails
	(*FileIODetails)(nil),             // 18: google.events.cloud.dataflow.v1beta3.FileIODetails
	(*BigTableIODetails)(nil),         // 19: google.events.cloud.dataflow.v1beta3.BigTableIODetails
	(*BigQueryIODetails)(nil),         // 20: google.events.cloud.dataflow.v1beta3.BigQueryIODetails
	(*SpannerIODetails)(nil),          // 21: google.events.cloud.dataflow.v1beta3.SpannerIODetails
	(*SdkVersion)(nil),                // 22: google.events.cloud.dataflow.v1beta3.SdkVersion
	(*JobMetadata)(nil),               // 23: google.events.cloud.dataflow.v1beta3.JobMetadata
	(*ExecutionStageState)(nil),       // 24: google.events.cloud.dataflow.v1beta3.ExecutionStageState
	(*JobExecutionInfo)(nil),          // 25: google.events.cloud.dataflow.v1beta3.JobExecutionInfo
	(*JobExecutionStageInfo)(nil),     // 26: google.events.cloud.dataflow.v1beta3.JobExecutionStageInfo
	(*JobEventData)(nil),              // 27: google.events.cloud.dataflow.v1beta3.JobEventData
	nil,                               // 28: google.events.cloud.dataflow.v1beta3.WorkerPool.MetadataEntry
	nil,                               // 29: google.events.cloud.dataflow.v1beta3.Job.LabelsEntry
	nil,                               // 30: google.events.cloud.dataflow.v1beta3.JobExecutionInfo.StagesEntry
	(*structpb.Struct)(nil),           // 31: google.protobuf.Struct
	(*timestamppb.Timestamp)(nil),     // 32: google.protobuf.Timestamp
}
var file_cloud_dataflow_v1beta3_data_proto_depIdxs = []int32{
	13, // 0: google.events.cloud.dataflow.v1beta3.Environment.worker_pools:type_name -> google.events.cloud.dataflow.v1beta3.WorkerPool
	31, // 1: google.events.cloud.dataflow.v1beta3.Environment.user_agent:type_name -> google.protobuf.Struct
	31, // 2: google.events.cloud.dataflow.v1beta3.Environment.version:type_name -> google.protobuf.Struct
	31, // 3: google.events.cloud.dataflow.v1beta3.Environment.sdk_pipeline_options:type_name -> google.protobuf.Struct
	1,  // 4: google.events.cloud.dataflow.v1beta3.Environment.flex_resource_scheduling_goal:type_name -> google.events.cloud.dataflow.v1beta3.FlexResourceSchedulingGoal
	6,  // 5: google.events.cloud.dataflow.v1beta3.Environment.shuffle_mode:type_name -> google.events.cloud.dataflow.v1beta3.ShuffleMode
	14, // 6: google.events.cloud.dataflow.v1beta3.Environment.debug_options:type_name -> google.events.cloud.dataflow.v1beta3.DebugOptions
	4,  // 7: google.events.cloud.dataflow.v1beta3.AutoscalingSettings.algorithm:type_name -> google.events.cloud.dataflow.v1beta3.AutoscalingAlgorithm
	10, // 8: google.events.cloud.dataflow.v1beta3.WorkerPool.packages:type_name -> google.events.cloud.dataflow.v1beta3.Package
	3,  // 9: google.events.cloud.dataflow.v1beta3.WorkerPool.default_package_set:type_name -> google.events.cloud.dataflow.v1beta3.DefaultPackageSet
	2,  // 10: google.events.cloud.dataflow.v1beta3.WorkerPool.teardown_policy:type_name -> google.events.cloud.dataflow.v1beta3.TeardownPolicy
	28, // 11: google.events.cloud.dataflow.v1beta3.WorkerPool.metadata:type_name -> google.events.cloud.dataflow.v1beta3.WorkerPool.MetadataEntry
	11, // 12: google.events.cloud.dataflow.v1beta3.WorkerPool.autoscaling_settings:type_name -> google.events.cloud.dataflow.v1beta3.AutoscalingSettings
	5,  // 13: google.events.cloud.dataflow.v1beta3.WorkerPool.ip_configuration:type_name -> google.events.cloud.dataflow.v1beta3.WorkerIPAddressConfiguration
	12, // 14: google.events.cloud.dataflow.v1beta3.WorkerPool.sdk_harness_container_images:type_name -> google.events.cloud.dataflow.v1beta3.SdkHarnessContainerImage
	0,  // 15: google.events.cloud.dataflow.v1beta3.Job.type:type_name -> google.events.cloud.dataflow.v1beta3.JobType
	9,  // 16: google.events.cloud.dataflow.v1beta3.Job.environment:type_name -> google.events.cloud.dataflow.v1beta3.Environment
	7,  // 17: google.events.cloud.dataflow.v1beta3.Job.current_state:type_name -> google.events.cloud.dataflow.v1beta3.JobState
	32, // 18: google.events.cloud.dataflow.v1beta3.Job.current_state_time:type_name -> google.protobuf.Timestamp
	7,  // 19: google.events.cloud.dataflow.v1beta3.Job.requested_state:type_name -> google.events.cloud.dataflow.v1beta3.JobState
	25, // 20: google.events.cloud.dataflow.v1beta3.Job.execution_info:type_name -> google.events.cloud.dataflow.v1beta3.JobExecutionInfo
	32, // 21: google.events.cloud.dataflow.v1beta3.Job.create_time:type_name -> google.protobuf.Timestamp
	29, // 22: google.events.cloud.dataflow.v1beta3.Job.labels:type_name -> google.events.cloud.dataflow.v1beta3.Job.LabelsEntry
	24, // 23: google.events.cloud.dataflow.v1beta3.Job.stage_states:type_name -> google.events.cloud.dataflow.v1beta3.ExecutionStageState
	23, // 24: google.events.cloud.dataflow.v1beta3.Job.job_metadata:type_name -> google.events.cloud.dataflow.v1beta3.JobMetadata
	32, // 25: google.events.cloud.dataflow.v1beta3.Job.start_time:type_name -> google.protobuf.Timestamp
	8,  // 26: google.events.cloud.dataflow.v1beta3.SdkVersion.sdk_support_status:type_name -> google.events.cloud.dataflow.v1beta3.SdkVersion.SdkSupportStatus
	22, // 27: google.events.cloud.dataflow.v1beta3.JobMetadata.sdk_version:type_name -> google.events.cloud.dataflow.v1beta3.SdkVersion
	21, // 28: google.events.cloud.dataflow.v1beta3.JobMetadata.spanner_details:type_name -> google.events.cloud.dataflow.v1beta3.SpannerIODetails
	20, // 29: google.events.cloud.dataflow.v1beta3.JobMetadata.bigquery_details:type_name -> google.events.cloud.dataflow.v1beta3.BigQueryIODetails
	19, // 30: google.events.cloud.dataflow.v1beta3.JobMetadata.big_table_details:type_name -> google.events.cloud.dataflow.v1beta3.BigTableIODetails
	17, // 31: google.events.cloud.dataflow.v1beta3.JobMetadata.pubsub_details:type_name -> google.events.cloud.dataflow.v1beta3.PubSubIODetails
	18, // 32: google.events.cloud.dataflow.v1beta3.JobMetadata.file_details:type_name -> google.events.cloud.dataflow.v1beta3.FileIODetails
	16, // 33: google.events.cloud.dataflow.v1beta3.JobMetadata.datastore_details:type_name -> google.events.cloud.dataflow.v1beta3.DatastoreIODetails
	7,  // 34: google.events.cloud.dataflow.v1beta3.ExecutionStageState.execution_stage_state:type_name -> google.events.cloud.dataflow.v1beta3.JobState
	32, // 35: google.events.cloud.dataflow.v1beta3.ExecutionStageState.current_state_time:type_name -> google.protobuf.Timestamp
	30, // 36: google.events.cloud.dataflow.v1beta3.JobExecutionInfo.stages:type_name -> google.events.cloud.dataflow.v1beta3.JobExecutionInfo.StagesEntry
	15, // 37: google.events.cloud.dataflow.v1beta3.JobEventData.payload:type_name -> google.events.cloud.dataflow.v1beta3.Job
	26, // 38: google.events.cloud.dataflow.v1beta3.JobExecutionInfo.StagesEntry.value:type_name -> google.events.cloud.dataflow.v1beta3.JobExecutionStageInfo
	39, // [39:39] is the sub-list for method output_type
	39, // [39:39] is the sub-list for method input_type
	39, // [39:39] is the sub-list for extension type_name
	39, // [39:39] is the sub-list for extension extendee
	0,  // [0:39] is the sub-list for field type_name
}

func init() { file_cloud_dataflow_v1beta3_data_proto_init() }
func file_cloud_dataflow_v1beta3_data_proto_init() {
	if File_cloud_dataflow_v1beta3_data_proto != nil {
		return
	}
	type x struct{}
	out := protoimpl.TypeBuilder{
		File: protoimpl.DescBuilder{
			GoPackagePath: reflect.TypeOf(x{}).PkgPath(),
			RawDescriptor: unsafe.Slice(unsafe.StringData(file_cloud_dataflow_v1beta3_data_proto_rawDesc), len(file_cloud_dataflow_v1beta3_data_proto_rawDesc)),
			NumEnums:      9,
			NumMessages:   22,
			NumExtensions: 0,
			NumServices:   0,
		},
		GoTypes:           file_cloud_dataflow_v1beta3_data_proto_goTypes,
		DependencyIndexes: file_cloud_dataflow_v1beta3_data_proto_depIdxs,
		EnumInfos:         file_cloud_dataflow_v1beta3_data_proto_enumTypes,
		MessageInfos:      file_cloud_dataflow_v1beta3_data_proto_msgTypes,
	}.Build()
	File_cloud_dataflow_v1beta3_data_proto = out.File
	file_cloud_dataflow_v1beta3_data_proto_goTypes = nil
	file_cloud_dataflow_v1beta3_data_proto_depIdxs = nil
}
